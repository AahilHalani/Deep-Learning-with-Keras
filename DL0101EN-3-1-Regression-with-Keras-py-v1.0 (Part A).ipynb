{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0101EN-SkillsNetwork/images/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Regression Models with Keras</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed in the videos, despite the popularity of more powerful libraries such as PyToch and TensorFlow, they are not easy to use and have a steep learning curve. So, for people who are just starting to learn deep learning, there is no better library to use other than the Keras library. \n",
    "\n",
    "Keras is a high-level API for building deep learning models. It has gained favor for its ease of use and syntactic simplicity facilitating fast development. As you will see in this lab and the other labs in this course, building a very complex deep learning network can be achieved with Keras with only few lines of code. You will appreciate Keras even more, once you learn how to build deep models using PyTorch and TensorFlow in the other courses.\n",
    "\n",
    "So, in this lab, you will learn how to use the Keras library to build a regression model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regression Models with Keras</h2>\n",
    "\n",
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. How to use the Keras library to build a regression model.</h5>\n",
    "<h5> 2. Download and Clean dataset </h5>\n",
    "<h5> 3. Build a Neural Network </h5>\n",
    "<h5> 4. Train and Test the Network. </h5>     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3>\n",
    "    \n",
    "1. <a href=\"#item31\">Download and Clean Dataset</a>  \n",
    "2. <a href=\"#item32\">Import Keras</a>  \n",
    "3. <a href=\"#item33\">Build a Neural Network</a>  \n",
    "4. <a href=\"#item34\">Train and Test the Network</a>  \n",
    "\n",
    "</font>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item31\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Clean Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the <em>pandas</em> and the Numpy libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented. \n",
    "# If you run this notebook on a different environment, e.g. your desktop, you may need to uncomment and install certain libraries.\n",
    "\n",
    "#!pip install numpy==1.21.4\n",
    "#!pip install pandas==1.3.4\n",
    "#!pip install keras==2.1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be playing around with the same dataset that we used in the videos.\n",
    "\n",
    "<strong>The dataset is about the compressive strength of different samples of concrete based on the volumes of the different ingredients that were used to make them. Ingredients include:</strong>\n",
    "\n",
    "<strong>1. Cement</strong>\n",
    "\n",
    "<strong>2. Blast Furnace Slag</strong>\n",
    "\n",
    "<strong>3. Fly Ash</strong>\n",
    "\n",
    "<strong>4. Water</strong>\n",
    "\n",
    "<strong>5. Superplasticizer</strong>\n",
    "\n",
    "<strong>6. Coarse Aggregate</strong>\n",
    "\n",
    "<strong>7. Fine Aggregate</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data and read it into a <em>pandas</em> dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first concrete sample has 540 cubic meter of cement, 0 cubic meter of blast furnace slag, 0 cubic meter of fly ash, 162 cubic meter of water, 2.5 cubic meter of superplaticizer, 1040 cubic meter of coarse aggregate, 676 cubic meter of fine aggregate. Such a concrete mix which is 28 days old, has a compressive strength of 79.99 MPa. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check how many data points we have.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there are approximately 1000 samples to train our model on. Because of the few samples, we have to be careful not to overfit the training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the dataset for any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cement                0\n",
       "Blast Furnace Slag    0\n",
       "Fly Ash               0\n",
       "Water                 0\n",
       "Superplasticizer      0\n",
       "Coarse Aggregate      0\n",
       "Fine Aggregate        0\n",
       "Age                   0\n",
       "Strength              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks very clean and is ready to be used to build our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data into predictors and target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable in this problem is the concrete sample strength. Therefore, our predictors will be all the other columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',\n",
      "       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "concrete_data_columns = concrete_data.columns\n",
    "print(concrete_data_columns)\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item2\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check of the predictors and the target dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  \n",
       "0            1040.0           676.0   28  \n",
       "1            1055.0           676.0   28  \n",
       "2             932.0           594.0  270  \n",
       "3             932.0           594.0  365  \n",
       "4             978.4           825.5  360  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.99\n",
       "1    61.89\n",
       "2    40.27\n",
       "3    41.05\n",
       "4    44.30\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the last step is to normalize the data by substracting the mean and dividing by the standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4101079523.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Normalization commented out\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Normalization commented out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>0.862735</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.476712</td>\n",
       "      <td>-0.856472</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>-0.916319</td>\n",
       "      <td>-0.620147</td>\n",
       "      <td>1.055651</td>\n",
       "      <td>-1.217079</td>\n",
       "      <td>-0.279597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>3.551340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.491187</td>\n",
       "      <td>0.795140</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>2.174405</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>-0.526262</td>\n",
       "      <td>-2.239829</td>\n",
       "      <td>5.055221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.790075</td>\n",
       "      <td>0.678079</td>\n",
       "      <td>-0.846733</td>\n",
       "      <td>0.488555</td>\n",
       "      <td>-1.038638</td>\n",
       "      <td>0.070492</td>\n",
       "      <td>0.647569</td>\n",
       "      <td>4.976069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Cement  Blast Furnace Slag   Fly Ash     Water  Superplasticizer  \\\n",
       "0  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "1  2.476712           -0.856472 -0.846733 -0.916319         -0.620147   \n",
       "2  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "3  0.491187            0.795140 -0.846733  2.174405         -1.038638   \n",
       "4 -0.790075            0.678079 -0.846733  0.488555         -1.038638   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate       Age  \n",
       "0          0.862735       -1.217079 -0.279597  \n",
       "1          1.055651       -1.217079 -0.279597  \n",
       "2         -0.526262       -2.239829  3.551340  \n",
       "3         -0.526262       -2.239829  5.055221  \n",
       "4          0.070492        0.647569  4.976069  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictors_norm = (predictors - predictors.mean()) / predictors.std()\n",
    "# predictors_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the number of predictors to *n_cols* since we will need this number when building our network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n_cols = predictors_norm.shape[1] # number of predictors\n",
    "# n_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item1\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the videos that Keras normally runs on top of a low-level library such as TensorFlow. This means that to be able to use the Keras library, you will have to install TensorFlow first and when you import the Keras library, it will be explicitly displayed what backend was used to install the Keras library. In CC Labs, we used TensorFlow as the backend to install Keras, so it should clearly print that when we import Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's go ahead and import the Keras library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the TensorFlow backend was used to install the Keras library.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the rest of the packages from the Keras library that we will need to build our regressoin model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Build a Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function that defines our regression model for us so that we can conveniently call it to create our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Part A: One hidden layer of 10 units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # define regression model\n",
    "# def regression_model():\n",
    "#     # create model\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "#     model.add(Dense(1))\n",
    "    \n",
    "#     # compile model\n",
    "#     model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def reg_sgd():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(50, activation = 'relu', input_shape=(n_cols,)))\n",
    "#     model.add(Dense(50, activation = 'relu',))\n",
    "#     model.add(Dense(1))\n",
    "    \n",
    "#     model.compile(optimizer='sgd', loss = 'mean_squared_error')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The above function create a model that has two hidden layers, each of 50 hidden units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item4\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test the Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function now to create our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will train and test the model at the same time using the *fit* method. We will leave out 30% of the data for validation and we will train the model for 100 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 82007.7418 - val_loss: 60905.6074\n",
      "Epoch 2/50\n",
      " - 0s - loss: 47711.8289 - val_loss: 35144.7434\n",
      "Epoch 3/50\n",
      " - 0s - loss: 27288.4553 - val_loss: 19850.4360\n",
      "Epoch 4/50\n",
      " - 0s - loss: 15195.5724 - val_loss: 11082.1733\n",
      "Epoch 5/50\n",
      " - 0s - loss: 8309.1490 - val_loss: 6314.7272\n",
      "Epoch 6/50\n",
      " - 0s - loss: 4640.8264 - val_loss: 3840.3885\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2773.9271 - val_loss: 2916.9960\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2191.3804 - val_loss: 2736.6496\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2069.6171 - val_loss: 2650.1792\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1995.1504 - val_loss: 2541.8656\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1921.8564 - val_loss: 2459.2508\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1862.4058 - val_loss: 2383.2906\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1800.9809 - val_loss: 2308.0654\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1744.5275 - val_loss: 2235.6939\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1686.9547 - val_loss: 2161.6586\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1632.6698 - val_loss: 2096.8969\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1582.4958 - val_loss: 2030.0377\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1538.5736 - val_loss: 1980.7897\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1489.4049 - val_loss: 1918.0709\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1449.8084 - val_loss: 1862.3198\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1412.2405 - val_loss: 1811.8151\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1381.6154 - val_loss: 1770.0851\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1340.3320 - val_loss: 1718.3015\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1308.6191 - val_loss: 1674.5136\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1275.1839 - val_loss: 1631.3332\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1239.9699 - val_loss: 1594.9656\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1209.3129 - val_loss: 1549.4714\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1177.1650 - val_loss: 1508.8069\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1147.3918 - val_loss: 1467.9759\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1119.0333 - val_loss: 1429.7790\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1089.7997 - val_loss: 1395.2267\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1063.9231 - val_loss: 1359.9449\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1036.6175 - val_loss: 1327.3581\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1011.0791 - val_loss: 1292.7878\n",
      "Epoch 35/50\n",
      " - 0s - loss: 986.0219 - val_loss: 1258.8463\n",
      "Epoch 36/50\n",
      " - 0s - loss: 959.6216 - val_loss: 1228.4782\n",
      "Epoch 37/50\n",
      " - 0s - loss: 938.0319 - val_loss: 1199.0752\n",
      "Epoch 38/50\n",
      " - 0s - loss: 918.1188 - val_loss: 1166.2415\n",
      "Epoch 39/50\n",
      " - 0s - loss: 893.6483 - val_loss: 1141.0916\n",
      "Epoch 40/50\n",
      " - 0s - loss: 871.6244 - val_loss: 1112.2934\n",
      "Epoch 41/50\n",
      " - 0s - loss: 851.3332 - val_loss: 1085.2259\n",
      "Epoch 42/50\n",
      " - 0s - loss: 832.5609 - val_loss: 1057.3410\n",
      "Epoch 43/50\n",
      " - 0s - loss: 811.2129 - val_loss: 1032.0502\n",
      "Epoch 44/50\n",
      " - 0s - loss: 792.7553 - val_loss: 1007.7368\n",
      "Epoch 45/50\n",
      " - 0s - loss: 774.5733 - val_loss: 982.4127\n",
      "Epoch 46/50\n",
      " - 0s - loss: 756.6899 - val_loss: 958.7921\n",
      "Epoch 47/50\n",
      " - 0s - loss: 739.8685 - val_loss: 935.6216\n",
      "Epoch 48/50\n",
      " - 0s - loss: 721.6696 - val_loss: 912.5444\n",
      "Epoch 49/50\n",
      " - 0s - loss: 705.5867 - val_loss: 891.3654\n",
      "Epoch 50/50\n",
      " - 0s - loss: 690.9675 - val_loss: 867.3289\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 11401.3669 - val_loss: 9196.7363\n",
      "Epoch 2/50\n",
      " - 0s - loss: 8804.3074 - val_loss: 8257.1832\n",
      "Epoch 3/50\n",
      " - 0s - loss: 7568.1481 - val_loss: 7629.0477\n",
      "Epoch 4/50\n",
      " - 0s - loss: 6784.3892 - val_loss: 6500.8062\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5996.8149 - val_loss: 5726.6219\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5222.4215 - val_loss: 4954.1545\n",
      "Epoch 7/50\n",
      " - 0s - loss: 4609.7565 - val_loss: 4386.7018\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4061.9770 - val_loss: 3736.3449\n",
      "Epoch 9/50\n",
      " - 0s - loss: 3597.7010 - val_loss: 3259.8874\n",
      "Epoch 10/50\n",
      " - 0s - loss: 3178.3203 - val_loss: 2878.7272\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2838.4393 - val_loss: 2586.7621\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2516.9466 - val_loss: 2237.7849\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2264.4147 - val_loss: 2022.0851\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2040.6450 - val_loss: 1823.8822\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1850.4460 - val_loss: 1655.6334\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1688.9778 - val_loss: 1510.2621\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1552.3926 - val_loss: 1408.2244\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1446.6229 - val_loss: 1324.6720\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1342.0952 - val_loss: 1226.7115\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1248.8712 - val_loss: 1159.8920\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1174.2637 - val_loss: 1103.0086\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1118.2694 - val_loss: 1035.5342\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1043.2326 - val_loss: 986.9410\n",
      "Epoch 24/50\n",
      " - 0s - loss: 979.5319 - val_loss: 937.1821\n",
      "Epoch 25/50\n",
      " - 0s - loss: 926.1602 - val_loss: 893.7098\n",
      "Epoch 26/50\n",
      " - 0s - loss: 878.9600 - val_loss: 849.9236\n",
      "Epoch 27/50\n",
      " - 0s - loss: 840.8672 - val_loss: 813.8609\n",
      "Epoch 28/50\n",
      " - 0s - loss: 794.1247 - val_loss: 783.7475\n",
      "Epoch 29/50\n",
      " - 0s - loss: 763.6439 - val_loss: 747.4701\n",
      "Epoch 30/50\n",
      " - 0s - loss: 713.4705 - val_loss: 719.9359\n",
      "Epoch 31/50\n",
      " - 0s - loss: 677.9801 - val_loss: 694.0310\n",
      "Epoch 32/50\n",
      " - 0s - loss: 646.4836 - val_loss: 665.1309\n",
      "Epoch 33/50\n",
      " - 0s - loss: 619.7840 - val_loss: 639.7443\n",
      "Epoch 34/50\n",
      " - 0s - loss: 592.6536 - val_loss: 616.2069\n",
      "Epoch 35/50\n",
      " - 0s - loss: 568.4396 - val_loss: 598.3153\n",
      "Epoch 36/50\n",
      " - 0s - loss: 545.0862 - val_loss: 573.1381\n",
      "Epoch 37/50\n",
      " - 0s - loss: 522.2804 - val_loss: 552.8541\n",
      "Epoch 38/50\n",
      " - 0s - loss: 502.0565 - val_loss: 535.6940\n",
      "Epoch 39/50\n",
      " - 0s - loss: 481.2122 - val_loss: 517.3487\n",
      "Epoch 40/50\n",
      " - 0s - loss: 462.0869 - val_loss: 500.2101\n",
      "Epoch 41/50\n",
      " - 0s - loss: 443.5631 - val_loss: 483.8214\n",
      "Epoch 42/50\n",
      " - 0s - loss: 427.4940 - val_loss: 467.8507\n",
      "Epoch 43/50\n",
      " - 0s - loss: 417.2227 - val_loss: 455.1417\n",
      "Epoch 44/50\n",
      " - 0s - loss: 396.8594 - val_loss: 440.3794\n",
      "Epoch 45/50\n",
      " - 0s - loss: 385.5112 - val_loss: 434.3576\n",
      "Epoch 46/50\n",
      " - 0s - loss: 376.6713 - val_loss: 413.2897\n",
      "Epoch 47/50\n",
      " - 0s - loss: 359.6207 - val_loss: 400.0764\n",
      "Epoch 48/50\n",
      " - 0s - loss: 354.2542 - val_loss: 399.3455\n",
      "Epoch 49/50\n",
      " - 0s - loss: 338.2310 - val_loss: 388.7870\n",
      "Epoch 50/50\n",
      " - 0s - loss: 345.4361 - val_loss: 367.2453\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 9528.5410 - val_loss: 5986.1487\n",
      "Epoch 2/50\n",
      " - 0s - loss: 6356.8885 - val_loss: 5088.8729\n",
      "Epoch 3/50\n",
      " - 0s - loss: 5401.9089 - val_loss: 4609.0616\n",
      "Epoch 4/50\n",
      " - 0s - loss: 4614.2104 - val_loss: 4085.9874\n",
      "Epoch 5/50\n",
      " - 0s - loss: 4040.6058 - val_loss: 3719.9109\n",
      "Epoch 6/50\n",
      " - 0s - loss: 3584.2983 - val_loss: 3331.6877\n",
      "Epoch 7/50\n",
      " - 0s - loss: 3203.8860 - val_loss: 3000.6900\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2845.8172 - val_loss: 2715.0735\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2547.2722 - val_loss: 2488.8247\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2279.6880 - val_loss: 2175.5298\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2063.2359 - val_loss: 2052.3616\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1874.4564 - val_loss: 1726.9218\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1637.4025 - val_loss: 1574.5522\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1450.5480 - val_loss: 1434.4236\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1282.8937 - val_loss: 1252.5068\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1155.3436 - val_loss: 1113.8633\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1026.5980 - val_loss: 1041.0484\n",
      "Epoch 18/50\n",
      " - 0s - loss: 912.6703 - val_loss: 887.3655\n",
      "Epoch 19/50\n",
      " - 0s - loss: 808.3700 - val_loss: 813.9114\n",
      "Epoch 20/50\n",
      " - 0s - loss: 723.2409 - val_loss: 720.8384\n",
      "Epoch 21/50\n",
      " - 0s - loss: 645.9303 - val_loss: 653.8660\n",
      "Epoch 22/50\n",
      " - 0s - loss: 575.9687 - val_loss: 579.2064\n",
      "Epoch 23/50\n",
      " - 0s - loss: 525.2185 - val_loss: 551.2798\n",
      "Epoch 24/50\n",
      " - 0s - loss: 476.5433 - val_loss: 474.3462\n",
      "Epoch 25/50\n",
      " - 0s - loss: 427.6294 - val_loss: 430.5683\n",
      "Epoch 26/50\n",
      " - 0s - loss: 394.1907 - val_loss: 400.2623\n",
      "Epoch 27/50\n",
      " - 0s - loss: 352.9822 - val_loss: 369.4481\n",
      "Epoch 28/50\n",
      " - 0s - loss: 324.7604 - val_loss: 329.3943\n",
      "Epoch 29/50\n",
      " - 0s - loss: 297.2670 - val_loss: 304.0723\n",
      "Epoch 30/50\n",
      " - 0s - loss: 274.8431 - val_loss: 276.6782\n",
      "Epoch 31/50\n",
      " - 0s - loss: 259.5993 - val_loss: 252.2849\n",
      "Epoch 32/50\n",
      " - 0s - loss: 241.6824 - val_loss: 234.9042\n",
      "Epoch 33/50\n",
      " - 0s - loss: 226.5239 - val_loss: 222.1926\n",
      "Epoch 34/50\n",
      " - 0s - loss: 212.2550 - val_loss: 206.6589\n",
      "Epoch 35/50\n",
      " - 0s - loss: 202.2571 - val_loss: 198.1204\n",
      "Epoch 36/50\n",
      " - 0s - loss: 194.1262 - val_loss: 188.8837\n",
      "Epoch 37/50\n",
      " - 0s - loss: 187.4468 - val_loss: 180.7920\n",
      "Epoch 38/50\n",
      " - 0s - loss: 181.5188 - val_loss: 176.0001\n",
      "Epoch 39/50\n",
      " - 0s - loss: 173.0263 - val_loss: 157.9957\n",
      "Epoch 40/50\n",
      " - 0s - loss: 167.7467 - val_loss: 162.3058\n",
      "Epoch 41/50\n",
      " - 0s - loss: 166.2992 - val_loss: 150.2708\n",
      "Epoch 42/50\n",
      " - 0s - loss: 159.0344 - val_loss: 140.3328\n",
      "Epoch 43/50\n",
      " - 0s - loss: 157.4747 - val_loss: 137.3725\n",
      "Epoch 44/50\n",
      " - 0s - loss: 152.7002 - val_loss: 133.2414\n",
      "Epoch 45/50\n",
      " - 0s - loss: 150.8391 - val_loss: 138.8795\n",
      "Epoch 46/50\n",
      " - 0s - loss: 148.2606 - val_loss: 126.5171\n",
      "Epoch 47/50\n",
      " - 0s - loss: 144.5660 - val_loss: 125.0281\n",
      "Epoch 48/50\n",
      " - 0s - loss: 143.3583 - val_loss: 122.9657\n",
      "Epoch 49/50\n",
      " - 0s - loss: 139.0893 - val_loss: 130.0402\n",
      "Epoch 50/50\n",
      " - 0s - loss: 140.2529 - val_loss: 118.1673\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 44148.5555 - val_loss: 27855.7810\n",
      "Epoch 2/50\n",
      " - 0s - loss: 20490.0177 - val_loss: 10708.4494\n",
      "Epoch 3/50\n",
      " - 0s - loss: 7650.8332 - val_loss: 3766.4531\n",
      "Epoch 4/50\n",
      " - 0s - loss: 3338.3548 - val_loss: 2722.3196\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2764.8316 - val_loss: 2752.5338\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2681.1566 - val_loss: 2617.4346\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2559.6017 - val_loss: 2467.8842\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2463.6528 - val_loss: 2369.0788\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2372.9967 - val_loss: 2288.7371\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2282.9700 - val_loss: 2194.4555\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2192.9706 - val_loss: 2122.0987\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2108.5335 - val_loss: 2042.1007\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2031.1463 - val_loss: 1965.4918\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1953.8824 - val_loss: 1886.0637\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1882.7708 - val_loss: 1812.0204\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1808.8745 - val_loss: 1758.4540\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1744.1002 - val_loss: 1677.3734\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1677.5903 - val_loss: 1618.6828\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1616.1382 - val_loss: 1563.2015\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1557.2400 - val_loss: 1507.3052\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1503.4703 - val_loss: 1453.0255\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1449.8252 - val_loss: 1410.0910\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1399.4150 - val_loss: 1348.8663\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1350.5231 - val_loss: 1300.2838\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1307.3945 - val_loss: 1265.1637\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1262.1118 - val_loss: 1210.3208\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1217.4991 - val_loss: 1177.5505\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1178.7068 - val_loss: 1143.3781\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1138.6602 - val_loss: 1097.9122\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1102.8658 - val_loss: 1057.6045\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1067.3774 - val_loss: 1031.6681\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1033.8710 - val_loss: 1001.7341\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1000.3580 - val_loss: 962.1160\n",
      "Epoch 34/50\n",
      " - 0s - loss: 970.2171 - val_loss: 931.6473\n",
      "Epoch 35/50\n",
      " - 0s - loss: 942.3271 - val_loss: 907.6969\n",
      "Epoch 36/50\n",
      " - 0s - loss: 912.4087 - val_loss: 874.8271\n",
      "Epoch 37/50\n",
      " - 0s - loss: 882.3669 - val_loss: 853.9745\n",
      "Epoch 38/50\n",
      " - 0s - loss: 855.1140 - val_loss: 823.3504\n",
      "Epoch 39/50\n",
      " - 0s - loss: 829.3918 - val_loss: 800.9566\n",
      "Epoch 40/50\n",
      " - 0s - loss: 805.0489 - val_loss: 775.8900\n",
      "Epoch 41/50\n",
      " - 0s - loss: 779.4287 - val_loss: 750.2825\n",
      "Epoch 42/50\n",
      " - 0s - loss: 756.9336 - val_loss: 728.5438\n",
      "Epoch 43/50\n",
      " - 0s - loss: 736.6793 - val_loss: 712.8559\n",
      "Epoch 44/50\n",
      " - 0s - loss: 713.3769 - val_loss: 686.0814\n",
      "Epoch 45/50\n",
      " - 0s - loss: 693.7260 - val_loss: 665.9368\n",
      "Epoch 46/50\n",
      " - 0s - loss: 672.7123 - val_loss: 649.3633\n",
      "Epoch 47/50\n",
      " - 0s - loss: 654.4966 - val_loss: 630.2821\n",
      "Epoch 48/50\n",
      " - 0s - loss: 636.1734 - val_loss: 609.6265\n",
      "Epoch 49/50\n",
      " - 0s - loss: 619.1406 - val_loss: 596.8031\n",
      "Epoch 50/50\n",
      " - 0s - loss: 600.8217 - val_loss: 576.4533\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 6768.4935 - val_loss: 4436.5460\n",
      "Epoch 2/50\n",
      " - 0s - loss: 4354.2681 - val_loss: 3450.9406\n",
      "Epoch 3/50\n",
      " - 0s - loss: 3212.1534 - val_loss: 2658.0235\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2542.3392 - val_loss: 2299.6572\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2164.2411 - val_loss: 2027.1247\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1910.2982 - val_loss: 1819.9137\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1681.3936 - val_loss: 1631.5156\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1523.7619 - val_loss: 1463.4133\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1345.8669 - val_loss: 1315.1409\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1195.2418 - val_loss: 1173.6868\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1072.1160 - val_loss: 1055.6016\n",
      "Epoch 12/50\n",
      " - 0s - loss: 958.5814 - val_loss: 938.3825\n",
      "Epoch 13/50\n",
      " - 0s - loss: 863.5309 - val_loss: 844.8810\n",
      "Epoch 14/50\n",
      " - 0s - loss: 769.8681 - val_loss: 772.9285\n",
      "Epoch 15/50\n",
      " - 0s - loss: 693.8041 - val_loss: 676.8296\n",
      "Epoch 16/50\n",
      " - 0s - loss: 620.8415 - val_loss: 616.3254\n",
      "Epoch 17/50\n",
      " - 0s - loss: 560.4725 - val_loss: 537.3334\n",
      "Epoch 18/50\n",
      " - 0s - loss: 497.8805 - val_loss: 465.0447\n",
      "Epoch 19/50\n",
      " - 0s - loss: 437.6062 - val_loss: 407.0860\n",
      "Epoch 20/50\n",
      " - 0s - loss: 393.1009 - val_loss: 359.8022\n",
      "Epoch 21/50\n",
      " - 0s - loss: 342.8245 - val_loss: 324.8704\n",
      "Epoch 22/50\n",
      " - 0s - loss: 315.7321 - val_loss: 294.7779\n",
      "Epoch 23/50\n",
      " - 0s - loss: 280.3132 - val_loss: 261.2000\n",
      "Epoch 24/50\n",
      " - 0s - loss: 255.3089 - val_loss: 241.7868\n",
      "Epoch 25/50\n",
      " - 0s - loss: 244.7071 - val_loss: 230.3026\n",
      "Epoch 26/50\n",
      " - 0s - loss: 226.6856 - val_loss: 209.8540\n",
      "Epoch 27/50\n",
      " - 0s - loss: 215.4975 - val_loss: 204.5995\n",
      "Epoch 28/50\n",
      " - 0s - loss: 204.4203 - val_loss: 189.8067\n",
      "Epoch 29/50\n",
      " - 0s - loss: 195.2893 - val_loss: 181.6722\n",
      "Epoch 30/50\n",
      " - 0s - loss: 189.0769 - val_loss: 176.1710\n",
      "Epoch 31/50\n",
      " - 0s - loss: 183.1340 - val_loss: 170.9349\n",
      "Epoch 32/50\n",
      " - 0s - loss: 179.3138 - val_loss: 166.5319\n",
      "Epoch 33/50\n",
      " - 0s - loss: 176.1275 - val_loss: 163.5001\n",
      "Epoch 34/50\n",
      " - 0s - loss: 171.4752 - val_loss: 160.1224\n",
      "Epoch 35/50\n",
      " - 0s - loss: 165.7615 - val_loss: 158.8702\n",
      "Epoch 36/50\n",
      " - 0s - loss: 166.0880 - val_loss: 151.8415\n",
      "Epoch 37/50\n",
      " - 0s - loss: 161.7506 - val_loss: 149.1185\n",
      "Epoch 38/50\n",
      " - 0s - loss: 162.3101 - val_loss: 149.7060\n",
      "Epoch 39/50\n",
      " - 0s - loss: 156.6474 - val_loss: 148.4900\n",
      "Epoch 40/50\n",
      " - 0s - loss: 154.2456 - val_loss: 149.6357\n",
      "Epoch 41/50\n",
      " - 0s - loss: 152.1916 - val_loss: 141.8443\n",
      "Epoch 42/50\n",
      " - 0s - loss: 147.7616 - val_loss: 139.7880\n",
      "Epoch 43/50\n",
      " - 0s - loss: 147.9457 - val_loss: 136.5203\n",
      "Epoch 44/50\n",
      " - 0s - loss: 141.7899 - val_loss: 138.3674\n",
      "Epoch 45/50\n",
      " - 0s - loss: 141.3772 - val_loss: 133.4664\n",
      "Epoch 46/50\n",
      " - 0s - loss: 139.7059 - val_loss: 139.9557\n",
      "Epoch 47/50\n",
      " - 0s - loss: 135.7960 - val_loss: 134.1761\n",
      "Epoch 48/50\n",
      " - 0s - loss: 138.2396 - val_loss: 129.7682\n",
      "Epoch 49/50\n",
      " - 0s - loss: 133.5351 - val_loss: 128.8703\n",
      "Epoch 50/50\n",
      " - 0s - loss: 131.0953 - val_loss: 126.0076\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 46500.9502 - val_loss: 34026.3273\n",
      "Epoch 2/50\n",
      " - 0s - loss: 26244.0624 - val_loss: 18983.8427\n",
      "Epoch 3/50\n",
      " - 0s - loss: 14796.5135 - val_loss: 10878.8594\n",
      "Epoch 4/50\n",
      " - 0s - loss: 8741.9776 - val_loss: 6587.6318\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5533.3197 - val_loss: 4259.2301\n",
      "Epoch 6/50\n",
      " - 0s - loss: 3688.6301 - val_loss: 2919.3584\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2592.7244 - val_loss: 2090.1162\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1898.1498 - val_loss: 1549.1893\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1431.0891 - val_loss: 1191.8838\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1114.6550 - val_loss: 948.5700\n",
      "Epoch 11/50\n",
      " - 0s - loss: 903.6547 - val_loss: 776.4028\n",
      "Epoch 12/50\n",
      " - 0s - loss: 749.6548 - val_loss: 659.8484\n",
      "Epoch 13/50\n",
      " - 0s - loss: 643.8639 - val_loss: 575.1053\n",
      "Epoch 14/50\n",
      " - 0s - loss: 565.9991 - val_loss: 512.0006\n",
      "Epoch 15/50\n",
      " - 0s - loss: 509.7166 - val_loss: 465.0788\n",
      "Epoch 16/50\n",
      " - 0s - loss: 467.9943 - val_loss: 431.0724\n",
      "Epoch 17/50\n",
      " - 0s - loss: 437.0122 - val_loss: 406.0973\n",
      "Epoch 18/50\n",
      " - 0s - loss: 413.0904 - val_loss: 388.1454\n",
      "Epoch 19/50\n",
      " - 0s - loss: 396.0207 - val_loss: 373.4834\n",
      "Epoch 20/50\n",
      " - 0s - loss: 381.5072 - val_loss: 362.6028\n",
      "Epoch 21/50\n",
      " - 0s - loss: 370.7417 - val_loss: 352.5386\n",
      "Epoch 22/50\n",
      " - 0s - loss: 360.3842 - val_loss: 344.8403\n",
      "Epoch 23/50\n",
      " - 0s - loss: 351.9902 - val_loss: 338.5498\n",
      "Epoch 24/50\n",
      " - 0s - loss: 345.2724 - val_loss: 333.0976\n",
      "Epoch 25/50\n",
      " - 0s - loss: 338.9036 - val_loss: 329.3712\n",
      "Epoch 26/50\n",
      " - 0s - loss: 334.6263 - val_loss: 325.7969\n",
      "Epoch 27/50\n",
      " - 0s - loss: 330.1002 - val_loss: 322.8397\n",
      "Epoch 28/50\n",
      " - 0s - loss: 326.5346 - val_loss: 320.2610\n",
      "Epoch 29/50\n",
      " - 0s - loss: 323.0239 - val_loss: 317.7282\n",
      "Epoch 30/50\n",
      " - 0s - loss: 319.7996 - val_loss: 315.2169\n",
      "Epoch 31/50\n",
      " - 0s - loss: 316.7970 - val_loss: 313.1944\n",
      "Epoch 32/50\n",
      " - 0s - loss: 314.5580 - val_loss: 310.9157\n",
      "Epoch 33/50\n",
      " - 0s - loss: 311.7206 - val_loss: 308.8978\n",
      "Epoch 34/50\n",
      " - 0s - loss: 309.3535 - val_loss: 306.6806\n",
      "Epoch 35/50\n",
      " - 0s - loss: 306.9044 - val_loss: 304.5921\n",
      "Epoch 36/50\n",
      " - 0s - loss: 304.0775 - val_loss: 302.9383\n",
      "Epoch 37/50\n",
      " - 0s - loss: 301.7225 - val_loss: 301.0208\n",
      "Epoch 38/50\n",
      " - 0s - loss: 299.3163 - val_loss: 299.1342\n",
      "Epoch 39/50\n",
      " - 0s - loss: 296.9706 - val_loss: 297.3321\n",
      "Epoch 40/50\n",
      " - 0s - loss: 294.8808 - val_loss: 295.7660\n",
      "Epoch 41/50\n",
      " - 0s - loss: 293.1264 - val_loss: 294.2387\n",
      "Epoch 42/50\n",
      " - 0s - loss: 291.1527 - val_loss: 292.8052\n",
      "Epoch 43/50\n",
      " - 0s - loss: 289.2722 - val_loss: 291.4446\n",
      "Epoch 44/50\n",
      " - 0s - loss: 287.2948 - val_loss: 290.2541\n",
      "Epoch 45/50\n",
      " - 0s - loss: 285.6729 - val_loss: 288.8377\n",
      "Epoch 46/50\n",
      " - 0s - loss: 283.9241 - val_loss: 287.7288\n",
      "Epoch 47/50\n",
      " - 0s - loss: 282.4319 - val_loss: 286.5064\n",
      "Epoch 48/50\n",
      " - 0s - loss: 280.8102 - val_loss: 285.3498\n",
      "Epoch 49/50\n",
      " - 0s - loss: 279.2043 - val_loss: 284.3440\n",
      "Epoch 50/50\n",
      " - 0s - loss: 277.8944 - val_loss: 283.3256\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 321933.7994 - val_loss: 250147.3293\n",
      "Epoch 2/50\n",
      " - 0s - loss: 212492.5193 - val_loss: 161655.7450\n",
      "Epoch 3/50\n",
      " - 0s - loss: 136619.1076 - val_loss: 102238.8061\n",
      "Epoch 4/50\n",
      " - 0s - loss: 86396.5053 - val_loss: 63292.5868\n",
      "Epoch 5/50\n",
      " - 0s - loss: 53306.6295 - val_loss: 37936.8452\n",
      "Epoch 6/50\n",
      " - 0s - loss: 31944.4974 - val_loss: 21747.4271\n",
      "Epoch 7/50\n",
      " - 0s - loss: 18384.1352 - val_loss: 11937.5174\n",
      "Epoch 8/50\n",
      " - 0s - loss: 10194.1207 - val_loss: 6275.7128\n",
      "Epoch 9/50\n",
      " - 0s - loss: 5443.5928 - val_loss: 3243.0161\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2946.5397 - val_loss: 1747.0944\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1713.3478 - val_loss: 1091.6479\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1169.5519 - val_loss: 840.9760\n",
      "Epoch 13/50\n",
      " - 0s - loss: 943.3253 - val_loss: 764.0496\n",
      "Epoch 14/50\n",
      " - 0s - loss: 849.0920 - val_loss: 744.0021\n",
      "Epoch 15/50\n",
      " - 0s - loss: 815.8361 - val_loss: 736.6294\n",
      "Epoch 16/50\n",
      " - 0s - loss: 794.6233 - val_loss: 729.7802\n",
      "Epoch 17/50\n",
      " - 0s - loss: 777.1835 - val_loss: 719.5391\n",
      "Epoch 18/50\n",
      " - 0s - loss: 762.5617 - val_loss: 708.5804\n",
      "Epoch 19/50\n",
      " - 0s - loss: 748.4784 - val_loss: 697.9475\n",
      "Epoch 20/50\n",
      " - 0s - loss: 733.2879 - val_loss: 683.4151\n",
      "Epoch 21/50\n",
      " - 0s - loss: 718.1868 - val_loss: 670.6576\n",
      "Epoch 22/50\n",
      " - 0s - loss: 704.9469 - val_loss: 655.1139\n",
      "Epoch 23/50\n",
      " - 0s - loss: 689.3275 - val_loss: 644.1871\n",
      "Epoch 24/50\n",
      " - 0s - loss: 676.9607 - val_loss: 628.2921\n",
      "Epoch 25/50\n",
      " - 0s - loss: 661.5409 - val_loss: 617.3911\n",
      "Epoch 26/50\n",
      " - 0s - loss: 647.4141 - val_loss: 607.9743\n",
      "Epoch 27/50\n",
      " - 0s - loss: 634.0349 - val_loss: 595.1939\n",
      "Epoch 28/50\n",
      " - 0s - loss: 620.3583 - val_loss: 579.9694\n",
      "Epoch 29/50\n",
      " - 0s - loss: 607.0352 - val_loss: 568.1369\n",
      "Epoch 30/50\n",
      " - 0s - loss: 593.3275 - val_loss: 558.4990\n",
      "Epoch 31/50\n",
      " - 0s - loss: 581.0379 - val_loss: 546.7447\n",
      "Epoch 32/50\n",
      " - 0s - loss: 568.3331 - val_loss: 534.0919\n",
      "Epoch 33/50\n",
      " - 0s - loss: 557.6260 - val_loss: 526.7555\n",
      "Epoch 34/50\n",
      " - 0s - loss: 543.8011 - val_loss: 511.9069\n",
      "Epoch 35/50\n",
      " - 0s - loss: 532.0957 - val_loss: 500.6707\n",
      "Epoch 36/50\n",
      " - 0s - loss: 520.7384 - val_loss: 489.7827\n",
      "Epoch 37/50\n",
      " - 0s - loss: 509.2851 - val_loss: 479.6104\n",
      "Epoch 38/50\n",
      " - 0s - loss: 498.8705 - val_loss: 472.5667\n",
      "Epoch 39/50\n",
      " - 0s - loss: 487.8356 - val_loss: 460.7982\n",
      "Epoch 40/50\n",
      " - 0s - loss: 477.1579 - val_loss: 450.2216\n",
      "Epoch 41/50\n",
      " - 0s - loss: 467.3995 - val_loss: 440.3108\n",
      "Epoch 42/50\n",
      " - 0s - loss: 456.7953 - val_loss: 431.6677\n",
      "Epoch 43/50\n",
      " - 0s - loss: 447.3921 - val_loss: 421.8991\n",
      "Epoch 44/50\n",
      " - 0s - loss: 437.9895 - val_loss: 411.6848\n",
      "Epoch 45/50\n",
      " - 0s - loss: 428.3608 - val_loss: 403.2879\n",
      "Epoch 46/50\n",
      " - 0s - loss: 419.3293 - val_loss: 397.4296\n",
      "Epoch 47/50\n",
      " - 0s - loss: 410.6083 - val_loss: 388.5272\n",
      "Epoch 48/50\n",
      " - 0s - loss: 401.6365 - val_loss: 379.1278\n",
      "Epoch 49/50\n",
      " - 0s - loss: 393.7301 - val_loss: 369.9405\n",
      "Epoch 50/50\n",
      " - 0s - loss: 385.5573 - val_loss: 364.3051\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 5172.7659 - val_loss: 5063.3131\n",
      "Epoch 2/50\n",
      " - 0s - loss: 4239.0200 - val_loss: 4082.3900\n",
      "Epoch 3/50\n",
      " - 0s - loss: 3652.1517 - val_loss: 3449.4989\n",
      "Epoch 4/50\n",
      " - 0s - loss: 3123.7788 - val_loss: 3099.5310\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2703.7135 - val_loss: 2579.4123\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2341.3173 - val_loss: 2256.3929\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2042.4650 - val_loss: 1997.6077\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1792.4341 - val_loss: 1748.8823\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1599.5032 - val_loss: 1545.0484\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1394.2220 - val_loss: 1361.7062\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1244.5956 - val_loss: 1218.0950\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1134.4042 - val_loss: 1080.5635\n",
      "Epoch 13/50\n",
      " - 0s - loss: 986.4792 - val_loss: 965.4107\n",
      "Epoch 14/50\n",
      " - 0s - loss: 889.3618 - val_loss: 868.9048\n",
      "Epoch 15/50\n",
      " - 0s - loss: 807.9741 - val_loss: 790.8260\n",
      "Epoch 16/50\n",
      " - 0s - loss: 721.0932 - val_loss: 712.7896\n",
      "Epoch 17/50\n",
      " - 0s - loss: 650.3086 - val_loss: 643.6593\n",
      "Epoch 18/50\n",
      " - 0s - loss: 592.1662 - val_loss: 585.1310\n",
      "Epoch 19/50\n",
      " - 0s - loss: 541.2749 - val_loss: 536.5111\n",
      "Epoch 20/50\n",
      " - 0s - loss: 497.7285 - val_loss: 500.9733\n",
      "Epoch 21/50\n",
      " - 0s - loss: 460.7152 - val_loss: 459.8109\n",
      "Epoch 22/50\n",
      " - 0s - loss: 423.0890 - val_loss: 418.7438\n",
      "Epoch 23/50\n",
      " - 0s - loss: 384.2088 - val_loss: 388.7147\n",
      "Epoch 24/50\n",
      " - 0s - loss: 357.8922 - val_loss: 360.1873\n",
      "Epoch 25/50\n",
      " - 0s - loss: 335.7597 - val_loss: 334.0889\n",
      "Epoch 26/50\n",
      " - 0s - loss: 312.7414 - val_loss: 311.2353\n",
      "Epoch 27/50\n",
      " - 0s - loss: 294.2941 - val_loss: 304.8811\n",
      "Epoch 28/50\n",
      " - 0s - loss: 279.4916 - val_loss: 276.9330\n",
      "Epoch 29/50\n",
      " - 0s - loss: 259.2760 - val_loss: 257.6969\n",
      "Epoch 30/50\n",
      " - 0s - loss: 245.1808 - val_loss: 241.3268\n",
      "Epoch 31/50\n",
      " - 0s - loss: 234.6985 - val_loss: 228.8273\n",
      "Epoch 32/50\n",
      " - 0s - loss: 222.7067 - val_loss: 215.6327\n",
      "Epoch 33/50\n",
      " - 0s - loss: 213.2716 - val_loss: 205.9150\n",
      "Epoch 34/50\n",
      " - 0s - loss: 203.4173 - val_loss: 200.4240\n",
      "Epoch 35/50\n",
      " - 0s - loss: 195.3087 - val_loss: 187.5494\n",
      "Epoch 36/50\n",
      " - 0s - loss: 189.2025 - val_loss: 181.1464\n",
      "Epoch 37/50\n",
      " - 0s - loss: 181.1656 - val_loss: 172.5277\n",
      "Epoch 38/50\n",
      " - 0s - loss: 175.1451 - val_loss: 168.5160\n",
      "Epoch 39/50\n",
      " - 0s - loss: 172.9058 - val_loss: 161.3103\n",
      "Epoch 40/50\n",
      " - 0s - loss: 164.9187 - val_loss: 159.9746\n",
      "Epoch 41/50\n",
      " - 0s - loss: 162.2214 - val_loss: 155.7695\n",
      "Epoch 42/50\n",
      " - 0s - loss: 161.9010 - val_loss: 155.3112\n",
      "Epoch 43/50\n",
      " - 0s - loss: 157.4464 - val_loss: 145.4035\n",
      "Epoch 44/50\n",
      " - 0s - loss: 151.4505 - val_loss: 140.8863\n",
      "Epoch 45/50\n",
      " - 0s - loss: 150.1182 - val_loss: 137.5424\n",
      "Epoch 46/50\n",
      " - 0s - loss: 145.4033 - val_loss: 142.4365\n",
      "Epoch 47/50\n",
      " - 0s - loss: 150.2268 - val_loss: 143.0894\n",
      "Epoch 48/50\n",
      " - 0s - loss: 145.7597 - val_loss: 131.3382\n",
      "Epoch 49/50\n",
      " - 0s - loss: 140.5058 - val_loss: 127.7604\n",
      "Epoch 50/50\n",
      " - 0s - loss: 141.7562 - val_loss: 135.8671\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 118651.1744 - val_loss: 82455.8367\n",
      "Epoch 2/50\n",
      " - 0s - loss: 58161.8454 - val_loss: 37614.8939\n",
      "Epoch 3/50\n",
      " - 0s - loss: 25405.9757 - val_loss: 15657.7467\n",
      "Epoch 4/50\n",
      " - 0s - loss: 10484.1430 - val_loss: 6345.6936\n",
      "Epoch 5/50\n",
      " - 0s - loss: 4526.1420 - val_loss: 2889.9834\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2433.8218 - val_loss: 1787.0256\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1809.4277 - val_loss: 1476.7769\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1609.4281 - val_loss: 1395.6590\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1537.5904 - val_loss: 1365.6887\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1503.7402 - val_loss: 1342.2080\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1469.9042 - val_loss: 1309.6653\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1438.7420 - val_loss: 1278.0026\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1408.9218 - val_loss: 1250.5844\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1374.8647 - val_loss: 1219.0516\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1343.8290 - val_loss: 1188.0069\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1312.3505 - val_loss: 1158.5298\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1282.8630 - val_loss: 1128.6325\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1253.0366 - val_loss: 1099.5523\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1223.7871 - val_loss: 1072.0625\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1196.2297 - val_loss: 1043.5012\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1169.8734 - val_loss: 1020.8794\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1140.9124 - val_loss: 993.3483\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1115.6737 - val_loss: 968.2568\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1089.3010 - val_loss: 946.2375\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1066.5022 - val_loss: 919.5304\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1041.9001 - val_loss: 900.3723\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1019.2638 - val_loss: 882.4047\n",
      "Epoch 28/50\n",
      " - 0s - loss: 996.9754 - val_loss: 855.8789\n",
      "Epoch 29/50\n",
      " - 0s - loss: 974.8114 - val_loss: 836.5472\n",
      "Epoch 30/50\n",
      " - 0s - loss: 951.4173 - val_loss: 817.3586\n",
      "Epoch 31/50\n",
      " - 0s - loss: 930.0208 - val_loss: 798.7842\n",
      "Epoch 32/50\n",
      " - 0s - loss: 910.7517 - val_loss: 778.6871\n",
      "Epoch 33/50\n",
      " - 0s - loss: 889.6326 - val_loss: 758.4040\n",
      "Epoch 34/50\n",
      " - 0s - loss: 870.9480 - val_loss: 741.0265\n",
      "Epoch 35/50\n",
      " - 0s - loss: 850.5831 - val_loss: 722.9309\n",
      "Epoch 36/50\n",
      " - 0s - loss: 830.2573 - val_loss: 706.7575\n",
      "Epoch 37/50\n",
      " - 0s - loss: 811.1325 - val_loss: 688.7533\n",
      "Epoch 38/50\n",
      " - 0s - loss: 794.7273 - val_loss: 671.1905\n",
      "Epoch 39/50\n",
      " - 0s - loss: 774.2692 - val_loss: 654.2868\n",
      "Epoch 40/50\n",
      " - 0s - loss: 757.0920 - val_loss: 637.9814\n",
      "Epoch 41/50\n",
      " - 0s - loss: 738.3390 - val_loss: 623.3633\n",
      "Epoch 42/50\n",
      " - 0s - loss: 719.6663 - val_loss: 606.8191\n",
      "Epoch 43/50\n",
      " - 0s - loss: 701.3984 - val_loss: 593.7037\n",
      "Epoch 44/50\n",
      " - 0s - loss: 684.4752 - val_loss: 578.1345\n",
      "Epoch 45/50\n",
      " - 0s - loss: 666.9863 - val_loss: 562.7513\n",
      "Epoch 46/50\n",
      " - 0s - loss: 650.2182 - val_loss: 547.9814\n",
      "Epoch 47/50\n",
      " - 0s - loss: 633.3631 - val_loss: 534.9908\n",
      "Epoch 48/50\n",
      " - 0s - loss: 616.0195 - val_loss: 521.7774\n",
      "Epoch 49/50\n",
      " - 0s - loss: 600.2629 - val_loss: 507.9424\n",
      "Epoch 50/50\n",
      " - 0s - loss: 583.2496 - val_loss: 498.3249\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 47264.9705 - val_loss: 32551.2191\n",
      "Epoch 2/50\n",
      " - 0s - loss: 24564.4717 - val_loss: 15877.4242\n",
      "Epoch 3/50\n",
      " - 0s - loss: 11770.6381 - val_loss: 7046.5017\n",
      "Epoch 4/50\n",
      " - 0s - loss: 4869.3245 - val_loss: 2762.3071\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2026.2204 - val_loss: 1518.6629\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1413.1177 - val_loss: 1285.7469\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1232.3865 - val_loss: 1135.9191\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1112.2142 - val_loss: 1032.9304\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1018.6207 - val_loss: 943.2716\n",
      "Epoch 10/50\n",
      " - 0s - loss: 936.7266 - val_loss: 864.9700\n",
      "Epoch 11/50\n",
      " - 0s - loss: 863.3178 - val_loss: 792.1918\n",
      "Epoch 12/50\n",
      " - 0s - loss: 791.9825 - val_loss: 733.6274\n",
      "Epoch 13/50\n",
      " - 0s - loss: 732.6471 - val_loss: 678.0869\n",
      "Epoch 14/50\n",
      " - 0s - loss: 678.4476 - val_loss: 626.7161\n",
      "Epoch 15/50\n",
      " - 0s - loss: 626.0381 - val_loss: 587.2384\n",
      "Epoch 16/50\n",
      " - 0s - loss: 578.7073 - val_loss: 541.1963\n",
      "Epoch 17/50\n",
      " - 0s - loss: 535.6810 - val_loss: 504.4229\n",
      "Epoch 18/50\n",
      " - 0s - loss: 498.3696 - val_loss: 470.2400\n",
      "Epoch 19/50\n",
      " - 0s - loss: 461.5441 - val_loss: 437.0610\n",
      "Epoch 20/50\n",
      " - 0s - loss: 430.7982 - val_loss: 407.7947\n",
      "Epoch 21/50\n",
      " - 0s - loss: 400.7807 - val_loss: 383.6266\n",
      "Epoch 22/50\n",
      " - 0s - loss: 375.0058 - val_loss: 360.1314\n",
      "Epoch 23/50\n",
      " - 0s - loss: 351.6338 - val_loss: 339.5079\n",
      "Epoch 24/50\n",
      " - 0s - loss: 331.5245 - val_loss: 318.3073\n",
      "Epoch 25/50\n",
      " - 0s - loss: 312.2254 - val_loss: 301.5602\n",
      "Epoch 26/50\n",
      " - 0s - loss: 292.8893 - val_loss: 284.5322\n",
      "Epoch 27/50\n",
      " - 0s - loss: 276.8197 - val_loss: 270.8368\n",
      "Epoch 28/50\n",
      " - 0s - loss: 263.1388 - val_loss: 258.1895\n",
      "Epoch 29/50\n",
      " - 0s - loss: 250.8373 - val_loss: 246.1958\n",
      "Epoch 30/50\n",
      " - 0s - loss: 239.7396 - val_loss: 235.3969\n",
      "Epoch 31/50\n",
      " - 0s - loss: 230.0087 - val_loss: 228.4074\n",
      "Epoch 32/50\n",
      " - 0s - loss: 219.8108 - val_loss: 218.4662\n",
      "Epoch 33/50\n",
      " - 0s - loss: 213.1957 - val_loss: 210.8489\n",
      "Epoch 34/50\n",
      " - 0s - loss: 205.1889 - val_loss: 205.1121\n",
      "Epoch 35/50\n",
      " - 0s - loss: 198.5947 - val_loss: 197.7988\n",
      "Epoch 36/50\n",
      " - 0s - loss: 193.4212 - val_loss: 192.0560\n",
      "Epoch 37/50\n",
      " - 0s - loss: 189.8284 - val_loss: 188.5160\n",
      "Epoch 38/50\n",
      " - 0s - loss: 183.1029 - val_loss: 182.2929\n",
      "Epoch 39/50\n",
      " - 0s - loss: 179.0395 - val_loss: 178.0585\n",
      "Epoch 40/50\n",
      " - 0s - loss: 174.6810 - val_loss: 174.1799\n",
      "Epoch 41/50\n",
      " - 0s - loss: 170.9681 - val_loss: 170.6229\n",
      "Epoch 42/50\n",
      " - 0s - loss: 167.9536 - val_loss: 167.5245\n",
      "Epoch 43/50\n",
      " - 0s - loss: 164.6116 - val_loss: 163.9501\n",
      "Epoch 44/50\n",
      " - 0s - loss: 161.8448 - val_loss: 161.2222\n",
      "Epoch 45/50\n",
      " - 0s - loss: 159.5584 - val_loss: 158.8346\n",
      "Epoch 46/50\n",
      " - 0s - loss: 156.9863 - val_loss: 156.3557\n",
      "Epoch 47/50\n",
      " - 0s - loss: 153.8660 - val_loss: 154.7988\n",
      "Epoch 48/50\n",
      " - 0s - loss: 151.7500 - val_loss: 152.8892\n",
      "Epoch 49/50\n",
      " - 0s - loss: 149.1635 - val_loss: 150.6564\n",
      "Epoch 50/50\n",
      " - 0s - loss: 146.8848 - val_loss: 148.7078\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 20007.6699 - val_loss: 7493.6232\n",
      "Epoch 2/50\n",
      " - 0s - loss: 5032.6002 - val_loss: 1758.9918\n",
      "Epoch 3/50\n",
      " - 0s - loss: 2332.6773 - val_loss: 1821.0869\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2148.7758 - val_loss: 1578.1818\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1888.6866 - val_loss: 1326.0375\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1701.5607 - val_loss: 1192.8202\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1526.4626 - val_loss: 1081.3269\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1370.5756 - val_loss: 983.5506\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1224.6985 - val_loss: 882.9154\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1100.7800 - val_loss: 805.2389\n",
      "Epoch 11/50\n",
      " - 0s - loss: 975.6788 - val_loss: 706.7116\n",
      "Epoch 12/50\n",
      " - 0s - loss: 869.5358 - val_loss: 648.7803\n",
      "Epoch 13/50\n",
      " - 0s - loss: 779.5088 - val_loss: 593.8831\n",
      "Epoch 14/50\n",
      " - 0s - loss: 697.2290 - val_loss: 532.2602\n",
      "Epoch 15/50\n",
      " - 0s - loss: 626.1394 - val_loss: 496.6685\n",
      "Epoch 16/50\n",
      " - 0s - loss: 565.2334 - val_loss: 446.7081\n",
      "Epoch 17/50\n",
      " - 0s - loss: 511.5619 - val_loss: 420.2388\n",
      "Epoch 18/50\n",
      " - 0s - loss: 464.5464 - val_loss: 383.7999\n",
      "Epoch 19/50\n",
      " - 0s - loss: 426.4642 - val_loss: 361.7397\n",
      "Epoch 20/50\n",
      " - 0s - loss: 392.6313 - val_loss: 344.0938\n",
      "Epoch 21/50\n",
      " - 0s - loss: 365.8483 - val_loss: 321.6579\n",
      "Epoch 22/50\n",
      " - 0s - loss: 340.8073 - val_loss: 307.9252\n",
      "Epoch 23/50\n",
      " - 0s - loss: 320.9425 - val_loss: 298.9666\n",
      "Epoch 24/50\n",
      " - 0s - loss: 303.3971 - val_loss: 283.2830\n",
      "Epoch 25/50\n",
      " - 0s - loss: 291.8043 - val_loss: 273.9594\n",
      "Epoch 26/50\n",
      " - 0s - loss: 278.3373 - val_loss: 264.2979\n",
      "Epoch 27/50\n",
      " - 0s - loss: 266.8607 - val_loss: 261.1088\n",
      "Epoch 28/50\n",
      " - 0s - loss: 256.8590 - val_loss: 249.2949\n",
      "Epoch 29/50\n",
      " - 0s - loss: 249.0243 - val_loss: 244.8540\n",
      "Epoch 30/50\n",
      " - 0s - loss: 241.2058 - val_loss: 240.4191\n",
      "Epoch 31/50\n",
      " - 0s - loss: 235.0160 - val_loss: 233.1930\n",
      "Epoch 32/50\n",
      " - 0s - loss: 229.1818 - val_loss: 228.9298\n",
      "Epoch 33/50\n",
      " - 0s - loss: 224.9506 - val_loss: 223.3070\n",
      "Epoch 34/50\n",
      " - 0s - loss: 219.4018 - val_loss: 220.4740\n",
      "Epoch 35/50\n",
      " - 0s - loss: 214.9548 - val_loss: 214.5259\n",
      "Epoch 36/50\n",
      " - 0s - loss: 211.5414 - val_loss: 211.5586\n",
      "Epoch 37/50\n",
      " - 0s - loss: 208.1329 - val_loss: 204.2581\n",
      "Epoch 38/50\n",
      " - 0s - loss: 203.6326 - val_loss: 202.3307\n",
      "Epoch 39/50\n",
      " - 0s - loss: 200.4789 - val_loss: 199.8162\n",
      "Epoch 40/50\n",
      " - 0s - loss: 197.7220 - val_loss: 193.2084\n",
      "Epoch 41/50\n",
      " - 0s - loss: 194.7844 - val_loss: 191.2601\n",
      "Epoch 42/50\n",
      " - 0s - loss: 190.8068 - val_loss: 185.8756\n",
      "Epoch 43/50\n",
      " - 0s - loss: 188.7296 - val_loss: 184.7852\n",
      "Epoch 44/50\n",
      " - 0s - loss: 185.4973 - val_loss: 179.3512\n",
      "Epoch 45/50\n",
      " - 0s - loss: 182.4505 - val_loss: 177.0353\n",
      "Epoch 46/50\n",
      " - 0s - loss: 180.1925 - val_loss: 174.4403\n",
      "Epoch 47/50\n",
      " - 0s - loss: 177.5070 - val_loss: 170.6911\n",
      "Epoch 48/50\n",
      " - 0s - loss: 175.6115 - val_loss: 168.7475\n",
      "Epoch 49/50\n",
      " - 0s - loss: 174.2435 - val_loss: 163.4063\n",
      "Epoch 50/50\n",
      " - 0s - loss: 171.5898 - val_loss: 162.3132\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 150928.7953 - val_loss: 115185.2377\n",
      "Epoch 2/50\n",
      " - 0s - loss: 88519.4541 - val_loss: 62343.3371\n",
      "Epoch 3/50\n",
      " - 0s - loss: 44949.3098 - val_loss: 28624.7210\n",
      "Epoch 4/50\n",
      " - 0s - loss: 19649.8334 - val_loss: 11572.6986\n",
      "Epoch 5/50\n",
      " - 0s - loss: 8399.6619 - val_loss: 5367.2441\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5123.4610 - val_loss: 3812.7839\n",
      "Epoch 7/50\n",
      " - 0s - loss: 4449.6534 - val_loss: 3536.2411\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4303.7948 - val_loss: 3415.2685\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4169.1624 - val_loss: 3305.7141\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4029.1695 - val_loss: 3197.6307\n",
      "Epoch 11/50\n",
      " - 0s - loss: 3888.4763 - val_loss: 3088.4856\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3749.6407 - val_loss: 2979.4161\n",
      "Epoch 13/50\n",
      " - 0s - loss: 3615.9506 - val_loss: 2871.4268\n",
      "Epoch 14/50\n",
      " - 0s - loss: 3490.1863 - val_loss: 2755.1540\n",
      "Epoch 15/50\n",
      " - 0s - loss: 3349.3349 - val_loss: 2657.9963\n",
      "Epoch 16/50\n",
      " - 0s - loss: 3220.3951 - val_loss: 2552.3366\n",
      "Epoch 17/50\n",
      " - 0s - loss: 3099.7713 - val_loss: 2458.3005\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2974.5286 - val_loss: 2357.1573\n",
      "Epoch 19/50\n",
      " - 0s - loss: 2852.4843 - val_loss: 2263.6909\n",
      "Epoch 20/50\n",
      " - 0s - loss: 2738.7695 - val_loss: 2165.9170\n",
      "Epoch 21/50\n",
      " - 0s - loss: 2623.0600 - val_loss: 2079.6790\n",
      "Epoch 22/50\n",
      " - 0s - loss: 2516.5629 - val_loss: 1996.5689\n",
      "Epoch 23/50\n",
      " - 0s - loss: 2409.3619 - val_loss: 1922.6479\n",
      "Epoch 24/50\n",
      " - 0s - loss: 2313.1785 - val_loss: 1839.2893\n",
      "Epoch 25/50\n",
      " - 0s - loss: 2214.0745 - val_loss: 1764.8978\n",
      "Epoch 26/50\n",
      " - 0s - loss: 2124.0225 - val_loss: 1694.4509\n",
      "Epoch 27/50\n",
      " - 0s - loss: 2035.2421 - val_loss: 1627.3573\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1953.2942 - val_loss: 1558.6394\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1865.3097 - val_loss: 1499.2921\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1792.3883 - val_loss: 1445.8244\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1713.6195 - val_loss: 1380.9335\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1640.2089 - val_loss: 1325.8954\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1571.5952 - val_loss: 1273.7996\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1505.9829 - val_loss: 1223.0458\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1452.9858 - val_loss: 1173.6087\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1379.0677 - val_loss: 1134.6034\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1323.8832 - val_loss: 1091.5099\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1269.9099 - val_loss: 1046.1635\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1214.9018 - val_loss: 1006.2470\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1165.4628 - val_loss: 967.9454\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1115.9910 - val_loss: 930.4978\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1071.3524 - val_loss: 896.2574\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1028.1213 - val_loss: 862.8444\n",
      "Epoch 44/50\n",
      " - 0s - loss: 989.6608 - val_loss: 831.9006\n",
      "Epoch 45/50\n",
      " - 0s - loss: 946.6841 - val_loss: 799.0071\n",
      "Epoch 46/50\n",
      " - 0s - loss: 907.6714 - val_loss: 771.8160\n",
      "Epoch 47/50\n",
      " - 0s - loss: 872.6720 - val_loss: 743.9700\n",
      "Epoch 48/50\n",
      " - 0s - loss: 836.9107 - val_loss: 716.8025\n",
      "Epoch 49/50\n",
      " - 0s - loss: 806.3846 - val_loss: 690.5395\n",
      "Epoch 50/50\n",
      " - 0s - loss: 770.7364 - val_loss: 668.0712\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 19910.8666 - val_loss: 7463.9982\n",
      "Epoch 2/50\n",
      " - 0s - loss: 6847.9851 - val_loss: 6295.4643\n",
      "Epoch 3/50\n",
      " - 0s - loss: 6645.5355 - val_loss: 5845.5857\n",
      "Epoch 4/50\n",
      " - 0s - loss: 5832.9813 - val_loss: 4975.2102\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5479.8409 - val_loss: 4617.4630\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5092.2403 - val_loss: 4433.6184\n",
      "Epoch 7/50\n",
      " - 0s - loss: 4711.7493 - val_loss: 4026.1170\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4389.1924 - val_loss: 3752.3244\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4076.1496 - val_loss: 3489.0321\n",
      "Epoch 10/50\n",
      " - 0s - loss: 3781.3023 - val_loss: 3262.6006\n",
      "Epoch 11/50\n",
      " - 0s - loss: 3565.9733 - val_loss: 3046.1253\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3307.5234 - val_loss: 2880.2675\n",
      "Epoch 13/50\n",
      " - 0s - loss: 3118.1856 - val_loss: 2639.2259\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2900.7179 - val_loss: 2542.8359\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2725.9214 - val_loss: 2396.7709\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2581.5750 - val_loss: 2234.2213\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2429.3751 - val_loss: 2153.4374\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2295.3417 - val_loss: 2024.4771\n",
      "Epoch 19/50\n",
      " - 0s - loss: 2170.8709 - val_loss: 1911.6290\n",
      "Epoch 20/50\n",
      " - 0s - loss: 2073.0565 - val_loss: 1837.6548\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1969.4959 - val_loss: 1755.5425\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1863.1538 - val_loss: 1672.8664\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1776.2498 - val_loss: 1594.4923\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1695.6939 - val_loss: 1528.8664\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1619.8005 - val_loss: 1473.8254\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1549.4025 - val_loss: 1420.7406\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1490.0154 - val_loss: 1345.7329\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1413.4982 - val_loss: 1315.4582\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1350.4786 - val_loss: 1241.1506\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1295.4834 - val_loss: 1192.4728\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1249.5096 - val_loss: 1145.3131\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1193.3622 - val_loss: 1097.0173\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1147.5340 - val_loss: 1059.3371\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1095.3876 - val_loss: 1018.6258\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1043.0365 - val_loss: 981.5013\n",
      "Epoch 36/50\n",
      " - 0s - loss: 998.0051 - val_loss: 928.9193\n",
      "Epoch 37/50\n",
      " - 0s - loss: 960.6502 - val_loss: 910.1602\n",
      "Epoch 38/50\n",
      " - 0s - loss: 917.4615 - val_loss: 856.7575\n",
      "Epoch 39/50\n",
      " - 0s - loss: 875.8843 - val_loss: 825.1503\n",
      "Epoch 40/50\n",
      " - 0s - loss: 835.8634 - val_loss: 789.1210\n",
      "Epoch 41/50\n",
      " - 0s - loss: 802.4524 - val_loss: 760.6112\n",
      "Epoch 42/50\n",
      " - 0s - loss: 767.7048 - val_loss: 722.0301\n",
      "Epoch 43/50\n",
      " - 0s - loss: 740.7352 - val_loss: 693.0233\n",
      "Epoch 44/50\n",
      " - 0s - loss: 705.6972 - val_loss: 693.4388\n",
      "Epoch 45/50\n",
      " - 0s - loss: 680.4293 - val_loss: 635.6695\n",
      "Epoch 46/50\n",
      " - 0s - loss: 646.3659 - val_loss: 611.5959\n",
      "Epoch 47/50\n",
      " - 0s - loss: 612.6698 - val_loss: 598.2015\n",
      "Epoch 48/50\n",
      " - 0s - loss: 585.5512 - val_loss: 565.7780\n",
      "Epoch 49/50\n",
      " - 0s - loss: 557.8510 - val_loss: 543.9572\n",
      "Epoch 50/50\n",
      " - 0s - loss: 532.2165 - val_loss: 512.2930\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 49192.2523 - val_loss: 22625.9751\n",
      "Epoch 2/50\n",
      " - 0s - loss: 10771.3597 - val_loss: 3101.6509\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1423.6026 - val_loss: 925.7109\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1030.1159 - val_loss: 969.6636\n",
      "Epoch 5/50\n",
      " - 0s - loss: 995.9332 - val_loss: 911.9604\n",
      "Epoch 6/50\n",
      " - 0s - loss: 918.6657 - val_loss: 922.2453\n",
      "Epoch 7/50\n",
      " - 0s - loss: 908.5335 - val_loss: 911.6754\n",
      "Epoch 8/50\n",
      " - 0s - loss: 893.6814 - val_loss: 895.6568\n",
      "Epoch 9/50\n",
      " - 0s - loss: 880.1959 - val_loss: 879.6849\n",
      "Epoch 10/50\n",
      " - 0s - loss: 867.5070 - val_loss: 873.5335\n",
      "Epoch 11/50\n",
      " - 0s - loss: 854.6939 - val_loss: 863.2373\n",
      "Epoch 12/50\n",
      " - 0s - loss: 842.0708 - val_loss: 854.8750\n",
      "Epoch 13/50\n",
      " - 0s - loss: 833.6496 - val_loss: 848.8633\n",
      "Epoch 14/50\n",
      " - 0s - loss: 827.4191 - val_loss: 836.4150\n",
      "Epoch 15/50\n",
      " - 0s - loss: 814.8257 - val_loss: 840.0192\n",
      "Epoch 16/50\n",
      " - 0s - loss: 807.0643 - val_loss: 829.7882\n",
      "Epoch 17/50\n",
      " - 0s - loss: 798.9604 - val_loss: 823.6107\n",
      "Epoch 18/50\n",
      " - 0s - loss: 790.2572 - val_loss: 816.3414\n",
      "Epoch 19/50\n",
      " - 0s - loss: 783.8128 - val_loss: 812.0751\n",
      "Epoch 20/50\n",
      " - 0s - loss: 776.2451 - val_loss: 802.2941\n",
      "Epoch 21/50\n",
      " - 0s - loss: 768.6717 - val_loss: 797.3751\n",
      "Epoch 22/50\n",
      " - 0s - loss: 762.5190 - val_loss: 795.9369\n",
      "Epoch 23/50\n",
      " - 0s - loss: 753.1877 - val_loss: 784.8657\n",
      "Epoch 24/50\n",
      " - 0s - loss: 745.4823 - val_loss: 784.2487\n",
      "Epoch 25/50\n",
      " - 0s - loss: 736.9754 - val_loss: 766.3387\n",
      "Epoch 26/50\n",
      " - 0s - loss: 731.2684 - val_loss: 767.2966\n",
      "Epoch 27/50\n",
      " - 0s - loss: 723.2401 - val_loss: 757.9445\n",
      "Epoch 28/50\n",
      " - 0s - loss: 715.4092 - val_loss: 750.7525\n",
      "Epoch 29/50\n",
      " - 0s - loss: 708.4573 - val_loss: 748.4433\n",
      "Epoch 30/50\n",
      " - 0s - loss: 701.9232 - val_loss: 743.1025\n",
      "Epoch 31/50\n",
      " - 0s - loss: 693.3158 - val_loss: 727.9759\n",
      "Epoch 32/50\n",
      " - 0s - loss: 686.9206 - val_loss: 724.3447\n",
      "Epoch 33/50\n",
      " - 0s - loss: 679.7688 - val_loss: 717.7711\n",
      "Epoch 34/50\n",
      " - 0s - loss: 670.6164 - val_loss: 702.8923\n",
      "Epoch 35/50\n",
      " - 0s - loss: 663.2301 - val_loss: 706.1727\n",
      "Epoch 36/50\n",
      " - 0s - loss: 656.0262 - val_loss: 687.4658\n",
      "Epoch 37/50\n",
      " - 0s - loss: 650.8153 - val_loss: 686.2109\n",
      "Epoch 38/50\n",
      " - 0s - loss: 641.0789 - val_loss: 669.8266\n",
      "Epoch 39/50\n",
      " - 0s - loss: 631.3321 - val_loss: 674.4758\n",
      "Epoch 40/50\n",
      " - 0s - loss: 628.2997 - val_loss: 653.8963\n",
      "Epoch 41/50\n",
      " - 0s - loss: 618.6745 - val_loss: 656.9598\n",
      "Epoch 42/50\n",
      " - 0s - loss: 609.3529 - val_loss: 634.0929\n",
      "Epoch 43/50\n",
      " - 0s - loss: 603.1864 - val_loss: 634.6142\n",
      "Epoch 44/50\n",
      " - 0s - loss: 594.5652 - val_loss: 630.1983\n",
      "Epoch 45/50\n",
      " - 0s - loss: 586.0360 - val_loss: 619.0957\n",
      "Epoch 46/50\n",
      " - 0s - loss: 578.5368 - val_loss: 613.2941\n",
      "Epoch 47/50\n",
      " - 0s - loss: 570.2966 - val_loss: 597.6433\n",
      "Epoch 48/50\n",
      " - 0s - loss: 563.3892 - val_loss: 592.9572\n",
      "Epoch 49/50\n",
      " - 0s - loss: 561.4707 - val_loss: 592.7446\n",
      "Epoch 50/50\n",
      " - 0s - loss: 559.0842 - val_loss: 568.6992\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 1719.0146 - val_loss: 1536.6446\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1477.7732 - val_loss: 1340.3331\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1292.0212 - val_loss: 1160.8270\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1128.2521 - val_loss: 1018.1074\n",
      "Epoch 5/50\n",
      " - 0s - loss: 997.8888 - val_loss: 896.1078\n",
      "Epoch 6/50\n",
      " - 0s - loss: 874.3235 - val_loss: 778.8619\n",
      "Epoch 7/50\n",
      " - 0s - loss: 744.3182 - val_loss: 684.3123\n",
      "Epoch 8/50\n",
      " - 0s - loss: 649.3430 - val_loss: 608.6289\n",
      "Epoch 9/50\n",
      " - 0s - loss: 561.4366 - val_loss: 525.8745\n",
      "Epoch 10/50\n",
      " - 0s - loss: 488.4433 - val_loss: 466.7087\n",
      "Epoch 11/50\n",
      " - 0s - loss: 429.9760 - val_loss: 419.4115\n",
      "Epoch 12/50\n",
      " - 0s - loss: 382.3074 - val_loss: 371.0543\n",
      "Epoch 13/50\n",
      " - 0s - loss: 353.2458 - val_loss: 338.6438\n",
      "Epoch 14/50\n",
      " - 0s - loss: 322.4863 - val_loss: 306.8080\n",
      "Epoch 15/50\n",
      " - 0s - loss: 296.5315 - val_loss: 273.5612\n",
      "Epoch 16/50\n",
      " - 0s - loss: 268.0760 - val_loss: 253.6300\n",
      "Epoch 17/50\n",
      " - 0s - loss: 250.3534 - val_loss: 235.9662\n",
      "Epoch 18/50\n",
      " - 0s - loss: 235.1074 - val_loss: 222.3202\n",
      "Epoch 19/50\n",
      " - 0s - loss: 222.0220 - val_loss: 208.6670\n",
      "Epoch 20/50\n",
      " - 0s - loss: 211.6241 - val_loss: 196.0782\n",
      "Epoch 21/50\n",
      " - 0s - loss: 202.0959 - val_loss: 189.4394\n",
      "Epoch 22/50\n",
      " - 0s - loss: 193.4912 - val_loss: 176.3795\n",
      "Epoch 23/50\n",
      " - 0s - loss: 183.7688 - val_loss: 173.8119\n",
      "Epoch 24/50\n",
      " - 0s - loss: 179.0048 - val_loss: 163.7780\n",
      "Epoch 25/50\n",
      " - 0s - loss: 170.1048 - val_loss: 155.8398\n",
      "Epoch 26/50\n",
      " - 0s - loss: 166.7537 - val_loss: 149.4011\n",
      "Epoch 27/50\n",
      " - 0s - loss: 160.5023 - val_loss: 142.7902\n",
      "Epoch 28/50\n",
      " - 0s - loss: 156.6366 - val_loss: 144.6337\n",
      "Epoch 29/50\n",
      " - 0s - loss: 154.6727 - val_loss: 134.7939\n",
      "Epoch 30/50\n",
      " - 0s - loss: 147.1040 - val_loss: 129.4865\n",
      "Epoch 31/50\n",
      " - 0s - loss: 142.4624 - val_loss: 124.9190\n",
      "Epoch 32/50\n",
      " - 0s - loss: 140.5989 - val_loss: 122.3832\n",
      "Epoch 33/50\n",
      " - 0s - loss: 136.5948 - val_loss: 127.6337\n",
      "Epoch 34/50\n",
      " - 0s - loss: 136.4001 - val_loss: 118.0942\n",
      "Epoch 35/50\n",
      " - 0s - loss: 132.8668 - val_loss: 113.5373\n",
      "Epoch 36/50\n",
      " - 0s - loss: 130.9970 - val_loss: 111.3582\n",
      "Epoch 37/50\n",
      " - 0s - loss: 130.6727 - val_loss: 108.3418\n",
      "Epoch 38/50\n",
      " - 0s - loss: 126.2810 - val_loss: 107.3042\n",
      "Epoch 39/50\n",
      " - 0s - loss: 125.2677 - val_loss: 107.6917\n",
      "Epoch 40/50\n",
      " - 0s - loss: 124.7489 - val_loss: 105.6488\n",
      "Epoch 41/50\n",
      " - 0s - loss: 124.0472 - val_loss: 115.3208\n",
      "Epoch 42/50\n",
      " - 0s - loss: 124.4990 - val_loss: 106.9566\n",
      "Epoch 43/50\n",
      " - 0s - loss: 119.5268 - val_loss: 99.6710\n",
      "Epoch 44/50\n",
      " - 0s - loss: 118.3781 - val_loss: 105.1546\n",
      "Epoch 45/50\n",
      " - 0s - loss: 117.9882 - val_loss: 97.5108\n",
      "Epoch 46/50\n",
      " - 0s - loss: 118.1485 - val_loss: 98.3480\n",
      "Epoch 47/50\n",
      " - 0s - loss: 117.8820 - val_loss: 117.9327\n",
      "Epoch 48/50\n",
      " - 0s - loss: 120.3781 - val_loss: 100.2146\n",
      "Epoch 49/50\n",
      " - 0s - loss: 116.4531 - val_loss: 95.3713\n",
      "Epoch 50/50\n",
      " - 0s - loss: 115.8766 - val_loss: 94.1357\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 496127.9747 - val_loss: 404785.1660\n",
      "Epoch 2/50\n",
      " - 0s - loss: 344016.7634 - val_loss: 278584.9418\n",
      "Epoch 3/50\n",
      " - 0s - loss: 236998.9107 - val_loss: 191695.4098\n",
      "Epoch 4/50\n",
      " - 0s - loss: 163836.2240 - val_loss: 132901.8504\n",
      "Epoch 5/50\n",
      " - 0s - loss: 114365.1430 - val_loss: 93239.2086\n",
      "Epoch 6/50\n",
      " - 0s - loss: 80861.7000 - val_loss: 66518.3193\n",
      "Epoch 7/50\n",
      " - 0s - loss: 58280.9989 - val_loss: 48125.3520\n",
      "Epoch 8/50\n",
      " - 0s - loss: 42719.9569 - val_loss: 35335.2914\n",
      "Epoch 9/50\n",
      " - 0s - loss: 31873.4005 - val_loss: 26614.1765\n",
      "Epoch 10/50\n",
      " - 0s - loss: 24526.2455 - val_loss: 20753.5323\n",
      "Epoch 11/50\n",
      " - 0s - loss: 19567.5493 - val_loss: 16787.2024\n",
      "Epoch 12/50\n",
      " - 0s - loss: 16030.6107 - val_loss: 13936.1136\n",
      "Epoch 13/50\n",
      " - 0s - loss: 13410.2850 - val_loss: 11790.8882\n",
      "Epoch 14/50\n",
      " - 0s - loss: 11416.7348 - val_loss: 10087.2706\n",
      "Epoch 15/50\n",
      " - 0s - loss: 9840.6934 - val_loss: 8746.2261\n",
      "Epoch 16/50\n",
      " - 0s - loss: 8592.1919 - val_loss: 7653.5817\n",
      "Epoch 17/50\n",
      " - 0s - loss: 7589.3794 - val_loss: 6756.5276\n",
      "Epoch 18/50\n",
      " - 0s - loss: 6764.3232 - val_loss: 6011.9775\n",
      "Epoch 19/50\n",
      " - 0s - loss: 6082.3551 - val_loss: 5394.1239\n",
      "Epoch 20/50\n",
      " - 0s - loss: 5516.3547 - val_loss: 4880.0415\n",
      "Epoch 21/50\n",
      " - 0s - loss: 5044.0683 - val_loss: 4446.1505\n",
      "Epoch 22/50\n",
      " - 0s - loss: 4646.3757 - val_loss: 4087.8002\n",
      "Epoch 23/50\n",
      " - 0s - loss: 4316.1104 - val_loss: 3778.4632\n",
      "Epoch 24/50\n",
      " - 0s - loss: 4025.4055 - val_loss: 3518.9176\n",
      "Epoch 25/50\n",
      " - 0s - loss: 3783.4933 - val_loss: 3295.9051\n",
      "Epoch 26/50\n",
      " - 0s - loss: 3572.2484 - val_loss: 3106.2999\n",
      "Epoch 27/50\n",
      " - 0s - loss: 3389.0673 - val_loss: 2945.1680\n",
      "Epoch 28/50\n",
      " - 0s - loss: 3230.2938 - val_loss: 2806.0293\n",
      "Epoch 29/50\n",
      " - 0s - loss: 3093.9664 - val_loss: 2684.0606\n",
      "Epoch 30/50\n",
      " - 0s - loss: 2976.0365 - val_loss: 2577.5020\n",
      "Epoch 31/50\n",
      " - 0s - loss: 2871.1665 - val_loss: 2488.2820\n",
      "Epoch 32/50\n",
      " - 0s - loss: 2780.4936 - val_loss: 2406.2363\n",
      "Epoch 33/50\n",
      " - 0s - loss: 2698.8541 - val_loss: 2331.6276\n",
      "Epoch 34/50\n",
      " - 0s - loss: 2619.7082 - val_loss: 2269.0640\n",
      "Epoch 35/50\n",
      " - 0s - loss: 2553.8643 - val_loss: 2210.0739\n",
      "Epoch 36/50\n",
      " - 0s - loss: 2491.7210 - val_loss: 2158.5207\n",
      "Epoch 37/50\n",
      " - 0s - loss: 2435.4271 - val_loss: 2115.3377\n",
      "Epoch 38/50\n",
      " - 0s - loss: 2386.4791 - val_loss: 2075.2589\n",
      "Epoch 39/50\n",
      " - 0s - loss: 2341.9149 - val_loss: 2038.9483\n",
      "Epoch 40/50\n",
      " - 0s - loss: 2301.0877 - val_loss: 2006.4532\n",
      "Epoch 41/50\n",
      " - 0s - loss: 2263.6612 - val_loss: 1977.8770\n",
      "Epoch 42/50\n",
      " - 0s - loss: 2228.8076 - val_loss: 1951.6364\n",
      "Epoch 43/50\n",
      " - 0s - loss: 2196.5104 - val_loss: 1926.8742\n",
      "Epoch 44/50\n",
      " - 0s - loss: 2166.0689 - val_loss: 1903.7209\n",
      "Epoch 45/50\n",
      " - 0s - loss: 2138.5045 - val_loss: 1881.3768\n",
      "Epoch 46/50\n",
      " - 0s - loss: 2111.5402 - val_loss: 1859.9595\n",
      "Epoch 47/50\n",
      " - 0s - loss: 2085.9587 - val_loss: 1839.8058\n",
      "Epoch 48/50\n",
      " - 0s - loss: 2061.2161 - val_loss: 1821.1795\n",
      "Epoch 49/50\n",
      " - 0s - loss: 2038.0071 - val_loss: 1803.6663\n",
      "Epoch 50/50\n",
      " - 0s - loss: 2016.4259 - val_loss: 1786.8539\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 120917.0902 - val_loss: 94166.6689\n",
      "Epoch 2/50\n",
      " - 0s - loss: 63735.4756 - val_loss: 45914.4723\n",
      "Epoch 3/50\n",
      " - 0s - loss: 29333.8522 - val_loss: 21514.2344\n",
      "Epoch 4/50\n",
      " - 0s - loss: 14911.5106 - val_loss: 13475.5353\n",
      "Epoch 5/50\n",
      " - 0s - loss: 11508.1438 - val_loss: 11535.7980\n",
      "Epoch 6/50\n",
      " - 0s - loss: 10782.7970 - val_loss: 11051.7208\n",
      "Epoch 7/50\n",
      " - 0s - loss: 10474.7708 - val_loss: 10709.0245\n",
      "Epoch 8/50\n",
      " - 0s - loss: 10126.3777 - val_loss: 10379.1190\n",
      "Epoch 9/50\n",
      " - 0s - loss: 9774.0697 - val_loss: 10081.5069\n",
      "Epoch 10/50\n",
      " - 0s - loss: 9418.0264 - val_loss: 9734.7669\n",
      "Epoch 11/50\n",
      " - 0s - loss: 9071.2326 - val_loss: 9428.5370\n",
      "Epoch 12/50\n",
      " - 0s - loss: 8722.8208 - val_loss: 9059.2721\n",
      "Epoch 13/50\n",
      " - 0s - loss: 8374.9349 - val_loss: 8738.3438\n",
      "Epoch 14/50\n",
      " - 0s - loss: 8050.7415 - val_loss: 8379.0358\n",
      "Epoch 15/50\n",
      " - 0s - loss: 7709.3425 - val_loss: 8102.2997\n",
      "Epoch 16/50\n",
      " - 0s - loss: 7399.6523 - val_loss: 7808.9786\n",
      "Epoch 17/50\n",
      " - 0s - loss: 7083.9569 - val_loss: 7443.3261\n",
      "Epoch 18/50\n",
      " - 0s - loss: 6784.1532 - val_loss: 7167.8972\n",
      "Epoch 19/50\n",
      " - 0s - loss: 6480.2202 - val_loss: 6864.5265\n",
      "Epoch 20/50\n",
      " - 0s - loss: 6206.3401 - val_loss: 6626.3754\n",
      "Epoch 21/50\n",
      " - 0s - loss: 5927.1666 - val_loss: 6335.8838\n",
      "Epoch 22/50\n",
      " - 0s - loss: 5660.0305 - val_loss: 6063.7387\n",
      "Epoch 23/50\n",
      " - 0s - loss: 5404.1359 - val_loss: 5776.5785\n",
      "Epoch 24/50\n",
      " - 0s - loss: 5161.5193 - val_loss: 5560.8488\n",
      "Epoch 25/50\n",
      " - 0s - loss: 4923.8509 - val_loss: 5335.1152\n",
      "Epoch 26/50\n",
      " - 0s - loss: 4687.4935 - val_loss: 5081.2507\n",
      "Epoch 27/50\n",
      " - 0s - loss: 4477.2275 - val_loss: 4885.7669\n",
      "Epoch 28/50\n",
      " - 0s - loss: 4264.9050 - val_loss: 4665.8795\n",
      "Epoch 29/50\n",
      " - 0s - loss: 4058.0616 - val_loss: 4421.0982\n",
      "Epoch 30/50\n",
      " - 0s - loss: 3858.6308 - val_loss: 4269.7914\n",
      "Epoch 31/50\n",
      " - 0s - loss: 3680.3004 - val_loss: 4065.4012\n",
      "Epoch 32/50\n",
      " - 0s - loss: 3495.2740 - val_loss: 3913.6547\n",
      "Epoch 33/50\n",
      " - 0s - loss: 3330.1754 - val_loss: 3710.7445\n",
      "Epoch 34/50\n",
      " - 0s - loss: 3161.4239 - val_loss: 3533.4206\n",
      "Epoch 35/50\n",
      " - 0s - loss: 3004.7878 - val_loss: 3410.1928\n",
      "Epoch 36/50\n",
      " - 0s - loss: 2864.0292 - val_loss: 3248.9518\n",
      "Epoch 37/50\n",
      " - 0s - loss: 2714.8582 - val_loss: 3083.5583\n",
      "Epoch 38/50\n",
      " - 0s - loss: 2577.5205 - val_loss: 2929.9202\n",
      "Epoch 39/50\n",
      " - 0s - loss: 2451.7381 - val_loss: 2790.5405\n",
      "Epoch 40/50\n",
      " - 0s - loss: 2330.8635 - val_loss: 2672.7311\n",
      "Epoch 41/50\n",
      " - 0s - loss: 2214.6422 - val_loss: 2562.6332\n",
      "Epoch 42/50\n",
      " - 0s - loss: 2106.2524 - val_loss: 2438.5684\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1998.5474 - val_loss: 2312.1431\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1905.3470 - val_loss: 2215.0370\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1808.0812 - val_loss: 2109.3218\n",
      "Epoch 46/50\n",
      " - 0s - loss: 1720.7001 - val_loss: 2003.7090\n",
      "Epoch 47/50\n",
      " - 0s - loss: 1637.3819 - val_loss: 1933.3744\n",
      "Epoch 48/50\n",
      " - 0s - loss: 1553.3543 - val_loss: 1821.0830\n",
      "Epoch 49/50\n",
      " - 0s - loss: 1482.9622 - val_loss: 1736.7255\n",
      "Epoch 50/50\n",
      " - 0s - loss: 1412.4184 - val_loss: 1649.8995\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 33204.1351 - val_loss: 16616.7287\n",
      "Epoch 2/50\n",
      " - 0s - loss: 12491.9157 - val_loss: 8647.3810\n",
      "Epoch 3/50\n",
      " - 0s - loss: 9043.8201 - val_loss: 8312.8023\n",
      "Epoch 4/50\n",
      " - 0s - loss: 8312.8084 - val_loss: 7449.7731\n",
      "Epoch 5/50\n",
      " - 0s - loss: 7483.3134 - val_loss: 6768.3529\n",
      "Epoch 6/50\n",
      " - 0s - loss: 6835.3833 - val_loss: 6225.8927\n",
      "Epoch 7/50\n",
      " - 0s - loss: 6264.3743 - val_loss: 5667.8594\n",
      "Epoch 8/50\n",
      " - 0s - loss: 5671.7410 - val_loss: 5186.9483\n",
      "Epoch 9/50\n",
      " - 0s - loss: 5140.7371 - val_loss: 4731.2592\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4678.1005 - val_loss: 4288.0997\n",
      "Epoch 11/50\n",
      " - 0s - loss: 4248.5264 - val_loss: 3889.6559\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3835.6044 - val_loss: 3515.9619\n",
      "Epoch 13/50\n",
      " - 0s - loss: 3494.3723 - val_loss: 3167.8760\n",
      "Epoch 14/50\n",
      " - 0s - loss: 3123.4455 - val_loss: 2864.8686\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2819.4289 - val_loss: 2560.1255\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2525.7923 - val_loss: 2291.3869\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2250.9439 - val_loss: 2052.8526\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2018.8241 - val_loss: 1819.3282\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1789.2742 - val_loss: 1607.6251\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1583.8954 - val_loss: 1423.2221\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1410.4309 - val_loss: 1248.6776\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1240.1708 - val_loss: 1110.3308\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1095.1972 - val_loss: 967.2858\n",
      "Epoch 24/50\n",
      " - 0s - loss: 966.5272 - val_loss: 858.7019\n",
      "Epoch 25/50\n",
      " - 0s - loss: 854.2022 - val_loss: 747.5876\n",
      "Epoch 26/50\n",
      " - 0s - loss: 751.0031 - val_loss: 664.7592\n",
      "Epoch 27/50\n",
      " - 0s - loss: 654.5408 - val_loss: 581.2992\n",
      "Epoch 28/50\n",
      " - 0s - loss: 581.1087 - val_loss: 508.0061\n",
      "Epoch 29/50\n",
      " - 0s - loss: 508.5020 - val_loss: 452.4910\n",
      "Epoch 30/50\n",
      " - 0s - loss: 449.9342 - val_loss: 397.6954\n",
      "Epoch 31/50\n",
      " - 0s - loss: 402.1130 - val_loss: 354.4910\n",
      "Epoch 32/50\n",
      " - 0s - loss: 359.2759 - val_loss: 316.7403\n",
      "Epoch 33/50\n",
      " - 0s - loss: 319.7578 - val_loss: 282.1405\n",
      "Epoch 34/50\n",
      " - 0s - loss: 288.2243 - val_loss: 256.5200\n",
      "Epoch 35/50\n",
      " - 0s - loss: 260.9195 - val_loss: 234.2249\n",
      "Epoch 36/50\n",
      " - 0s - loss: 239.5812 - val_loss: 214.6695\n",
      "Epoch 37/50\n",
      " - 0s - loss: 218.9316 - val_loss: 198.4770\n",
      "Epoch 38/50\n",
      " - 0s - loss: 204.4689 - val_loss: 185.2784\n",
      "Epoch 39/50\n",
      " - 0s - loss: 189.6087 - val_loss: 174.9707\n",
      "Epoch 40/50\n",
      " - 0s - loss: 177.9550 - val_loss: 166.0549\n",
      "Epoch 41/50\n",
      " - 0s - loss: 167.3698 - val_loss: 159.3907\n",
      "Epoch 42/50\n",
      " - 0s - loss: 160.8386 - val_loss: 153.2058\n",
      "Epoch 43/50\n",
      " - 0s - loss: 152.7403 - val_loss: 147.9935\n",
      "Epoch 44/50\n",
      " - 0s - loss: 146.7596 - val_loss: 144.4485\n",
      "Epoch 45/50\n",
      " - 0s - loss: 142.8635 - val_loss: 141.0248\n",
      "Epoch 46/50\n",
      " - 0s - loss: 139.5835 - val_loss: 138.3564\n",
      "Epoch 47/50\n",
      " - 0s - loss: 135.0809 - val_loss: 136.3090\n",
      "Epoch 48/50\n",
      " - 0s - loss: 132.6212 - val_loss: 134.6689\n",
      "Epoch 49/50\n",
      " - 0s - loss: 129.2163 - val_loss: 134.2520\n",
      "Epoch 50/50\n",
      " - 0s - loss: 127.8633 - val_loss: 131.6239\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 679159.8968 - val_loss: 584585.3197\n",
      "Epoch 2/50\n",
      " - 0s - loss: 515817.3328 - val_loss: 443272.7294\n",
      "Epoch 3/50\n",
      " - 0s - loss: 387734.7178 - val_loss: 329113.6083\n",
      "Epoch 4/50\n",
      " - 0s - loss: 284592.3693 - val_loss: 235926.6781\n",
      "Epoch 5/50\n",
      " - 0s - loss: 201319.1793 - val_loss: 163802.7422\n",
      "Epoch 6/50\n",
      " - 0s - loss: 138187.7460 - val_loss: 110897.9454\n",
      "Epoch 7/50\n",
      " - 0s - loss: 93099.7075 - val_loss: 73956.8178\n",
      "Epoch 8/50\n",
      " - 0s - loss: 62185.1500 - val_loss: 49469.2833\n",
      "Epoch 9/50\n",
      " - 0s - loss: 41809.3653 - val_loss: 33854.4495\n",
      "Epoch 10/50\n",
      " - 0s - loss: 29201.8854 - val_loss: 24068.9270\n",
      "Epoch 11/50\n",
      " - 0s - loss: 21322.1672 - val_loss: 18426.1954\n",
      "Epoch 12/50\n",
      " - 0s - loss: 16884.0650 - val_loss: 15121.9629\n",
      "Epoch 13/50\n",
      " - 0s - loss: 14282.5231 - val_loss: 13306.5267\n",
      "Epoch 14/50\n",
      " - 0s - loss: 12871.7433 - val_loss: 12227.7075\n",
      "Epoch 15/50\n",
      " - 0s - loss: 12031.4822 - val_loss: 11582.1012\n",
      "Epoch 16/50\n",
      " - 0s - loss: 11502.1522 - val_loss: 11140.7391\n",
      "Epoch 17/50\n",
      " - 0s - loss: 11129.2667 - val_loss: 10785.2714\n",
      "Epoch 18/50\n",
      " - 0s - loss: 10810.5988 - val_loss: 10469.8751\n",
      "Epoch 19/50\n",
      " - 0s - loss: 10514.0892 - val_loss: 10188.2448\n",
      "Epoch 20/50\n",
      " - 0s - loss: 10237.0301 - val_loss: 9904.5364\n",
      "Epoch 21/50\n",
      " - 0s - loss: 9966.5896 - val_loss: 9625.0807\n",
      "Epoch 22/50\n",
      " - 0s - loss: 9691.8691 - val_loss: 9357.3968\n",
      "Epoch 23/50\n",
      " - 0s - loss: 9428.6462 - val_loss: 9086.9608\n",
      "Epoch 24/50\n",
      " - 0s - loss: 9159.9486 - val_loss: 8840.5405\n",
      "Epoch 25/50\n",
      " - 0s - loss: 8908.3113 - val_loss: 8586.0201\n",
      "Epoch 26/50\n",
      " - 0s - loss: 8669.6714 - val_loss: 8325.7862\n",
      "Epoch 27/50\n",
      " - 0s - loss: 8403.4187 - val_loss: 8081.3984\n",
      "Epoch 28/50\n",
      " - 0s - loss: 8160.2191 - val_loss: 7839.1529\n",
      "Epoch 29/50\n",
      " - 0s - loss: 7921.3051 - val_loss: 7601.2796\n",
      "Epoch 30/50\n",
      " - 0s - loss: 7684.2136 - val_loss: 7364.9621\n",
      "Epoch 31/50\n",
      " - 0s - loss: 7449.7260 - val_loss: 7132.3423\n",
      "Epoch 32/50\n",
      " - 0s - loss: 7222.2231 - val_loss: 6900.0735\n",
      "Epoch 33/50\n",
      " - 0s - loss: 6994.1695 - val_loss: 6669.7996\n",
      "Epoch 34/50\n",
      " - 0s - loss: 6761.3158 - val_loss: 6449.9210\n",
      "Epoch 35/50\n",
      " - 0s - loss: 6524.0984 - val_loss: 6242.1576\n",
      "Epoch 36/50\n",
      " - 0s - loss: 6310.8367 - val_loss: 6015.4391\n",
      "Epoch 37/50\n",
      " - 0s - loss: 6080.5603 - val_loss: 5796.0699\n",
      "Epoch 38/50\n",
      " - 0s - loss: 5863.1934 - val_loss: 5587.2154\n",
      "Epoch 39/50\n",
      " - 0s - loss: 5637.9942 - val_loss: 5376.3329\n",
      "Epoch 40/50\n",
      " - 0s - loss: 5425.5098 - val_loss: 5165.8612\n",
      "Epoch 41/50\n",
      " - 0s - loss: 5221.2762 - val_loss: 4949.1187\n",
      "Epoch 42/50\n",
      " - 0s - loss: 5004.5839 - val_loss: 4749.4451\n",
      "Epoch 43/50\n",
      " - 0s - loss: 4812.1441 - val_loss: 4554.2608\n",
      "Epoch 44/50\n",
      " - 0s - loss: 4612.7114 - val_loss: 4354.2461\n",
      "Epoch 45/50\n",
      " - 0s - loss: 4415.6874 - val_loss: 4159.4269\n",
      "Epoch 46/50\n",
      " - 0s - loss: 4220.9877 - val_loss: 3964.5947\n",
      "Epoch 47/50\n",
      " - 0s - loss: 4029.1666 - val_loss: 3790.4168\n",
      "Epoch 48/50\n",
      " - 0s - loss: 3844.7571 - val_loss: 3610.1263\n",
      "Epoch 49/50\n",
      " - 0s - loss: 3660.5142 - val_loss: 3439.2777\n",
      "Epoch 50/50\n",
      " - 0s - loss: 3482.5401 - val_loss: 3279.8942\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 103086.0833 - val_loss: 55872.5073\n",
      "Epoch 2/50\n",
      " - 0s - loss: 37808.0904 - val_loss: 17614.8025\n",
      "Epoch 3/50\n",
      " - 0s - loss: 12449.3043 - val_loss: 6329.1523\n",
      "Epoch 4/50\n",
      " - 0s - loss: 5545.9388 - val_loss: 4658.7704\n",
      "Epoch 5/50\n",
      " - 0s - loss: 4377.2050 - val_loss: 4478.8863\n",
      "Epoch 6/50\n",
      " - 0s - loss: 4079.3444 - val_loss: 4177.5354\n",
      "Epoch 7/50\n",
      " - 0s - loss: 3763.8585 - val_loss: 3751.9529\n",
      "Epoch 8/50\n",
      " - 0s - loss: 3459.8905 - val_loss: 3424.5088\n",
      "Epoch 9/50\n",
      " - 0s - loss: 3181.5256 - val_loss: 3148.9136\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2935.0041 - val_loss: 2881.3276\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2692.6516 - val_loss: 2656.6867\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2483.3992 - val_loss: 2448.4623\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2293.8360 - val_loss: 2232.1783\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2125.6148 - val_loss: 2072.7543\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1976.6004 - val_loss: 1919.0348\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1839.5309 - val_loss: 1772.9585\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1723.9994 - val_loss: 1630.7565\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1618.0491 - val_loss: 1520.0819\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1512.7564 - val_loss: 1447.4006\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1431.4089 - val_loss: 1336.6629\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1348.5742 - val_loss: 1255.4596\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1279.9319 - val_loss: 1169.1989\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1209.5389 - val_loss: 1112.7201\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1152.3051 - val_loss: 1046.0199\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1095.1302 - val_loss: 984.8996\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1048.1835 - val_loss: 938.0349\n",
      "Epoch 27/50\n",
      " - 0s - loss: 999.9513 - val_loss: 885.4255\n",
      "Epoch 28/50\n",
      " - 0s - loss: 957.8703 - val_loss: 846.7952\n",
      "Epoch 29/50\n",
      " - 0s - loss: 920.1029 - val_loss: 823.1430\n",
      "Epoch 30/50\n",
      " - 0s - loss: 885.9424 - val_loss: 766.9287\n",
      "Epoch 31/50\n",
      " - 0s - loss: 844.1300 - val_loss: 743.8190\n",
      "Epoch 32/50\n",
      " - 0s - loss: 810.5149 - val_loss: 712.7747\n",
      "Epoch 33/50\n",
      " - 0s - loss: 777.4002 - val_loss: 678.1481\n",
      "Epoch 34/50\n",
      " - 0s - loss: 749.8654 - val_loss: 652.2344\n",
      "Epoch 35/50\n",
      " - 0s - loss: 723.3126 - val_loss: 630.4988\n",
      "Epoch 36/50\n",
      " - 0s - loss: 694.4299 - val_loss: 601.8924\n",
      "Epoch 37/50\n",
      " - 0s - loss: 670.9077 - val_loss: 579.3900\n",
      "Epoch 38/50\n",
      " - 0s - loss: 647.5838 - val_loss: 562.1291\n",
      "Epoch 39/50\n",
      " - 0s - loss: 624.5283 - val_loss: 544.2185\n",
      "Epoch 40/50\n",
      " - 0s - loss: 604.6660 - val_loss: 524.2473\n",
      "Epoch 41/50\n",
      " - 0s - loss: 585.6488 - val_loss: 504.8144\n",
      "Epoch 42/50\n",
      " - 0s - loss: 561.6559 - val_loss: 498.7097\n",
      "Epoch 43/50\n",
      " - 0s - loss: 545.9321 - val_loss: 478.3031\n",
      "Epoch 44/50\n",
      " - 0s - loss: 529.0698 - val_loss: 460.5910\n",
      "Epoch 45/50\n",
      " - 0s - loss: 509.2798 - val_loss: 448.0706\n",
      "Epoch 46/50\n",
      " - 0s - loss: 494.4426 - val_loss: 435.6041\n",
      "Epoch 47/50\n",
      " - 0s - loss: 478.8188 - val_loss: 423.9993\n",
      "Epoch 48/50\n",
      " - 0s - loss: 461.4905 - val_loss: 410.8374\n",
      "Epoch 49/50\n",
      " - 0s - loss: 446.9738 - val_loss: 399.2802\n",
      "Epoch 50/50\n",
      " - 0s - loss: 432.7793 - val_loss: 388.3685\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 94685.7743 - val_loss: 57525.7058\n",
      "Epoch 2/50\n",
      " - 0s - loss: 38040.2340 - val_loss: 20001.1569\n",
      "Epoch 3/50\n",
      " - 0s - loss: 13492.4519 - val_loss: 7765.5828\n",
      "Epoch 4/50\n",
      " - 0s - loss: 6929.8807 - val_loss: 5536.1997\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5717.7732 - val_loss: 4911.2342\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5084.5726 - val_loss: 4331.1704\n",
      "Epoch 7/50\n",
      " - 0s - loss: 4528.3843 - val_loss: 3923.4447\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4109.9349 - val_loss: 3624.2542\n",
      "Epoch 9/50\n",
      " - 0s - loss: 3790.1262 - val_loss: 3384.5501\n",
      "Epoch 10/50\n",
      " - 0s - loss: 3537.1838 - val_loss: 3197.3463\n",
      "Epoch 11/50\n",
      " - 0s - loss: 3341.9025 - val_loss: 3011.6915\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3110.1423 - val_loss: 2620.7323\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2680.5289 - val_loss: 2151.0938\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2104.4899 - val_loss: 1732.2170\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1702.6193 - val_loss: 1424.7720\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1405.4409 - val_loss: 1195.3187\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1190.1309 - val_loss: 1019.6131\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1019.1954 - val_loss: 893.2149\n",
      "Epoch 19/50\n",
      " - 0s - loss: 892.0463 - val_loss: 791.8765\n",
      "Epoch 20/50\n",
      " - 0s - loss: 794.6684 - val_loss: 715.5393\n",
      "Epoch 21/50\n",
      " - 0s - loss: 714.8263 - val_loss: 645.9285\n",
      "Epoch 22/50\n",
      " - 0s - loss: 647.2336 - val_loss: 596.9993\n",
      "Epoch 23/50\n",
      " - 0s - loss: 598.2257 - val_loss: 556.8758\n",
      "Epoch 24/50\n",
      " - 0s - loss: 558.7994 - val_loss: 521.7679\n",
      "Epoch 25/50\n",
      " - 0s - loss: 523.7350 - val_loss: 494.0079\n",
      "Epoch 26/50\n",
      " - 0s - loss: 495.5154 - val_loss: 478.5579\n",
      "Epoch 27/50\n",
      " - 0s - loss: 473.1877 - val_loss: 450.0086\n",
      "Epoch 28/50\n",
      " - 0s - loss: 451.3608 - val_loss: 432.9324\n",
      "Epoch 29/50\n",
      " - 0s - loss: 432.9193 - val_loss: 417.1361\n",
      "Epoch 30/50\n",
      " - 0s - loss: 418.7784 - val_loss: 405.4135\n",
      "Epoch 31/50\n",
      " - 0s - loss: 404.2064 - val_loss: 390.8494\n",
      "Epoch 32/50\n",
      " - 0s - loss: 392.5103 - val_loss: 384.0420\n",
      "Epoch 33/50\n",
      " - 0s - loss: 380.7493 - val_loss: 367.7066\n",
      "Epoch 34/50\n",
      " - 0s - loss: 369.3472 - val_loss: 367.4627\n",
      "Epoch 35/50\n",
      " - 0s - loss: 365.7858 - val_loss: 349.6929\n",
      "Epoch 36/50\n",
      " - 0s - loss: 351.2394 - val_loss: 339.1347\n",
      "Epoch 37/50\n",
      " - 0s - loss: 341.5848 - val_loss: 330.4581\n",
      "Epoch 38/50\n",
      " - 0s - loss: 333.8753 - val_loss: 321.8054\n",
      "Epoch 39/50\n",
      " - 0s - loss: 325.8691 - val_loss: 314.0199\n",
      "Epoch 40/50\n",
      " - 0s - loss: 315.6884 - val_loss: 309.8187\n",
      "Epoch 41/50\n",
      " - 0s - loss: 311.3603 - val_loss: 305.0676\n",
      "Epoch 42/50\n",
      " - 0s - loss: 305.5936 - val_loss: 297.6575\n",
      "Epoch 43/50\n",
      " - 0s - loss: 297.6004 - val_loss: 288.8739\n",
      "Epoch 44/50\n",
      " - 0s - loss: 292.1482 - val_loss: 284.9392\n",
      "Epoch 45/50\n",
      " - 0s - loss: 286.6840 - val_loss: 277.9716\n",
      "Epoch 46/50\n",
      " - 0s - loss: 284.2156 - val_loss: 273.0398\n",
      "Epoch 47/50\n",
      " - 0s - loss: 279.3250 - val_loss: 269.2741\n",
      "Epoch 48/50\n",
      " - 0s - loss: 271.9782 - val_loss: 263.9654\n",
      "Epoch 49/50\n",
      " - 0s - loss: 272.6789 - val_loss: 266.1699\n",
      "Epoch 50/50\n",
      " - 0s - loss: 268.4620 - val_loss: 280.0484\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 34387.1732 - val_loss: 24532.5419\n",
      "Epoch 2/50\n",
      " - 0s - loss: 18884.6047 - val_loss: 12644.0639\n",
      "Epoch 3/50\n",
      " - 0s - loss: 8888.5346 - val_loss: 5150.5925\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2972.5940 - val_loss: 1449.4277\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1019.5487 - val_loss: 978.3216\n",
      "Epoch 6/50\n",
      " - 0s - loss: 864.6320 - val_loss: 787.8277\n",
      "Epoch 7/50\n",
      " - 0s - loss: 708.1525 - val_loss: 697.2844\n",
      "Epoch 8/50\n",
      " - 0s - loss: 639.1447 - val_loss: 641.8405\n",
      "Epoch 9/50\n",
      " - 0s - loss: 594.5333 - val_loss: 600.3953\n",
      "Epoch 10/50\n",
      " - 0s - loss: 560.5401 - val_loss: 566.2411\n",
      "Epoch 11/50\n",
      " - 0s - loss: 532.0066 - val_loss: 536.1105\n",
      "Epoch 12/50\n",
      " - 0s - loss: 505.9379 - val_loss: 508.6099\n",
      "Epoch 13/50\n",
      " - 0s - loss: 483.9180 - val_loss: 484.8444\n",
      "Epoch 14/50\n",
      " - 0s - loss: 464.5477 - val_loss: 461.9842\n",
      "Epoch 15/50\n",
      " - 0s - loss: 445.9783 - val_loss: 441.3503\n",
      "Epoch 16/50\n",
      " - 0s - loss: 429.7962 - val_loss: 419.2183\n",
      "Epoch 17/50\n",
      " - 0s - loss: 412.5358 - val_loss: 398.3176\n",
      "Epoch 18/50\n",
      " - 0s - loss: 396.1763 - val_loss: 378.8979\n",
      "Epoch 19/50\n",
      " - 0s - loss: 381.5948 - val_loss: 358.9300\n",
      "Epoch 20/50\n",
      " - 0s - loss: 365.9503 - val_loss: 339.9791\n",
      "Epoch 21/50\n",
      " - 0s - loss: 353.4938 - val_loss: 324.5877\n",
      "Epoch 22/50\n",
      " - 0s - loss: 341.8680 - val_loss: 309.4543\n",
      "Epoch 23/50\n",
      " - 0s - loss: 329.9611 - val_loss: 295.8561\n",
      "Epoch 24/50\n",
      " - 0s - loss: 319.9576 - val_loss: 282.6318\n",
      "Epoch 25/50\n",
      " - 0s - loss: 309.9198 - val_loss: 270.3806\n",
      "Epoch 26/50\n",
      " - 0s - loss: 299.2620 - val_loss: 257.5006\n",
      "Epoch 27/50\n",
      " - 0s - loss: 288.9641 - val_loss: 246.4223\n",
      "Epoch 28/50\n",
      " - 0s - loss: 279.7263 - val_loss: 236.0788\n",
      "Epoch 29/50\n",
      " - 0s - loss: 273.6103 - val_loss: 230.0163\n",
      "Epoch 30/50\n",
      " - 0s - loss: 267.1804 - val_loss: 224.9723\n",
      "Epoch 31/50\n",
      " - 0s - loss: 262.3463 - val_loss: 221.8025\n",
      "Epoch 32/50\n",
      " - 0s - loss: 257.8858 - val_loss: 218.4598\n",
      "Epoch 33/50\n",
      " - 0s - loss: 256.5466 - val_loss: 214.3969\n",
      "Epoch 34/50\n",
      " - 0s - loss: 249.0362 - val_loss: 211.2499\n",
      "Epoch 35/50\n",
      " - 0s - loss: 245.0972 - val_loss: 209.2977\n",
      "Epoch 36/50\n",
      " - 0s - loss: 241.0828 - val_loss: 206.4980\n",
      "Epoch 37/50\n",
      " - 0s - loss: 238.0676 - val_loss: 203.6952\n",
      "Epoch 38/50\n",
      " - 0s - loss: 233.5799 - val_loss: 200.6053\n",
      "Epoch 39/50\n",
      " - 0s - loss: 230.4545 - val_loss: 198.1937\n",
      "Epoch 40/50\n",
      " - 0s - loss: 227.7787 - val_loss: 194.6657\n",
      "Epoch 41/50\n",
      " - 0s - loss: 224.2295 - val_loss: 192.4690\n",
      "Epoch 42/50\n",
      " - 0s - loss: 220.2523 - val_loss: 190.2193\n",
      "Epoch 43/50\n",
      " - 0s - loss: 216.4534 - val_loss: 187.0958\n",
      "Epoch 44/50\n",
      " - 0s - loss: 211.1453 - val_loss: 184.0091\n",
      "Epoch 45/50\n",
      " - 0s - loss: 206.3396 - val_loss: 177.9681\n",
      "Epoch 46/50\n",
      " - 0s - loss: 202.6403 - val_loss: 176.5516\n",
      "Epoch 47/50\n",
      " - 0s - loss: 198.8405 - val_loss: 173.5651\n",
      "Epoch 48/50\n",
      " - 0s - loss: 196.1506 - val_loss: 172.0303\n",
      "Epoch 49/50\n",
      " - 0s - loss: 191.6027 - val_loss: 169.3285\n",
      "Epoch 50/50\n",
      " - 0s - loss: 188.3726 - val_loss: 167.5129\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 192619.8648 - val_loss: 161795.8104\n",
      "Epoch 2/50\n",
      " - 0s - loss: 142681.4622 - val_loss: 120291.4569\n",
      "Epoch 3/50\n",
      " - 0s - loss: 105836.4928 - val_loss: 88818.4713\n",
      "Epoch 4/50\n",
      " - 0s - loss: 77235.1941 - val_loss: 63735.9437\n",
      "Epoch 5/50\n",
      " - 0s - loss: 54341.3506 - val_loss: 43388.5148\n",
      "Epoch 6/50\n",
      " - 0s - loss: 35812.2107 - val_loss: 27142.6420\n",
      "Epoch 7/50\n",
      " - 0s - loss: 21411.0050 - val_loss: 15107.5878\n",
      "Epoch 8/50\n",
      " - 0s - loss: 11377.1466 - val_loss: 7402.4573\n",
      "Epoch 9/50\n",
      " - 0s - loss: 5450.1252 - val_loss: 3462.2851\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2624.7919 - val_loss: 1990.7793\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1726.2065 - val_loss: 1584.6794\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1491.5972 - val_loss: 1527.1985\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1443.9373 - val_loss: 1515.9700\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1428.7125 - val_loss: 1499.7508\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1412.3221 - val_loss: 1483.7602\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1397.8170 - val_loss: 1468.8812\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1383.3500 - val_loss: 1451.2451\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1367.5518 - val_loss: 1435.9945\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1352.3953 - val_loss: 1420.2966\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1337.4984 - val_loss: 1403.8096\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1321.6573 - val_loss: 1388.4808\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1307.4406 - val_loss: 1373.7257\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1294.3992 - val_loss: 1354.8932\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1279.0602 - val_loss: 1341.5445\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1261.9286 - val_loss: 1326.0709\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1247.3812 - val_loss: 1309.5190\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1232.9128 - val_loss: 1293.5343\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1220.3637 - val_loss: 1281.3054\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1204.7263 - val_loss: 1264.7823\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1191.0482 - val_loss: 1249.2027\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1178.0534 - val_loss: 1236.6827\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1163.9972 - val_loss: 1220.5223\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1150.5222 - val_loss: 1206.6750\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1140.2109 - val_loss: 1191.0942\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1126.5788 - val_loss: 1183.1671\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1112.2468 - val_loss: 1167.1982\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1100.5042 - val_loss: 1150.2730\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1087.5875 - val_loss: 1137.9916\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1075.7556 - val_loss: 1126.9882\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1063.6945 - val_loss: 1112.0358\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1053.4578 - val_loss: 1097.0678\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1040.3520 - val_loss: 1087.0451\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1028.7178 - val_loss: 1073.7214\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1017.6154 - val_loss: 1060.0067\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1007.4649 - val_loss: 1049.6106\n",
      "Epoch 46/50\n",
      " - 0s - loss: 996.5240 - val_loss: 1035.8638\n",
      "Epoch 47/50\n",
      " - 0s - loss: 988.4332 - val_loss: 1026.3703\n",
      "Epoch 48/50\n",
      " - 0s - loss: 972.7897 - val_loss: 1009.6762\n",
      "Epoch 49/50\n",
      " - 0s - loss: 964.1666 - val_loss: 998.3213\n",
      "Epoch 50/50\n",
      " - 0s - loss: 952.9964 - val_loss: 987.1276\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 270284.4437 - val_loss: 213258.2895\n",
      "Epoch 2/50\n",
      " - 0s - loss: 177536.6215 - val_loss: 135799.6377\n",
      "Epoch 3/50\n",
      " - 0s - loss: 110792.5505 - val_loss: 82191.1908\n",
      "Epoch 4/50\n",
      " - 0s - loss: 65692.9824 - val_loss: 47198.5052\n",
      "Epoch 5/50\n",
      " - 0s - loss: 36980.1783 - val_loss: 25557.9467\n",
      "Epoch 6/50\n",
      " - 0s - loss: 19602.5149 - val_loss: 13028.5657\n",
      "Epoch 7/50\n",
      " - 0s - loss: 9816.2468 - val_loss: 6243.5831\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4678.0246 - val_loss: 2891.1099\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2229.7663 - val_loss: 1381.7822\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1153.7154 - val_loss: 782.8671\n",
      "Epoch 11/50\n",
      " - 0s - loss: 742.2309 - val_loss: 569.8378\n",
      "Epoch 12/50\n",
      " - 0s - loss: 588.7518 - val_loss: 506.6369\n",
      "Epoch 13/50\n",
      " - 0s - loss: 538.4152 - val_loss: 488.1793\n",
      "Epoch 14/50\n",
      " - 0s - loss: 523.8650 - val_loss: 481.1434\n",
      "Epoch 15/50\n",
      " - 0s - loss: 514.6820 - val_loss: 476.5841\n",
      "Epoch 16/50\n",
      " - 0s - loss: 509.3839 - val_loss: 472.1952\n",
      "Epoch 17/50\n",
      " - 0s - loss: 504.2659 - val_loss: 467.6766\n",
      "Epoch 18/50\n",
      " - 0s - loss: 499.1705 - val_loss: 462.5342\n",
      "Epoch 19/50\n",
      " - 0s - loss: 494.1376 - val_loss: 457.3248\n",
      "Epoch 20/50\n",
      " - 0s - loss: 488.6139 - val_loss: 452.2022\n",
      "Epoch 21/50\n",
      " - 0s - loss: 483.7968 - val_loss: 447.9470\n",
      "Epoch 22/50\n",
      " - 0s - loss: 477.7830 - val_loss: 442.2785\n",
      "Epoch 23/50\n",
      " - 0s - loss: 473.1169 - val_loss: 437.4911\n",
      "Epoch 24/50\n",
      " - 0s - loss: 467.3607 - val_loss: 431.1888\n",
      "Epoch 25/50\n",
      " - 0s - loss: 461.7992 - val_loss: 425.8167\n",
      "Epoch 26/50\n",
      " - 0s - loss: 456.5195 - val_loss: 420.8942\n",
      "Epoch 27/50\n",
      " - 0s - loss: 451.1000 - val_loss: 415.7532\n",
      "Epoch 28/50\n",
      " - 0s - loss: 446.3930 - val_loss: 411.4942\n",
      "Epoch 29/50\n",
      " - 0s - loss: 440.3168 - val_loss: 405.2151\n",
      "Epoch 30/50\n",
      " - 0s - loss: 435.1745 - val_loss: 399.4026\n",
      "Epoch 31/50\n",
      " - 0s - loss: 429.7079 - val_loss: 394.0334\n",
      "Epoch 32/50\n",
      " - 0s - loss: 424.1277 - val_loss: 389.4033\n",
      "Epoch 33/50\n",
      " - 0s - loss: 418.6876 - val_loss: 383.7503\n",
      "Epoch 34/50\n",
      " - 0s - loss: 413.5564 - val_loss: 378.4042\n",
      "Epoch 35/50\n",
      " - 0s - loss: 407.8234 - val_loss: 372.1502\n",
      "Epoch 36/50\n",
      " - 0s - loss: 402.7287 - val_loss: 365.8790\n",
      "Epoch 37/50\n",
      " - 0s - loss: 396.8780 - val_loss: 360.4561\n",
      "Epoch 38/50\n",
      " - 0s - loss: 391.4477 - val_loss: 355.4765\n",
      "Epoch 39/50\n",
      " - 0s - loss: 385.7973 - val_loss: 349.2183\n",
      "Epoch 40/50\n",
      " - 0s - loss: 380.3311 - val_loss: 343.5051\n",
      "Epoch 41/50\n",
      " - 0s - loss: 375.2454 - val_loss: 337.7696\n",
      "Epoch 42/50\n",
      " - 0s - loss: 369.8595 - val_loss: 332.8004\n",
      "Epoch 43/50\n",
      " - 0s - loss: 364.7496 - val_loss: 327.8215\n",
      "Epoch 44/50\n",
      " - 0s - loss: 360.0490 - val_loss: 323.3549\n",
      "Epoch 45/50\n",
      " - 0s - loss: 355.8863 - val_loss: 317.5574\n",
      "Epoch 46/50\n",
      " - 0s - loss: 350.8895 - val_loss: 314.2388\n",
      "Epoch 47/50\n",
      " - 0s - loss: 346.3755 - val_loss: 309.1319\n",
      "Epoch 48/50\n",
      " - 0s - loss: 342.5315 - val_loss: 306.1738\n",
      "Epoch 49/50\n",
      " - 0s - loss: 337.9686 - val_loss: 299.8284\n",
      "Epoch 50/50\n",
      " - 0s - loss: 334.4918 - val_loss: 294.6937\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 13063.2358 - val_loss: 8221.6067\n",
      "Epoch 2/50\n",
      " - 0s - loss: 5616.6398 - val_loss: 3564.8908\n",
      "Epoch 3/50\n",
      " - 0s - loss: 3016.2576 - val_loss: 2381.6563\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2338.3179 - val_loss: 1955.3698\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1944.3884 - val_loss: 1641.7621\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1671.2669 - val_loss: 1447.1452\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1515.2736 - val_loss: 1339.5316\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1422.0482 - val_loss: 1277.7563\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1357.5878 - val_loss: 1233.3302\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1312.5950 - val_loss: 1200.6743\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1275.6606 - val_loss: 1170.9853\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1245.3068 - val_loss: 1143.5477\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1214.7955 - val_loss: 1112.0455\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1183.9651 - val_loss: 1085.4142\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1156.5194 - val_loss: 1057.5403\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1131.6272 - val_loss: 1032.7751\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1100.9987 - val_loss: 1004.4287\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1074.2725 - val_loss: 980.7478\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1048.4550 - val_loss: 955.6789\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1024.2211 - val_loss: 932.6928\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1003.1318 - val_loss: 915.8102\n",
      "Epoch 22/50\n",
      " - 0s - loss: 975.4762 - val_loss: 887.2283\n",
      "Epoch 23/50\n",
      " - 0s - loss: 952.2777 - val_loss: 864.4219\n",
      "Epoch 24/50\n",
      " - 0s - loss: 929.2283 - val_loss: 845.2368\n",
      "Epoch 25/50\n",
      " - 0s - loss: 907.7248 - val_loss: 825.8798\n",
      "Epoch 26/50\n",
      " - 0s - loss: 886.2863 - val_loss: 803.4617\n",
      "Epoch 27/50\n",
      " - 0s - loss: 866.1298 - val_loss: 784.9593\n",
      "Epoch 28/50\n",
      " - 0s - loss: 847.6782 - val_loss: 768.4311\n",
      "Epoch 29/50\n",
      " - 0s - loss: 827.0716 - val_loss: 747.2051\n",
      "Epoch 30/50\n",
      " - 0s - loss: 807.7187 - val_loss: 730.5838\n",
      "Epoch 31/50\n",
      " - 0s - loss: 790.4038 - val_loss: 713.9734\n",
      "Epoch 32/50\n",
      " - 0s - loss: 771.8313 - val_loss: 697.1108\n",
      "Epoch 33/50\n",
      " - 0s - loss: 754.4789 - val_loss: 681.5459\n",
      "Epoch 34/50\n",
      " - 0s - loss: 737.5289 - val_loss: 666.2473\n",
      "Epoch 35/50\n",
      " - 0s - loss: 721.8372 - val_loss: 652.8510\n",
      "Epoch 36/50\n",
      " - 0s - loss: 706.2849 - val_loss: 636.8298\n",
      "Epoch 37/50\n",
      " - 0s - loss: 691.0414 - val_loss: 622.2572\n",
      "Epoch 38/50\n",
      " - 0s - loss: 676.6266 - val_loss: 609.8546\n",
      "Epoch 39/50\n",
      " - 0s - loss: 662.1366 - val_loss: 596.3098\n",
      "Epoch 40/50\n",
      " - 0s - loss: 648.9125 - val_loss: 583.2553\n",
      "Epoch 41/50\n",
      " - 0s - loss: 638.7136 - val_loss: 569.9807\n",
      "Epoch 42/50\n",
      " - 0s - loss: 622.3763 - val_loss: 564.0506\n",
      "Epoch 43/50\n",
      " - 0s - loss: 608.7804 - val_loss: 548.3021\n",
      "Epoch 44/50\n",
      " - 0s - loss: 595.9714 - val_loss: 535.6975\n",
      "Epoch 45/50\n",
      " - 0s - loss: 585.1699 - val_loss: 525.8126\n",
      "Epoch 46/50\n",
      " - 0s - loss: 573.4400 - val_loss: 515.2937\n",
      "Epoch 47/50\n",
      " - 0s - loss: 562.2586 - val_loss: 505.0955\n",
      "Epoch 48/50\n",
      " - 0s - loss: 552.0057 - val_loss: 497.8991\n",
      "Epoch 49/50\n",
      " - 0s - loss: 540.9670 - val_loss: 486.2742\n",
      "Epoch 50/50\n",
      " - 0s - loss: 530.6749 - val_loss: 476.3367\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 23263.1807 - val_loss: 12232.5285\n",
      "Epoch 2/50\n",
      " - 0s - loss: 9787.3885 - val_loss: 5788.8105\n",
      "Epoch 3/50\n",
      " - 0s - loss: 5388.9027 - val_loss: 4376.8357\n",
      "Epoch 4/50\n",
      " - 0s - loss: 4325.0403 - val_loss: 4044.0812\n",
      "Epoch 5/50\n",
      " - 0s - loss: 3816.2836 - val_loss: 3499.9310\n",
      "Epoch 6/50\n",
      " - 0s - loss: 3353.0131 - val_loss: 2954.4112\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2923.7667 - val_loss: 2532.5084\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2551.3462 - val_loss: 2216.3915\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2219.9914 - val_loss: 1947.6093\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1930.9591 - val_loss: 1701.3106\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1682.8401 - val_loss: 1475.7129\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1452.2998 - val_loss: 1297.2207\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1270.7734 - val_loss: 1150.4273\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1133.4464 - val_loss: 1014.2490\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1010.3169 - val_loss: 926.7068\n",
      "Epoch 16/50\n",
      " - 0s - loss: 923.5554 - val_loss: 844.0933\n",
      "Epoch 17/50\n",
      " - 0s - loss: 848.0371 - val_loss: 783.4602\n",
      "Epoch 18/50\n",
      " - 0s - loss: 794.2451 - val_loss: 743.3312\n",
      "Epoch 19/50\n",
      " - 0s - loss: 748.2941 - val_loss: 703.4362\n",
      "Epoch 20/50\n",
      " - 0s - loss: 707.5888 - val_loss: 677.5574\n",
      "Epoch 21/50\n",
      " - 0s - loss: 677.7765 - val_loss: 656.2806\n",
      "Epoch 22/50\n",
      " - 0s - loss: 650.9781 - val_loss: 631.2000\n",
      "Epoch 23/50\n",
      " - 0s - loss: 629.4715 - val_loss: 603.6130\n",
      "Epoch 24/50\n",
      " - 0s - loss: 607.1833 - val_loss: 586.6279\n",
      "Epoch 25/50\n",
      " - 0s - loss: 586.9959 - val_loss: 563.3887\n",
      "Epoch 26/50\n",
      " - 0s - loss: 570.6781 - val_loss: 545.1891\n",
      "Epoch 27/50\n",
      " - 0s - loss: 553.8731 - val_loss: 520.8574\n",
      "Epoch 28/50\n",
      " - 0s - loss: 531.0868 - val_loss: 510.2836\n",
      "Epoch 29/50\n",
      " - 0s - loss: 519.8242 - val_loss: 487.2487\n",
      "Epoch 30/50\n",
      " - 0s - loss: 501.6668 - val_loss: 473.4423\n",
      "Epoch 31/50\n",
      " - 0s - loss: 482.7631 - val_loss: 457.4161\n",
      "Epoch 32/50\n",
      " - 0s - loss: 470.0287 - val_loss: 443.5988\n",
      "Epoch 33/50\n",
      " - 0s - loss: 454.0648 - val_loss: 429.8209\n",
      "Epoch 34/50\n",
      " - 0s - loss: 444.7903 - val_loss: 427.1468\n",
      "Epoch 35/50\n",
      " - 0s - loss: 434.2204 - val_loss: 403.0020\n",
      "Epoch 36/50\n",
      " - 0s - loss: 420.7151 - val_loss: 390.9818\n",
      "Epoch 37/50\n",
      " - 0s - loss: 410.7554 - val_loss: 379.2730\n",
      "Epoch 38/50\n",
      " - 0s - loss: 402.3582 - val_loss: 370.2873\n",
      "Epoch 39/50\n",
      " - 0s - loss: 392.6973 - val_loss: 358.5783\n",
      "Epoch 40/50\n",
      " - 0s - loss: 379.6987 - val_loss: 349.3363\n",
      "Epoch 41/50\n",
      " - 0s - loss: 372.0798 - val_loss: 339.8907\n",
      "Epoch 42/50\n",
      " - 0s - loss: 364.0629 - val_loss: 331.5684\n",
      "Epoch 43/50\n",
      " - 0s - loss: 358.6082 - val_loss: 323.0002\n",
      "Epoch 44/50\n",
      " - 0s - loss: 349.8857 - val_loss: 312.5975\n",
      "Epoch 45/50\n",
      " - 0s - loss: 345.9807 - val_loss: 305.3472\n",
      "Epoch 46/50\n",
      " - 0s - loss: 331.3112 - val_loss: 298.6455\n",
      "Epoch 47/50\n",
      " - 0s - loss: 324.4835 - val_loss: 291.3317\n",
      "Epoch 48/50\n",
      " - 0s - loss: 317.2316 - val_loss: 283.5204\n",
      "Epoch 49/50\n",
      " - 0s - loss: 309.0280 - val_loss: 276.7012\n",
      "Epoch 50/50\n",
      " - 0s - loss: 300.0654 - val_loss: 271.8168\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 43025.9670 - val_loss: 27627.1569\n",
      "Epoch 2/50\n",
      " - 0s - loss: 17500.4918 - val_loss: 9749.7800\n",
      "Epoch 3/50\n",
      " - 0s - loss: 5943.5936 - val_loss: 2951.7184\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2018.6024 - val_loss: 1115.1807\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1125.8190 - val_loss: 879.0436\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1015.8644 - val_loss: 864.9293\n",
      "Epoch 7/50\n",
      " - 0s - loss: 991.1149 - val_loss: 838.9341\n",
      "Epoch 8/50\n",
      " - 0s - loss: 960.1763 - val_loss: 811.8612\n",
      "Epoch 9/50\n",
      " - 0s - loss: 932.6612 - val_loss: 785.5414\n",
      "Epoch 10/50\n",
      " - 0s - loss: 904.2715 - val_loss: 762.0237\n",
      "Epoch 11/50\n",
      " - 0s - loss: 875.7492 - val_loss: 738.0261\n",
      "Epoch 12/50\n",
      " - 0s - loss: 846.7594 - val_loss: 715.0209\n",
      "Epoch 13/50\n",
      " - 0s - loss: 819.3423 - val_loss: 689.9672\n",
      "Epoch 14/50\n",
      " - 0s - loss: 791.6990 - val_loss: 666.3431\n",
      "Epoch 15/50\n",
      " - 0s - loss: 765.0989 - val_loss: 642.8414\n",
      "Epoch 16/50\n",
      " - 0s - loss: 738.0249 - val_loss: 622.6046\n",
      "Epoch 17/50\n",
      " - 0s - loss: 712.8230 - val_loss: 601.2578\n",
      "Epoch 18/50\n",
      " - 0s - loss: 689.7632 - val_loss: 582.3711\n",
      "Epoch 19/50\n",
      " - 0s - loss: 666.5379 - val_loss: 561.7614\n",
      "Epoch 20/50\n",
      " - 0s - loss: 643.2235 - val_loss: 544.9154\n",
      "Epoch 21/50\n",
      " - 0s - loss: 621.4662 - val_loss: 527.2295\n",
      "Epoch 22/50\n",
      " - 0s - loss: 602.1752 - val_loss: 510.8277\n",
      "Epoch 23/50\n",
      " - 0s - loss: 582.0531 - val_loss: 497.2395\n",
      "Epoch 24/50\n",
      " - 0s - loss: 564.3753 - val_loss: 483.2530\n",
      "Epoch 25/50\n",
      " - 0s - loss: 546.4234 - val_loss: 468.8400\n",
      "Epoch 26/50\n",
      " - 0s - loss: 530.5875 - val_loss: 457.2551\n",
      "Epoch 27/50\n",
      " - 0s - loss: 514.6508 - val_loss: 444.2246\n",
      "Epoch 28/50\n",
      " - 0s - loss: 499.9310 - val_loss: 432.8227\n",
      "Epoch 29/50\n",
      " - 0s - loss: 485.8610 - val_loss: 422.6403\n",
      "Epoch 30/50\n",
      " - 0s - loss: 472.5926 - val_loss: 414.7390\n",
      "Epoch 31/50\n",
      " - 0s - loss: 460.3690 - val_loss: 404.5582\n",
      "Epoch 32/50\n",
      " - 0s - loss: 448.3387 - val_loss: 394.2494\n",
      "Epoch 33/50\n",
      " - 0s - loss: 437.0196 - val_loss: 386.5768\n",
      "Epoch 34/50\n",
      " - 0s - loss: 426.7594 - val_loss: 380.6799\n",
      "Epoch 35/50\n",
      " - 0s - loss: 415.3298 - val_loss: 371.2334\n",
      "Epoch 36/50\n",
      " - 0s - loss: 405.6141 - val_loss: 363.6205\n",
      "Epoch 37/50\n",
      " - 0s - loss: 396.6042 - val_loss: 356.3036\n",
      "Epoch 38/50\n",
      " - 0s - loss: 387.3287 - val_loss: 349.9071\n",
      "Epoch 39/50\n",
      " - 0s - loss: 378.4076 - val_loss: 342.7792\n",
      "Epoch 40/50\n",
      " - 0s - loss: 370.0112 - val_loss: 335.8260\n",
      "Epoch 41/50\n",
      " - 0s - loss: 361.8525 - val_loss: 329.0135\n",
      "Epoch 42/50\n",
      " - 0s - loss: 353.3329 - val_loss: 323.2358\n",
      "Epoch 43/50\n",
      " - 0s - loss: 345.5764 - val_loss: 314.7909\n",
      "Epoch 44/50\n",
      " - 0s - loss: 337.6222 - val_loss: 310.4917\n",
      "Epoch 45/50\n",
      " - 0s - loss: 329.8747 - val_loss: 303.2422\n",
      "Epoch 46/50\n",
      " - 0s - loss: 322.3353 - val_loss: 298.3398\n",
      "Epoch 47/50\n",
      " - 0s - loss: 315.0640 - val_loss: 292.2827\n",
      "Epoch 48/50\n",
      " - 0s - loss: 308.9821 - val_loss: 285.1508\n",
      "Epoch 49/50\n",
      " - 0s - loss: 301.0678 - val_loss: 281.6758\n",
      "Epoch 50/50\n",
      " - 0s - loss: 295.3361 - val_loss: 275.9971\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 113853.2569 - val_loss: 77551.3654\n",
      "Epoch 2/50\n",
      " - 0s - loss: 49582.8349 - val_loss: 29996.3982\n",
      "Epoch 3/50\n",
      " - 0s - loss: 17635.8374 - val_loss: 10206.9418\n",
      "Epoch 4/50\n",
      " - 0s - loss: 5860.1176 - val_loss: 4345.3732\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2978.1568 - val_loss: 3264.6581\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2588.3380 - val_loss: 3136.2117\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2538.1001 - val_loss: 3077.7941\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2488.9743 - val_loss: 3019.0444\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2434.3523 - val_loss: 2964.1010\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2382.3829 - val_loss: 2912.1454\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2338.9517 - val_loss: 2856.2096\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2279.1814 - val_loss: 2784.1763\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2227.5029 - val_loss: 2720.4179\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2178.0595 - val_loss: 2661.2353\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2126.1186 - val_loss: 2600.9994\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2074.9829 - val_loss: 2542.5676\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2025.9409 - val_loss: 2483.0892\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1975.4230 - val_loss: 2425.2636\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1932.5880 - val_loss: 2362.0778\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1878.8456 - val_loss: 2310.3042\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1832.2133 - val_loss: 2255.2962\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1786.2292 - val_loss: 2194.3420\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1741.0245 - val_loss: 2138.5666\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1695.8473 - val_loss: 2085.4096\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1650.9271 - val_loss: 2038.1969\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1608.5635 - val_loss: 1980.7609\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1565.5175 - val_loss: 1935.3623\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1526.9039 - val_loss: 1880.5530\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1483.7081 - val_loss: 1831.4219\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1440.9201 - val_loss: 1788.3333\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1404.8270 - val_loss: 1737.9011\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1364.8716 - val_loss: 1694.9000\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1326.3855 - val_loss: 1649.1029\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1290.9906 - val_loss: 1604.2877\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1253.4522 - val_loss: 1559.2907\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1220.3116 - val_loss: 1518.4084\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1185.7739 - val_loss: 1477.8097\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1152.7232 - val_loss: 1437.8400\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1120.7076 - val_loss: 1399.5676\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1088.8032 - val_loss: 1357.7295\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1058.0023 - val_loss: 1322.7605\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1030.0858 - val_loss: 1284.1574\n",
      "Epoch 43/50\n",
      " - 0s - loss: 999.1263 - val_loss: 1253.5429\n",
      "Epoch 44/50\n",
      " - 0s - loss: 970.8494 - val_loss: 1214.4314\n",
      "Epoch 45/50\n",
      " - 0s - loss: 942.4281 - val_loss: 1182.0041\n",
      "Epoch 46/50\n",
      " - 0s - loss: 916.2744 - val_loss: 1149.4773\n",
      "Epoch 47/50\n",
      " - 0s - loss: 890.4236 - val_loss: 1118.6604\n",
      "Epoch 48/50\n",
      " - 0s - loss: 865.1109 - val_loss: 1086.1920\n",
      "Epoch 49/50\n",
      " - 0s - loss: 841.4835 - val_loss: 1055.5652\n",
      "Epoch 50/50\n",
      " - 0s - loss: 817.7648 - val_loss: 1026.5831\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 27323.7878 - val_loss: 12919.3659\n",
      "Epoch 2/50\n",
      " - 0s - loss: 7304.0572 - val_loss: 2687.2514\n",
      "Epoch 3/50\n",
      " - 0s - loss: 2307.9388 - val_loss: 2202.7820\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2056.4991 - val_loss: 1795.2073\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1694.3640 - val_loss: 1490.1334\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1483.0901 - val_loss: 1321.5834\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1308.0714 - val_loss: 1163.8752\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1147.2784 - val_loss: 1025.5649\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1008.7911 - val_loss: 922.1255\n",
      "Epoch 10/50\n",
      " - 0s - loss: 904.8529 - val_loss: 825.4899\n",
      "Epoch 11/50\n",
      " - 0s - loss: 816.6192 - val_loss: 754.6309\n",
      "Epoch 12/50\n",
      " - 0s - loss: 747.5745 - val_loss: 694.0504\n",
      "Epoch 13/50\n",
      " - 0s - loss: 689.1106 - val_loss: 644.2840\n",
      "Epoch 14/50\n",
      " - 0s - loss: 641.1610 - val_loss: 598.3197\n",
      "Epoch 15/50\n",
      " - 0s - loss: 598.0128 - val_loss: 558.0649\n",
      "Epoch 16/50\n",
      " - 0s - loss: 559.0598 - val_loss: 521.2508\n",
      "Epoch 17/50\n",
      " - 0s - loss: 523.5055 - val_loss: 491.2019\n",
      "Epoch 18/50\n",
      " - 0s - loss: 493.3320 - val_loss: 462.1673\n",
      "Epoch 19/50\n",
      " - 0s - loss: 466.2336 - val_loss: 435.2239\n",
      "Epoch 20/50\n",
      " - 0s - loss: 440.7987 - val_loss: 411.8367\n",
      "Epoch 21/50\n",
      " - 0s - loss: 418.8750 - val_loss: 389.5111\n",
      "Epoch 22/50\n",
      " - 0s - loss: 397.4415 - val_loss: 368.3750\n",
      "Epoch 23/50\n",
      " - 0s - loss: 378.3197 - val_loss: 349.6505\n",
      "Epoch 24/50\n",
      " - 0s - loss: 359.6953 - val_loss: 330.0795\n",
      "Epoch 25/50\n",
      " - 0s - loss: 343.0549 - val_loss: 312.2558\n",
      "Epoch 26/50\n",
      " - 0s - loss: 329.3968 - val_loss: 296.7537\n",
      "Epoch 27/50\n",
      " - 0s - loss: 313.9806 - val_loss: 280.9223\n",
      "Epoch 28/50\n",
      " - 0s - loss: 302.1327 - val_loss: 269.9332\n",
      "Epoch 29/50\n",
      " - 0s - loss: 286.5083 - val_loss: 255.1140\n",
      "Epoch 30/50\n",
      " - 0s - loss: 274.8650 - val_loss: 243.9064\n",
      "Epoch 31/50\n",
      " - 0s - loss: 264.1538 - val_loss: 234.8972\n",
      "Epoch 32/50\n",
      " - 0s - loss: 255.6285 - val_loss: 225.4530\n",
      "Epoch 33/50\n",
      " - 0s - loss: 246.5191 - val_loss: 220.6421\n",
      "Epoch 34/50\n",
      " - 0s - loss: 239.1026 - val_loss: 210.4191\n",
      "Epoch 35/50\n",
      " - 0s - loss: 231.9464 - val_loss: 203.6435\n",
      "Epoch 36/50\n",
      " - 0s - loss: 225.4111 - val_loss: 198.7620\n",
      "Epoch 37/50\n",
      " - 0s - loss: 219.8937 - val_loss: 190.6343\n",
      "Epoch 38/50\n",
      " - 0s - loss: 212.9158 - val_loss: 186.5050\n",
      "Epoch 39/50\n",
      " - 0s - loss: 207.7615 - val_loss: 179.6782\n",
      "Epoch 40/50\n",
      " - 0s - loss: 201.4163 - val_loss: 175.7235\n",
      "Epoch 41/50\n",
      " - 0s - loss: 197.0355 - val_loss: 169.4891\n",
      "Epoch 42/50\n",
      " - 0s - loss: 197.2794 - val_loss: 164.2209\n",
      "Epoch 43/50\n",
      " - 0s - loss: 189.3767 - val_loss: 164.3418\n",
      "Epoch 44/50\n",
      " - 0s - loss: 185.6356 - val_loss: 159.5665\n",
      "Epoch 45/50\n",
      " - 0s - loss: 182.9189 - val_loss: 157.1473\n",
      "Epoch 46/50\n",
      " - 0s - loss: 180.1786 - val_loss: 157.0147\n",
      "Epoch 47/50\n",
      " - 0s - loss: 177.3603 - val_loss: 149.4991\n",
      "Epoch 48/50\n",
      " - 0s - loss: 176.3109 - val_loss: 147.9499\n",
      "Epoch 49/50\n",
      " - 0s - loss: 173.2290 - val_loss: 145.5857\n",
      "Epoch 50/50\n",
      " - 0s - loss: 171.3267 - val_loss: 144.2642\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 515493.5258 - val_loss: 442649.7779\n",
      "Epoch 2/50\n",
      " - 0s - loss: 379473.8576 - val_loss: 318086.5916\n",
      "Epoch 3/50\n",
      " - 0s - loss: 265713.0186 - val_loss: 215978.6352\n",
      "Epoch 4/50\n",
      " - 0s - loss: 173523.5766 - val_loss: 133267.7090\n",
      "Epoch 5/50\n",
      " - 0s - loss: 103447.5942 - val_loss: 73338.0904\n",
      "Epoch 6/50\n",
      " - 0s - loss: 55699.4259 - val_loss: 38817.3339\n",
      "Epoch 7/50\n",
      " - 0s - loss: 30481.7657 - val_loss: 21372.3612\n",
      "Epoch 8/50\n",
      " - 0s - loss: 18536.5885 - val_loss: 13932.6002\n",
      "Epoch 9/50\n",
      " - 0s - loss: 13701.4590 - val_loss: 11058.0302\n",
      "Epoch 10/50\n",
      " - 0s - loss: 11886.5690 - val_loss: 9968.4929\n",
      "Epoch 11/50\n",
      " - 0s - loss: 11129.2021 - val_loss: 9434.2087\n",
      "Epoch 12/50\n",
      " - 0s - loss: 10665.7560 - val_loss: 9032.1351\n",
      "Epoch 13/50\n",
      " - 0s - loss: 10250.8155 - val_loss: 8681.8031\n",
      "Epoch 14/50\n",
      " - 0s - loss: 9849.7191 - val_loss: 8346.5779\n",
      "Epoch 15/50\n",
      " - 0s - loss: 9450.3309 - val_loss: 8015.5838\n",
      "Epoch 16/50\n",
      " - 0s - loss: 9069.6018 - val_loss: 7700.2034\n",
      "Epoch 17/50\n",
      " - 0s - loss: 8694.1020 - val_loss: 7390.3498\n",
      "Epoch 18/50\n",
      " - 0s - loss: 8341.2814 - val_loss: 7093.3846\n",
      "Epoch 19/50\n",
      " - 0s - loss: 7992.5040 - val_loss: 6803.7892\n",
      "Epoch 20/50\n",
      " - 0s - loss: 7666.0162 - val_loss: 6525.9155\n",
      "Epoch 21/50\n",
      " - 0s - loss: 7345.8904 - val_loss: 6256.8152\n",
      "Epoch 22/50\n",
      " - 0s - loss: 7028.8019 - val_loss: 6001.6053\n",
      "Epoch 23/50\n",
      " - 0s - loss: 6736.5720 - val_loss: 5760.2972\n",
      "Epoch 24/50\n",
      " - 0s - loss: 6459.4543 - val_loss: 5526.4310\n",
      "Epoch 25/50\n",
      " - 0s - loss: 6175.3224 - val_loss: 5297.2009\n",
      "Epoch 26/50\n",
      " - 0s - loss: 5916.6164 - val_loss: 5072.6186\n",
      "Epoch 27/50\n",
      " - 0s - loss: 5666.2158 - val_loss: 4869.4401\n",
      "Epoch 28/50\n",
      " - 0s - loss: 5420.6387 - val_loss: 4669.2019\n",
      "Epoch 29/50\n",
      " - 0s - loss: 5192.9130 - val_loss: 4476.3838\n",
      "Epoch 30/50\n",
      " - 0s - loss: 4967.0683 - val_loss: 4299.1909\n",
      "Epoch 31/50\n",
      " - 0s - loss: 4772.9652 - val_loss: 4124.2865\n",
      "Epoch 32/50\n",
      " - 0s - loss: 4568.3321 - val_loss: 3949.3232\n",
      "Epoch 33/50\n",
      " - 0s - loss: 4374.9120 - val_loss: 3789.4259\n",
      "Epoch 34/50\n",
      " - 0s - loss: 4186.7411 - val_loss: 3646.3357\n",
      "Epoch 35/50\n",
      " - 0s - loss: 4021.7597 - val_loss: 3500.4439\n",
      "Epoch 36/50\n",
      " - 0s - loss: 3851.5404 - val_loss: 3363.3955\n",
      "Epoch 37/50\n",
      " - 0s - loss: 3698.3072 - val_loss: 3237.5705\n",
      "Epoch 38/50\n",
      " - 0s - loss: 3541.3203 - val_loss: 3113.7976\n",
      "Epoch 39/50\n",
      " - 0s - loss: 3404.3474 - val_loss: 2991.2837\n",
      "Epoch 40/50\n",
      " - 0s - loss: 3265.2112 - val_loss: 2882.1663\n",
      "Epoch 41/50\n",
      " - 0s - loss: 3137.6475 - val_loss: 2780.1294\n",
      "Epoch 42/50\n",
      " - 0s - loss: 3010.9613 - val_loss: 2678.4594\n",
      "Epoch 43/50\n",
      " - 0s - loss: 2898.8158 - val_loss: 2581.9511\n",
      "Epoch 44/50\n",
      " - 0s - loss: 2784.8326 - val_loss: 2488.4651\n",
      "Epoch 45/50\n",
      " - 0s - loss: 2691.6289 - val_loss: 2407.2541\n",
      "Epoch 46/50\n",
      " - 0s - loss: 2578.2657 - val_loss: 2323.3373\n",
      "Epoch 47/50\n",
      " - 0s - loss: 2484.4034 - val_loss: 2245.8278\n",
      "Epoch 48/50\n",
      " - 0s - loss: 2395.6738 - val_loss: 2172.8399\n",
      "Epoch 49/50\n",
      " - 0s - loss: 2311.9293 - val_loss: 2106.0136\n",
      "Epoch 50/50\n",
      " - 0s - loss: 2237.4691 - val_loss: 2035.9420\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 116767.0827 - val_loss: 96025.7847\n",
      "Epoch 2/50\n",
      " - 0s - loss: 83673.8434 - val_loss: 67062.9659\n",
      "Epoch 3/50\n",
      " - 0s - loss: 57075.3252 - val_loss: 43457.3816\n",
      "Epoch 4/50\n",
      " - 0s - loss: 35535.7939 - val_loss: 25328.2603\n",
      "Epoch 5/50\n",
      " - 0s - loss: 19873.0937 - val_loss: 13222.3034\n",
      "Epoch 6/50\n",
      " - 0s - loss: 10016.8558 - val_loss: 6322.8155\n",
      "Epoch 7/50\n",
      " - 0s - loss: 4790.7360 - val_loss: 3071.0354\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2485.8380 - val_loss: 1867.7740\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1701.6754 - val_loss: 1528.9618\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1475.2876 - val_loss: 1450.5779\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1408.8589 - val_loss: 1412.4407\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1366.9413 - val_loss: 1372.7783\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1327.0691 - val_loss: 1331.3416\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1284.9711 - val_loss: 1290.8713\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1245.5312 - val_loss: 1249.9413\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1203.4154 - val_loss: 1209.9013\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1164.2185 - val_loss: 1173.6892\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1119.6555 - val_loss: 1131.1260\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1076.8214 - val_loss: 1089.9886\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1033.3320 - val_loss: 1051.4839\n",
      "Epoch 21/50\n",
      " - 0s - loss: 990.3056 - val_loss: 1012.3042\n",
      "Epoch 22/50\n",
      " - 0s - loss: 949.5863 - val_loss: 969.6067\n",
      "Epoch 23/50\n",
      " - 0s - loss: 905.7005 - val_loss: 925.1873\n",
      "Epoch 24/50\n",
      " - 0s - loss: 862.0803 - val_loss: 882.6889\n",
      "Epoch 25/50\n",
      " - 0s - loss: 819.7688 - val_loss: 839.1084\n",
      "Epoch 26/50\n",
      " - 0s - loss: 777.8314 - val_loss: 801.0257\n",
      "Epoch 27/50\n",
      " - 0s - loss: 740.7048 - val_loss: 767.7736\n",
      "Epoch 28/50\n",
      " - 0s - loss: 710.5620 - val_loss: 733.3188\n",
      "Epoch 29/50\n",
      " - 0s - loss: 680.5375 - val_loss: 702.2074\n",
      "Epoch 30/50\n",
      " - 0s - loss: 653.2123 - val_loss: 672.1733\n",
      "Epoch 31/50\n",
      " - 0s - loss: 627.5730 - val_loss: 645.0688\n",
      "Epoch 32/50\n",
      " - 0s - loss: 603.2598 - val_loss: 619.4865\n",
      "Epoch 33/50\n",
      " - 0s - loss: 581.0509 - val_loss: 593.5364\n",
      "Epoch 34/50\n",
      " - 0s - loss: 555.2662 - val_loss: 570.6414\n",
      "Epoch 35/50\n",
      " - 0s - loss: 535.2639 - val_loss: 546.8709\n",
      "Epoch 36/50\n",
      " - 0s - loss: 513.6543 - val_loss: 525.4064\n",
      "Epoch 37/50\n",
      " - 0s - loss: 494.0975 - val_loss: 503.3848\n",
      "Epoch 38/50\n",
      " - 0s - loss: 472.4021 - val_loss: 484.1152\n",
      "Epoch 39/50\n",
      " - 0s - loss: 454.7772 - val_loss: 465.4096\n",
      "Epoch 40/50\n",
      " - 0s - loss: 435.7359 - val_loss: 447.9453\n",
      "Epoch 41/50\n",
      " - 0s - loss: 418.0467 - val_loss: 432.2973\n",
      "Epoch 42/50\n",
      " - 0s - loss: 403.2301 - val_loss: 416.6324\n",
      "Epoch 43/50\n",
      " - 0s - loss: 386.5957 - val_loss: 401.7895\n",
      "Epoch 44/50\n",
      " - 0s - loss: 373.2676 - val_loss: 389.0346\n",
      "Epoch 45/50\n",
      " - 0s - loss: 361.2039 - val_loss: 375.5584\n",
      "Epoch 46/50\n",
      " - 0s - loss: 348.3492 - val_loss: 364.3122\n",
      "Epoch 47/50\n",
      " - 0s - loss: 336.3305 - val_loss: 352.8347\n",
      "Epoch 48/50\n",
      " - 0s - loss: 325.9680 - val_loss: 342.0207\n",
      "Epoch 49/50\n",
      " - 0s - loss: 314.2230 - val_loss: 331.2258\n",
      "Epoch 50/50\n",
      " - 0s - loss: 304.7666 - val_loss: 320.9831\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 87300.2572 - val_loss: 51091.3971\n",
      "Epoch 2/50\n",
      " - 0s - loss: 36669.0964 - val_loss: 18148.9207\n",
      "Epoch 3/50\n",
      " - 0s - loss: 13168.1616 - val_loss: 6929.9467\n",
      "Epoch 4/50\n",
      " - 0s - loss: 6558.7728 - val_loss: 5136.1173\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5576.0025 - val_loss: 5143.7324\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5408.7156 - val_loss: 4972.5384\n",
      "Epoch 7/50\n",
      " - 0s - loss: 5215.9518 - val_loss: 4822.5780\n",
      "Epoch 8/50\n",
      " - 0s - loss: 5034.9623 - val_loss: 4648.1208\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4867.0154 - val_loss: 4519.5357\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4692.8030 - val_loss: 4363.6689\n",
      "Epoch 11/50\n",
      " - 0s - loss: 4520.2738 - val_loss: 4223.9246\n",
      "Epoch 12/50\n",
      " - 0s - loss: 4342.1856 - val_loss: 4098.3253\n",
      "Epoch 13/50\n",
      " - 0s - loss: 4180.9357 - val_loss: 3939.2760\n",
      "Epoch 14/50\n",
      " - 0s - loss: 4013.0461 - val_loss: 3800.2130\n",
      "Epoch 15/50\n",
      " - 0s - loss: 3846.7704 - val_loss: 3665.6448\n",
      "Epoch 16/50\n",
      " - 0s - loss: 3693.6989 - val_loss: 3533.0923\n",
      "Epoch 17/50\n",
      " - 0s - loss: 3530.5111 - val_loss: 3386.3877\n",
      "Epoch 18/50\n",
      " - 0s - loss: 3396.0421 - val_loss: 3246.8367\n",
      "Epoch 19/50\n",
      " - 0s - loss: 3234.2481 - val_loss: 3128.6019\n",
      "Epoch 20/50\n",
      " - 0s - loss: 3089.0813 - val_loss: 3005.1448\n",
      "Epoch 21/50\n",
      " - 0s - loss: 2961.7900 - val_loss: 2872.8872\n",
      "Epoch 22/50\n",
      " - 0s - loss: 2825.9705 - val_loss: 2762.7521\n",
      "Epoch 23/50\n",
      " - 0s - loss: 2695.8885 - val_loss: 2644.1552\n",
      "Epoch 24/50\n",
      " - 0s - loss: 2583.5751 - val_loss: 2536.5130\n",
      "Epoch 25/50\n",
      " - 0s - loss: 2458.1596 - val_loss: 2422.8131\n",
      "Epoch 26/50\n",
      " - 0s - loss: 2357.3137 - val_loss: 2314.7081\n",
      "Epoch 27/50\n",
      " - 0s - loss: 2244.5324 - val_loss: 2233.5555\n",
      "Epoch 28/50\n",
      " - 0s - loss: 2144.6296 - val_loss: 2124.7072\n",
      "Epoch 29/50\n",
      " - 0s - loss: 2050.3930 - val_loss: 2031.9070\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1956.9638 - val_loss: 1942.2381\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1862.1835 - val_loss: 1857.0187\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1778.9672 - val_loss: 1776.5428\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1701.5230 - val_loss: 1697.9187\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1620.9107 - val_loss: 1619.6297\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1551.0560 - val_loss: 1550.5736\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1471.7560 - val_loss: 1476.2109\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1407.7057 - val_loss: 1408.1934\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1339.5931 - val_loss: 1342.8111\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1279.7073 - val_loss: 1281.6061\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1224.0940 - val_loss: 1219.7430\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1161.1943 - val_loss: 1165.2056\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1108.2435 - val_loss: 1107.2961\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1052.2882 - val_loss: 1055.6409\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1007.7119 - val_loss: 1005.0037\n",
      "Epoch 45/50\n",
      " - 0s - loss: 955.4691 - val_loss: 957.9122\n",
      "Epoch 46/50\n",
      " - 0s - loss: 909.8378 - val_loss: 909.8048\n",
      "Epoch 47/50\n",
      " - 0s - loss: 863.8117 - val_loss: 866.6034\n",
      "Epoch 48/50\n",
      " - 0s - loss: 827.9488 - val_loss: 823.9388\n",
      "Epoch 49/50\n",
      " - 0s - loss: 781.0558 - val_loss: 786.5115\n",
      "Epoch 50/50\n",
      " - 0s - loss: 749.8426 - val_loss: 746.0517\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 262204.4169 - val_loss: 206383.0480\n",
      "Epoch 2/50\n",
      " - 0s - loss: 160463.6415 - val_loss: 122540.8267\n",
      "Epoch 3/50\n",
      " - 0s - loss: 93113.0551 - val_loss: 69890.4245\n",
      "Epoch 4/50\n",
      " - 0s - loss: 51863.6889 - val_loss: 38393.6566\n",
      "Epoch 5/50\n",
      " - 0s - loss: 27786.2392 - val_loss: 20323.0172\n",
      "Epoch 6/50\n",
      " - 0s - loss: 14503.8615 - val_loss: 10575.4360\n",
      "Epoch 7/50\n",
      " - 0s - loss: 7588.1939 - val_loss: 5858.2835\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4440.6862 - val_loss: 3736.5387\n",
      "Epoch 9/50\n",
      " - 0s - loss: 3138.2052 - val_loss: 2865.3159\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2622.1887 - val_loss: 2539.4774\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2440.6564 - val_loss: 2401.8665\n",
      "Epoch 12/50\n",
      " - 0s - loss: 2358.3496 - val_loss: 2332.0272\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2308.1694 - val_loss: 2275.3146\n",
      "Epoch 14/50\n",
      " - 0s - loss: 2260.3720 - val_loss: 2229.4733\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2216.8917 - val_loss: 2183.8506\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2170.4827 - val_loss: 2141.4464\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2124.3149 - val_loss: 2096.1665\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2078.8414 - val_loss: 2050.1988\n",
      "Epoch 19/50\n",
      " - 0s - loss: 2032.7428 - val_loss: 2006.8769\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1990.0453 - val_loss: 1966.0955\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1942.6570 - val_loss: 1919.2981\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1897.5539 - val_loss: 1877.7326\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1855.5412 - val_loss: 1832.8224\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1810.6996 - val_loss: 1793.2158\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1770.4250 - val_loss: 1751.0448\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1727.0292 - val_loss: 1710.6430\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1683.6921 - val_loss: 1669.0243\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1645.8659 - val_loss: 1628.3044\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1603.7310 - val_loss: 1593.9015\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1564.6361 - val_loss: 1554.7132\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1525.7217 - val_loss: 1516.6802\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1488.7413 - val_loss: 1481.0248\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1452.9989 - val_loss: 1443.4721\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1416.0035 - val_loss: 1408.8496\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1380.4775 - val_loss: 1377.5610\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1347.7284 - val_loss: 1340.5130\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1312.3704 - val_loss: 1308.9622\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1278.7982 - val_loss: 1276.4804\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1247.0794 - val_loss: 1245.5791\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1216.6550 - val_loss: 1215.7430\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1187.0923 - val_loss: 1184.8090\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1156.0523 - val_loss: 1156.4323\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1127.2524 - val_loss: 1128.7693\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1098.8255 - val_loss: 1101.9960\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1072.6989 - val_loss: 1073.3921\n",
      "Epoch 46/50\n",
      " - 0s - loss: 1044.5422 - val_loss: 1047.8848\n",
      "Epoch 47/50\n",
      " - 0s - loss: 1019.0209 - val_loss: 1022.2896\n",
      "Epoch 48/50\n",
      " - 0s - loss: 994.8760 - val_loss: 998.6538\n",
      "Epoch 49/50\n",
      " - 0s - loss: 972.7599 - val_loss: 972.9698\n",
      "Epoch 50/50\n",
      " - 0s - loss: 945.1688 - val_loss: 950.0303\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 212217.3509 - val_loss: 152612.5284\n",
      "Epoch 2/50\n",
      " - 0s - loss: 119297.5108 - val_loss: 80567.3370\n",
      "Epoch 3/50\n",
      " - 0s - loss: 61260.1735 - val_loss: 40519.1465\n",
      "Epoch 4/50\n",
      " - 0s - loss: 31134.6347 - val_loss: 21047.6687\n",
      "Epoch 5/50\n",
      " - 0s - loss: 17334.8499 - val_loss: 13316.0309\n",
      "Epoch 6/50\n",
      " - 0s - loss: 12117.0268 - val_loss: 10821.3840\n",
      "Epoch 7/50\n",
      " - 0s - loss: 10381.6547 - val_loss: 10144.3202\n",
      "Epoch 8/50\n",
      " - 0s - loss: 9879.4498 - val_loss: 9921.4493\n",
      "Epoch 9/50\n",
      " - 0s - loss: 9671.2186 - val_loss: 9759.8974\n",
      "Epoch 10/50\n",
      " - 0s - loss: 9501.0137 - val_loss: 9590.5358\n",
      "Epoch 11/50\n",
      " - 0s - loss: 9330.7060 - val_loss: 9419.0451\n",
      "Epoch 12/50\n",
      " - 0s - loss: 9163.1046 - val_loss: 9225.5892\n",
      "Epoch 13/50\n",
      " - 0s - loss: 8985.0411 - val_loss: 9048.8206\n",
      "Epoch 14/50\n",
      " - 0s - loss: 8801.1326 - val_loss: 8852.4502\n",
      "Epoch 15/50\n",
      " - 0s - loss: 8637.5007 - val_loss: 8654.1088\n",
      "Epoch 16/50\n",
      " - 0s - loss: 8446.5352 - val_loss: 8476.6249\n",
      "Epoch 17/50\n",
      " - 0s - loss: 8264.9988 - val_loss: 8292.9668\n",
      "Epoch 18/50\n",
      " - 0s - loss: 8092.5895 - val_loss: 8098.6988\n",
      "Epoch 19/50\n",
      " - 0s - loss: 7910.1203 - val_loss: 7917.0453\n",
      "Epoch 20/50\n",
      " - 0s - loss: 7734.5230 - val_loss: 7734.5276\n",
      "Epoch 21/50\n",
      " - 0s - loss: 7560.7094 - val_loss: 7552.4358\n",
      "Epoch 22/50\n",
      " - 0s - loss: 7390.9399 - val_loss: 7383.1709\n",
      "Epoch 23/50\n",
      " - 0s - loss: 7220.4991 - val_loss: 7204.5863\n",
      "Epoch 24/50\n",
      " - 0s - loss: 7053.5239 - val_loss: 7026.6639\n",
      "Epoch 25/50\n",
      " - 0s - loss: 6885.2051 - val_loss: 6854.6338\n",
      "Epoch 26/50\n",
      " - 0s - loss: 6716.3868 - val_loss: 6696.3754\n",
      "Epoch 27/50\n",
      " - 0s - loss: 6559.5345 - val_loss: 6510.4809\n",
      "Epoch 28/50\n",
      " - 0s - loss: 6389.4662 - val_loss: 6355.3256\n",
      "Epoch 29/50\n",
      " - 0s - loss: 6239.0199 - val_loss: 6184.2848\n",
      "Epoch 30/50\n",
      " - 0s - loss: 6078.3162 - val_loss: 6011.7876\n",
      "Epoch 31/50\n",
      " - 0s - loss: 5931.4228 - val_loss: 5849.8160\n",
      "Epoch 32/50\n",
      " - 0s - loss: 5775.0274 - val_loss: 5708.8119\n",
      "Epoch 33/50\n",
      " - 0s - loss: 5638.1320 - val_loss: 5559.8776\n",
      "Epoch 34/50\n",
      " - 0s - loss: 5486.7571 - val_loss: 5396.2552\n",
      "Epoch 35/50\n",
      " - 0s - loss: 5342.3891 - val_loss: 5244.9526\n",
      "Epoch 36/50\n",
      " - 0s - loss: 5193.2891 - val_loss: 5099.8573\n",
      "Epoch 37/50\n",
      " - 0s - loss: 5045.8950 - val_loss: 4949.2653\n",
      "Epoch 38/50\n",
      " - 0s - loss: 4896.0939 - val_loss: 4796.3114\n",
      "Epoch 39/50\n",
      " - 0s - loss: 4755.4252 - val_loss: 4643.1428\n",
      "Epoch 40/50\n",
      " - 0s - loss: 4608.6445 - val_loss: 4490.7068\n",
      "Epoch 41/50\n",
      " - 0s - loss: 4468.4791 - val_loss: 4339.2206\n",
      "Epoch 42/50\n",
      " - 0s - loss: 4322.5001 - val_loss: 4192.0731\n",
      "Epoch 43/50\n",
      " - 0s - loss: 4183.4316 - val_loss: 4051.0324\n",
      "Epoch 44/50\n",
      " - 0s - loss: 4017.2650 - val_loss: 3886.7782\n",
      "Epoch 45/50\n",
      " - 0s - loss: 3875.3231 - val_loss: 3725.4893\n",
      "Epoch 46/50\n",
      " - 0s - loss: 3713.3150 - val_loss: 3579.8995\n",
      "Epoch 47/50\n",
      " - 0s - loss: 3575.3689 - val_loss: 3439.6360\n",
      "Epoch 48/50\n",
      " - 0s - loss: 3440.6064 - val_loss: 3299.6382\n",
      "Epoch 49/50\n",
      " - 0s - loss: 3314.9576 - val_loss: 3183.6108\n",
      "Epoch 50/50\n",
      " - 0s - loss: 3202.1435 - val_loss: 3075.6549\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 925748.9087 - val_loss: 789524.9562\n",
      "Epoch 2/50\n",
      " - 0s - loss: 657916.0724 - val_loss: 550540.3960\n",
      "Epoch 3/50\n",
      " - 0s - loss: 451638.3016 - val_loss: 372063.5079\n",
      "Epoch 4/50\n",
      " - 0s - loss: 300183.4968 - val_loss: 243686.2793\n",
      "Epoch 5/50\n",
      " - 0s - loss: 193090.3068 - val_loss: 154094.7861\n",
      "Epoch 6/50\n",
      " - 0s - loss: 120097.9937 - val_loss: 93621.9918\n",
      "Epoch 7/50\n",
      " - 0s - loss: 71482.8377 - val_loss: 54975.1534\n",
      "Epoch 8/50\n",
      " - 0s - loss: 41088.6495 - val_loss: 31041.6249\n",
      "Epoch 9/50\n",
      " - 0s - loss: 22748.0193 - val_loss: 17004.4742\n",
      "Epoch 10/50\n",
      " - 0s - loss: 12273.1642 - val_loss: 9187.6742\n",
      "Epoch 11/50\n",
      " - 0s - loss: 6622.8281 - val_loss: 5076.6388\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3772.6485 - val_loss: 3009.4985\n",
      "Epoch 13/50\n",
      " - 0s - loss: 2401.7883 - val_loss: 2028.1291\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1785.2098 - val_loss: 1575.2408\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1517.2822 - val_loss: 1373.3636\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1399.7618 - val_loss: 1287.5961\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1353.3355 - val_loss: 1239.2356\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1331.1355 - val_loss: 1207.8121\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1311.7110 - val_loss: 1191.3282\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1298.9558 - val_loss: 1176.3223\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1284.7175 - val_loss: 1166.1581\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1271.8489 - val_loss: 1153.5664\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1258.5863 - val_loss: 1142.2079\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1245.3501 - val_loss: 1129.4666\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1231.8823 - val_loss: 1117.4608\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1219.3605 - val_loss: 1105.8927\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1205.2244 - val_loss: 1095.0597\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1192.2801 - val_loss: 1082.1747\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1178.6931 - val_loss: 1071.2924\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1166.1555 - val_loss: 1060.0243\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1152.3338 - val_loss: 1048.0857\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1140.1503 - val_loss: 1038.1954\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1126.1234 - val_loss: 1025.2238\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1112.9538 - val_loss: 1013.7921\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1099.8336 - val_loss: 1001.6601\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1088.2436 - val_loss: 989.1458\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1074.3893 - val_loss: 979.2479\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1062.4632 - val_loss: 971.5598\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1049.3310 - val_loss: 958.2398\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1036.9728 - val_loss: 946.1334\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1024.3561 - val_loss: 937.1804\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1013.5303 - val_loss: 928.4850\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1000.7682 - val_loss: 917.5378\n",
      "Epoch 44/50\n",
      " - 0s - loss: 988.7968 - val_loss: 903.6533\n",
      "Epoch 45/50\n",
      " - 0s - loss: 976.8272 - val_loss: 894.1936\n",
      "Epoch 46/50\n",
      " - 0s - loss: 965.0062 - val_loss: 884.2718\n",
      "Epoch 47/50\n",
      " - 0s - loss: 953.6370 - val_loss: 876.3986\n",
      "Epoch 48/50\n",
      " - 0s - loss: 943.3534 - val_loss: 867.0753\n",
      "Epoch 49/50\n",
      " - 0s - loss: 931.4163 - val_loss: 855.6919\n",
      "Epoch 50/50\n",
      " - 0s - loss: 920.7701 - val_loss: 845.3683\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 233375.1567 - val_loss: 171562.6658\n",
      "Epoch 2/50\n",
      " - 0s - loss: 138267.3661 - val_loss: 96498.8210\n",
      "Epoch 3/50\n",
      " - 0s - loss: 76240.1146 - val_loss: 50654.3248\n",
      "Epoch 4/50\n",
      " - 0s - loss: 39719.0884 - val_loss: 25303.5028\n",
      "Epoch 5/50\n",
      " - 0s - loss: 20349.6296 - val_loss: 12638.6779\n",
      "Epoch 6/50\n",
      " - 0s - loss: 10892.8841 - val_loss: 7137.0599\n",
      "Epoch 7/50\n",
      " - 0s - loss: 6977.2076 - val_loss: 4981.5037\n",
      "Epoch 8/50\n",
      " - 0s - loss: 5538.6833 - val_loss: 4250.0790\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4931.2874 - val_loss: 4052.3018\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4765.2754 - val_loss: 3942.6062\n",
      "Epoch 11/50\n",
      " - 0s - loss: 4629.6818 - val_loss: 3862.5106\n",
      "Epoch 12/50\n",
      " - 0s - loss: 4514.7760 - val_loss: 3776.7128\n",
      "Epoch 13/50\n",
      " - 0s - loss: 4401.2519 - val_loss: 3687.9528\n",
      "Epoch 14/50\n",
      " - 0s - loss: 4286.2724 - val_loss: 3595.5599\n",
      "Epoch 15/50\n",
      " - 0s - loss: 4177.9989 - val_loss: 3507.4292\n",
      "Epoch 16/50\n",
      " - 0s - loss: 4060.4382 - val_loss: 3415.5962\n",
      "Epoch 17/50\n",
      " - 0s - loss: 3947.2208 - val_loss: 3327.1405\n",
      "Epoch 18/50\n",
      " - 0s - loss: 3837.2273 - val_loss: 3241.3746\n",
      "Epoch 19/50\n",
      " - 0s - loss: 3727.1697 - val_loss: 3155.8002\n",
      "Epoch 20/50\n",
      " - 0s - loss: 3615.8196 - val_loss: 3067.3482\n",
      "Epoch 21/50\n",
      " - 0s - loss: 3506.8925 - val_loss: 2979.8032\n",
      "Epoch 22/50\n",
      " - 0s - loss: 3399.1186 - val_loss: 2897.5778\n",
      "Epoch 23/50\n",
      " - 0s - loss: 3294.7913 - val_loss: 2820.5935\n",
      "Epoch 24/50\n",
      " - 0s - loss: 3192.7332 - val_loss: 2738.3585\n",
      "Epoch 25/50\n",
      " - 0s - loss: 3092.9228 - val_loss: 2658.4411\n",
      "Epoch 26/50\n",
      " - 0s - loss: 2993.7419 - val_loss: 2585.0794\n",
      "Epoch 27/50\n",
      " - 0s - loss: 2899.4307 - val_loss: 2507.8824\n",
      "Epoch 28/50\n",
      " - 0s - loss: 2804.1932 - val_loss: 2435.7806\n",
      "Epoch 29/50\n",
      " - 0s - loss: 2713.8773 - val_loss: 2363.9360\n",
      "Epoch 30/50\n",
      " - 0s - loss: 2623.0519 - val_loss: 2296.3201\n",
      "Epoch 31/50\n",
      " - 0s - loss: 2538.9266 - val_loss: 2229.9103\n",
      "Epoch 32/50\n",
      " - 0s - loss: 2454.8622 - val_loss: 2160.5945\n",
      "Epoch 33/50\n",
      " - 0s - loss: 2371.8053 - val_loss: 2095.1393\n",
      "Epoch 34/50\n",
      " - 0s - loss: 2292.9002 - val_loss: 2033.0010\n",
      "Epoch 35/50\n",
      " - 0s - loss: 2215.9628 - val_loss: 1974.6851\n",
      "Epoch 36/50\n",
      " - 0s - loss: 2142.6678 - val_loss: 1917.0680\n",
      "Epoch 37/50\n",
      " - 0s - loss: 2074.0233 - val_loss: 1860.9580\n",
      "Epoch 38/50\n",
      " - 0s - loss: 2003.5421 - val_loss: 1803.9075\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1935.6728 - val_loss: 1750.4814\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1871.5223 - val_loss: 1699.2612\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1810.6940 - val_loss: 1651.4313\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1751.2621 - val_loss: 1601.3422\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1690.4391 - val_loss: 1553.6046\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1635.0990 - val_loss: 1508.8513\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1581.5346 - val_loss: 1467.5879\n",
      "Epoch 46/50\n",
      " - 0s - loss: 1528.5459 - val_loss: 1425.3360\n",
      "Epoch 47/50\n",
      " - 0s - loss: 1477.9537 - val_loss: 1384.6312\n",
      "Epoch 48/50\n",
      " - 0s - loss: 1430.1053 - val_loss: 1345.5380\n",
      "Epoch 49/50\n",
      " - 0s - loss: 1383.1466 - val_loss: 1308.3737\n",
      "Epoch 50/50\n",
      " - 0s - loss: 1339.3620 - val_loss: 1272.2154\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 31240.2141 - val_loss: 17866.3639\n",
      "Epoch 2/50\n",
      " - 0s - loss: 10015.6182 - val_loss: 7516.3004\n",
      "Epoch 3/50\n",
      " - 0s - loss: 7524.7610 - val_loss: 6694.3416\n",
      "Epoch 4/50\n",
      " - 0s - loss: 7026.4356 - val_loss: 6215.4487\n",
      "Epoch 5/50\n",
      " - 0s - loss: 6359.8319 - val_loss: 5923.0575\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5854.2182 - val_loss: 5368.0541\n",
      "Epoch 7/50\n",
      " - 0s - loss: 5382.8458 - val_loss: 4957.9042\n",
      "Epoch 8/50\n",
      " - 0s - loss: 4955.6485 - val_loss: 4582.6513\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4565.1609 - val_loss: 4210.9909\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4225.4805 - val_loss: 3908.7037\n",
      "Epoch 11/50\n",
      " - 0s - loss: 3908.8783 - val_loss: 3620.3752\n",
      "Epoch 12/50\n",
      " - 0s - loss: 3634.1912 - val_loss: 3344.7126\n",
      "Epoch 13/50\n",
      " - 0s - loss: 3386.7953 - val_loss: 3171.3300\n",
      "Epoch 14/50\n",
      " - 0s - loss: 3151.6788 - val_loss: 2957.3089\n",
      "Epoch 15/50\n",
      " - 0s - loss: 2963.8697 - val_loss: 2744.0949\n",
      "Epoch 16/50\n",
      " - 0s - loss: 2779.2422 - val_loss: 2597.9297\n",
      "Epoch 17/50\n",
      " - 0s - loss: 2628.7706 - val_loss: 2483.5357\n",
      "Epoch 18/50\n",
      " - 0s - loss: 2479.1508 - val_loss: 2332.5539\n",
      "Epoch 19/50\n",
      " - 0s - loss: 2367.4806 - val_loss: 2231.1288\n",
      "Epoch 20/50\n",
      " - 0s - loss: 2244.0482 - val_loss: 2115.2534\n",
      "Epoch 21/50\n",
      " - 0s - loss: 2141.4824 - val_loss: 2023.4899\n",
      "Epoch 22/50\n",
      " - 0s - loss: 2038.3892 - val_loss: 1958.6794\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1954.8474 - val_loss: 1860.4803\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1867.0968 - val_loss: 1796.1234\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1791.3685 - val_loss: 1726.5936\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1724.9878 - val_loss: 1657.9391\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1654.6792 - val_loss: 1612.5544\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1596.5846 - val_loss: 1535.6524\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1538.9556 - val_loss: 1488.1741\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1481.1624 - val_loss: 1437.8164\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1430.5240 - val_loss: 1387.0026\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1380.5218 - val_loss: 1345.9167\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1334.7421 - val_loss: 1306.3780\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1299.2750 - val_loss: 1251.6136\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1258.8398 - val_loss: 1233.7648\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1213.5356 - val_loss: 1174.0633\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1167.2498 - val_loss: 1150.7181\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1132.6292 - val_loss: 1113.2594\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1099.7760 - val_loss: 1069.5188\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1060.9394 - val_loss: 1050.3279\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1025.7424 - val_loss: 1008.7038\n",
      "Epoch 42/50\n",
      " - 0s - loss: 997.0232 - val_loss: 977.8666\n",
      "Epoch 43/50\n",
      " - 0s - loss: 964.7027 - val_loss: 958.4466\n",
      "Epoch 44/50\n",
      " - 0s - loss: 936.7054 - val_loss: 923.5332\n",
      "Epoch 45/50\n",
      " - 0s - loss: 907.0009 - val_loss: 899.2761\n",
      "Epoch 46/50\n",
      " - 0s - loss: 882.3956 - val_loss: 871.3321\n",
      "Epoch 47/50\n",
      " - 0s - loss: 868.3261 - val_loss: 862.4913\n",
      "Epoch 48/50\n",
      " - 0s - loss: 832.1049 - val_loss: 819.0033\n",
      "Epoch 49/50\n",
      " - 0s - loss: 805.9284 - val_loss: 813.3472\n",
      "Epoch 50/50\n",
      " - 0s - loss: 781.5316 - val_loss: 783.0504\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 1s - loss: 64832.5024 - val_loss: 53727.6687\n",
      "Epoch 2/50\n",
      " - 0s - loss: 44529.1146 - val_loss: 35716.7522\n",
      "Epoch 3/50\n",
      " - 0s - loss: 28511.6588 - val_loss: 22071.8930\n",
      "Epoch 4/50\n",
      " - 0s - loss: 16565.6399 - val_loss: 12109.9332\n",
      "Epoch 5/50\n",
      " - 0s - loss: 8456.3357 - val_loss: 6060.8796\n",
      "Epoch 6/50\n",
      " - 0s - loss: 4395.1359 - val_loss: 3974.2828\n",
      "Epoch 7/50\n",
      " - 0s - loss: 3278.7525 - val_loss: 3414.0606\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2823.3997 - val_loss: 2926.2194\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2423.8109 - val_loss: 2554.6945\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2123.4883 - val_loss: 2257.7972\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1875.3462 - val_loss: 2015.5974\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1667.5102 - val_loss: 1806.8908\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1503.3637 - val_loss: 1631.0390\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1344.9310 - val_loss: 1482.7745\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1223.5981 - val_loss: 1357.2323\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1118.5054 - val_loss: 1244.5055\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1026.5530 - val_loss: 1147.7702\n",
      "Epoch 18/50\n",
      " - 0s - loss: 948.2621 - val_loss: 1066.4580\n",
      "Epoch 19/50\n",
      " - 0s - loss: 880.6606 - val_loss: 994.9089\n",
      "Epoch 20/50\n",
      " - 0s - loss: 820.8034 - val_loss: 929.5193\n",
      "Epoch 21/50\n",
      " - 0s - loss: 768.2834 - val_loss: 873.7464\n",
      "Epoch 22/50\n",
      " - 0s - loss: 722.5931 - val_loss: 822.6229\n",
      "Epoch 23/50\n",
      " - 0s - loss: 683.0580 - val_loss: 778.6686\n",
      "Epoch 24/50\n",
      " - 0s - loss: 645.8577 - val_loss: 735.3617\n",
      "Epoch 25/50\n",
      " - 0s - loss: 613.1832 - val_loss: 699.5820\n",
      "Epoch 26/50\n",
      " - 0s - loss: 582.9344 - val_loss: 667.8730\n",
      "Epoch 27/50\n",
      " - 0s - loss: 558.1150 - val_loss: 637.4562\n",
      "Epoch 28/50\n",
      " - 0s - loss: 534.7520 - val_loss: 610.4633\n",
      "Epoch 29/50\n",
      " - 0s - loss: 513.8551 - val_loss: 586.5168\n",
      "Epoch 30/50\n",
      " - 0s - loss: 493.8817 - val_loss: 565.4520\n",
      "Epoch 31/50\n",
      " - 0s - loss: 476.9634 - val_loss: 542.9555\n",
      "Epoch 32/50\n",
      " - 0s - loss: 460.1903 - val_loss: 524.8265\n",
      "Epoch 33/50\n",
      " - 0s - loss: 445.0219 - val_loss: 506.2317\n",
      "Epoch 34/50\n",
      " - 0s - loss: 431.8218 - val_loss: 490.8280\n",
      "Epoch 35/50\n",
      " - 0s - loss: 419.2383 - val_loss: 475.3440\n",
      "Epoch 36/50\n",
      " - 0s - loss: 405.3642 - val_loss: 462.4618\n",
      "Epoch 37/50\n",
      " - 0s - loss: 394.2995 - val_loss: 448.4536\n",
      "Epoch 38/50\n",
      " - 0s - loss: 383.8058 - val_loss: 436.0299\n",
      "Epoch 39/50\n",
      " - 0s - loss: 372.8013 - val_loss: 425.1141\n",
      "Epoch 40/50\n",
      " - 0s - loss: 363.9985 - val_loss: 413.6043\n",
      "Epoch 41/50\n",
      " - 0s - loss: 355.0567 - val_loss: 403.3831\n",
      "Epoch 42/50\n",
      " - 0s - loss: 345.9061 - val_loss: 393.2514\n",
      "Epoch 43/50\n",
      " - 0s - loss: 338.4624 - val_loss: 383.4172\n",
      "Epoch 44/50\n",
      " - 0s - loss: 330.3287 - val_loss: 376.8297\n",
      "Epoch 45/50\n",
      " - 0s - loss: 322.2922 - val_loss: 365.9392\n",
      "Epoch 46/50\n",
      " - 0s - loss: 315.0985 - val_loss: 358.5330\n",
      "Epoch 47/50\n",
      " - 0s - loss: 308.2857 - val_loss: 351.6315\n",
      "Epoch 48/50\n",
      " - 0s - loss: 301.0128 - val_loss: 342.7564\n",
      "Epoch 49/50\n",
      " - 0s - loss: 294.6760 - val_loss: 335.8514\n",
      "Epoch 50/50\n",
      " - 0s - loss: 288.6624 - val_loss: 328.7975\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 79845.7652 - val_loss: 46613.4316\n",
      "Epoch 2/50\n",
      " - 0s - loss: 30138.1472 - val_loss: 14858.0670\n",
      "Epoch 3/50\n",
      " - 0s - loss: 9038.9788 - val_loss: 4608.4910\n",
      "Epoch 4/50\n",
      " - 0s - loss: 3260.5146 - val_loss: 2964.3530\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2468.9254 - val_loss: 2873.5733\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2362.5513 - val_loss: 2763.7799\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2249.0055 - val_loss: 2618.1342\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2134.9800 - val_loss: 2467.0095\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1991.8667 - val_loss: 2291.5585\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1822.5993 - val_loss: 2091.7481\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1653.2953 - val_loss: 1914.3446\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1519.6528 - val_loss: 1774.3106\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1403.2067 - val_loss: 1649.8547\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1304.7133 - val_loss: 1535.8939\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1213.1911 - val_loss: 1444.3634\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1132.9447 - val_loss: 1343.7295\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1050.3803 - val_loss: 1260.5667\n",
      "Epoch 18/50\n",
      " - 0s - loss: 984.1104 - val_loss: 1175.8713\n",
      "Epoch 19/50\n",
      " - 0s - loss: 917.6722 - val_loss: 1102.4116\n",
      "Epoch 20/50\n",
      " - 0s - loss: 861.1414 - val_loss: 1025.7879\n",
      "Epoch 21/50\n",
      " - 0s - loss: 799.6621 - val_loss: 961.1529\n",
      "Epoch 22/50\n",
      " - 0s - loss: 753.1609 - val_loss: 897.9062\n",
      "Epoch 23/50\n",
      " - 0s - loss: 701.1410 - val_loss: 841.6080\n",
      "Epoch 24/50\n",
      " - 0s - loss: 664.6447 - val_loss: 784.5529\n",
      "Epoch 25/50\n",
      " - 0s - loss: 622.0599 - val_loss: 743.8896\n",
      "Epoch 26/50\n",
      " - 0s - loss: 588.7655 - val_loss: 688.4510\n",
      "Epoch 27/50\n",
      " - 0s - loss: 548.9499 - val_loss: 641.2403\n",
      "Epoch 28/50\n",
      " - 0s - loss: 507.9958 - val_loss: 589.1747\n",
      "Epoch 29/50\n",
      " - 0s - loss: 468.8287 - val_loss: 546.7896\n",
      "Epoch 30/50\n",
      " - 0s - loss: 438.4940 - val_loss: 506.6151\n",
      "Epoch 31/50\n",
      " - 0s - loss: 410.2477 - val_loss: 477.0823\n",
      "Epoch 32/50\n",
      " - 0s - loss: 390.4734 - val_loss: 443.2032\n",
      "Epoch 33/50\n",
      " - 0s - loss: 365.1123 - val_loss: 419.0289\n",
      "Epoch 34/50\n",
      " - 0s - loss: 345.8729 - val_loss: 397.6351\n",
      "Epoch 35/50\n",
      " - 0s - loss: 330.6507 - val_loss: 375.6980\n",
      "Epoch 36/50\n",
      " - 0s - loss: 310.4083 - val_loss: 353.6379\n",
      "Epoch 37/50\n",
      " - 0s - loss: 293.7754 - val_loss: 330.0626\n",
      "Epoch 38/50\n",
      " - 0s - loss: 279.4915 - val_loss: 308.3187\n",
      "Epoch 39/50\n",
      " - 0s - loss: 263.4785 - val_loss: 290.0587\n",
      "Epoch 40/50\n",
      " - 0s - loss: 246.8173 - val_loss: 276.3362\n",
      "Epoch 41/50\n",
      " - 0s - loss: 239.4356 - val_loss: 266.1413\n",
      "Epoch 42/50\n",
      " - 0s - loss: 225.5473 - val_loss: 253.1295\n",
      "Epoch 43/50\n",
      " - 0s - loss: 217.3996 - val_loss: 244.1349\n",
      "Epoch 44/50\n",
      " - 0s - loss: 209.1433 - val_loss: 234.5836\n",
      "Epoch 45/50\n",
      " - 0s - loss: 202.4418 - val_loss: 227.9963\n",
      "Epoch 46/50\n",
      " - 0s - loss: 196.8163 - val_loss: 220.8337\n",
      "Epoch 47/50\n",
      " - 0s - loss: 192.2352 - val_loss: 215.0335\n",
      "Epoch 48/50\n",
      " - 0s - loss: 190.2011 - val_loss: 210.3692\n",
      "Epoch 49/50\n",
      " - 0s - loss: 183.8994 - val_loss: 208.0565\n",
      "Epoch 50/50\n",
      " - 0s - loss: 182.3628 - val_loss: 203.0244\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 15020.1286 - val_loss: 3644.3917\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1713.5560 - val_loss: 1080.7475\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1286.3607 - val_loss: 964.7151\n",
      "Epoch 4/50\n",
      " - 0s - loss: 923.3855 - val_loss: 720.6078\n",
      "Epoch 5/50\n",
      " - 0s - loss: 845.6153 - val_loss: 637.0028\n",
      "Epoch 6/50\n",
      " - 0s - loss: 750.6160 - val_loss: 569.4922\n",
      "Epoch 7/50\n",
      " - 0s - loss: 672.8431 - val_loss: 494.4622\n",
      "Epoch 8/50\n",
      " - 0s - loss: 593.7471 - val_loss: 430.4182\n",
      "Epoch 9/50\n",
      " - 0s - loss: 526.8841 - val_loss: 375.1545\n",
      "Epoch 10/50\n",
      " - 0s - loss: 471.4049 - val_loss: 333.0563\n",
      "Epoch 11/50\n",
      " - 0s - loss: 427.7551 - val_loss: 299.0771\n",
      "Epoch 12/50\n",
      " - 0s - loss: 388.4235 - val_loss: 271.7673\n",
      "Epoch 13/50\n",
      " - 0s - loss: 357.3578 - val_loss: 250.5214\n",
      "Epoch 14/50\n",
      " - 0s - loss: 329.9436 - val_loss: 232.8726\n",
      "Epoch 15/50\n",
      " - 0s - loss: 307.7733 - val_loss: 219.9122\n",
      "Epoch 16/50\n",
      " - 0s - loss: 289.7602 - val_loss: 209.2257\n",
      "Epoch 17/50\n",
      " - 0s - loss: 274.6689 - val_loss: 200.4228\n",
      "Epoch 18/50\n",
      " - 0s - loss: 264.3466 - val_loss: 193.8570\n",
      "Epoch 19/50\n",
      " - 0s - loss: 251.4576 - val_loss: 182.9441\n",
      "Epoch 20/50\n",
      " - 0s - loss: 240.0886 - val_loss: 176.6169\n",
      "Epoch 21/50\n",
      " - 0s - loss: 231.3261 - val_loss: 169.3048\n",
      "Epoch 22/50\n",
      " - 0s - loss: 222.3161 - val_loss: 160.2053\n",
      "Epoch 23/50\n",
      " - 0s - loss: 213.7224 - val_loss: 156.1690\n",
      "Epoch 24/50\n",
      " - 0s - loss: 206.1165 - val_loss: 149.6948\n",
      "Epoch 25/50\n",
      " - 0s - loss: 197.6450 - val_loss: 140.0271\n",
      "Epoch 26/50\n",
      " - 0s - loss: 189.4651 - val_loss: 134.5926\n",
      "Epoch 27/50\n",
      " - 0s - loss: 183.7556 - val_loss: 130.8387\n",
      "Epoch 28/50\n",
      " - 0s - loss: 176.9560 - val_loss: 127.3338\n",
      "Epoch 29/50\n",
      " - 0s - loss: 171.1517 - val_loss: 121.5503\n",
      "Epoch 30/50\n",
      " - 0s - loss: 167.7299 - val_loss: 120.7885\n",
      "Epoch 31/50\n",
      " - 0s - loss: 162.3369 - val_loss: 117.3886\n",
      "Epoch 32/50\n",
      " - 0s - loss: 159.3172 - val_loss: 113.5244\n",
      "Epoch 33/50\n",
      " - 0s - loss: 156.5787 - val_loss: 116.0212\n",
      "Epoch 34/50\n",
      " - 0s - loss: 153.4520 - val_loss: 111.8707\n",
      "Epoch 35/50\n",
      " - 0s - loss: 151.2041 - val_loss: 109.6191\n",
      "Epoch 36/50\n",
      " - 0s - loss: 148.3760 - val_loss: 109.8021\n",
      "Epoch 37/50\n",
      " - 0s - loss: 146.5746 - val_loss: 105.7104\n",
      "Epoch 38/50\n",
      " - 0s - loss: 144.9870 - val_loss: 106.1099\n",
      "Epoch 39/50\n",
      " - 0s - loss: 144.2174 - val_loss: 110.5260\n",
      "Epoch 40/50\n",
      " - 0s - loss: 143.0143 - val_loss: 104.5913\n",
      "Epoch 41/50\n",
      " - 0s - loss: 139.6915 - val_loss: 103.0317\n",
      "Epoch 42/50\n",
      " - 0s - loss: 139.0010 - val_loss: 103.4184\n",
      "Epoch 43/50\n",
      " - 0s - loss: 137.4870 - val_loss: 101.3743\n",
      "Epoch 44/50\n",
      " - 0s - loss: 136.1012 - val_loss: 99.8569\n",
      "Epoch 45/50\n",
      " - 0s - loss: 135.5164 - val_loss: 107.4654\n",
      "Epoch 46/50\n",
      " - 0s - loss: 134.7954 - val_loss: 98.8610\n",
      "Epoch 47/50\n",
      " - 0s - loss: 132.3579 - val_loss: 98.2359\n",
      "Epoch 48/50\n",
      " - 0s - loss: 131.1602 - val_loss: 101.6227\n",
      "Epoch 49/50\n",
      " - 0s - loss: 131.5096 - val_loss: 99.3315\n",
      "Epoch 50/50\n",
      " - 0s - loss: 129.7778 - val_loss: 101.5851\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 48685.1901 - val_loss: 27508.4603\n",
      "Epoch 2/50\n",
      " - 0s - loss: 15820.5721 - val_loss: 6965.3411\n",
      "Epoch 3/50\n",
      " - 0s - loss: 3433.1406 - val_loss: 1356.6911\n",
      "Epoch 4/50\n",
      " - 0s - loss: 987.2548 - val_loss: 770.4199\n",
      "Epoch 5/50\n",
      " - 0s - loss: 880.0070 - val_loss: 754.3451\n",
      "Epoch 6/50\n",
      " - 0s - loss: 835.7275 - val_loss: 718.0428\n",
      "Epoch 7/50\n",
      " - 0s - loss: 783.9678 - val_loss: 693.6920\n",
      "Epoch 8/50\n",
      " - 0s - loss: 749.6379 - val_loss: 670.8703\n",
      "Epoch 9/50\n",
      " - 0s - loss: 719.1722 - val_loss: 641.7853\n",
      "Epoch 10/50\n",
      " - 0s - loss: 686.9136 - val_loss: 614.1652\n",
      "Epoch 11/50\n",
      " - 0s - loss: 654.8780 - val_loss: 588.9784\n",
      "Epoch 12/50\n",
      " - 0s - loss: 625.4300 - val_loss: 561.8230\n",
      "Epoch 13/50\n",
      " - 0s - loss: 596.7284 - val_loss: 537.6056\n",
      "Epoch 14/50\n",
      " - 0s - loss: 569.4805 - val_loss: 514.7740\n",
      "Epoch 15/50\n",
      " - 0s - loss: 543.1232 - val_loss: 494.4775\n",
      "Epoch 16/50\n",
      " - 0s - loss: 518.2236 - val_loss: 472.0386\n",
      "Epoch 17/50\n",
      " - 0s - loss: 495.4043 - val_loss: 452.6898\n",
      "Epoch 18/50\n",
      " - 0s - loss: 473.5537 - val_loss: 435.1876\n",
      "Epoch 19/50\n",
      " - 0s - loss: 453.3036 - val_loss: 418.2726\n",
      "Epoch 20/50\n",
      " - 0s - loss: 435.6793 - val_loss: 403.1232\n",
      "Epoch 21/50\n",
      " - 0s - loss: 416.6331 - val_loss: 386.3297\n",
      "Epoch 22/50\n",
      " - 0s - loss: 401.4481 - val_loss: 372.3959\n",
      "Epoch 23/50\n",
      " - 0s - loss: 386.6535 - val_loss: 360.6176\n",
      "Epoch 24/50\n",
      " - 0s - loss: 372.7799 - val_loss: 347.8428\n",
      "Epoch 25/50\n",
      " - 0s - loss: 360.9013 - val_loss: 340.7546\n",
      "Epoch 26/50\n",
      " - 0s - loss: 348.4674 - val_loss: 327.4459\n",
      "Epoch 27/50\n",
      " - 0s - loss: 337.9982 - val_loss: 317.6479\n",
      "Epoch 28/50\n",
      " - 0s - loss: 328.2136 - val_loss: 309.2808\n",
      "Epoch 29/50\n",
      " - 0s - loss: 319.2456 - val_loss: 301.7079\n",
      "Epoch 30/50\n",
      " - 0s - loss: 311.3132 - val_loss: 296.0512\n",
      "Epoch 31/50\n",
      " - 0s - loss: 303.0026 - val_loss: 286.4839\n",
      "Epoch 32/50\n",
      " - 0s - loss: 295.4877 - val_loss: 282.4827\n",
      "Epoch 33/50\n",
      " - 0s - loss: 288.6065 - val_loss: 273.6369\n",
      "Epoch 34/50\n",
      " - 0s - loss: 282.8032 - val_loss: 269.0728\n",
      "Epoch 35/50\n",
      " - 0s - loss: 275.7346 - val_loss: 260.4514\n",
      "Epoch 36/50\n",
      " - 0s - loss: 270.7605 - val_loss: 257.8754\n",
      "Epoch 37/50\n",
      " - 0s - loss: 265.1330 - val_loss: 252.8956\n",
      "Epoch 38/50\n",
      " - 0s - loss: 260.3163 - val_loss: 247.3677\n",
      "Epoch 39/50\n",
      " - 0s - loss: 255.8710 - val_loss: 242.6723\n",
      "Epoch 40/50\n",
      " - 0s - loss: 250.9954 - val_loss: 237.3423\n",
      "Epoch 41/50\n",
      " - 0s - loss: 246.4821 - val_loss: 235.1172\n",
      "Epoch 42/50\n",
      " - 0s - loss: 243.0335 - val_loss: 231.9201\n",
      "Epoch 43/50\n",
      " - 0s - loss: 238.2885 - val_loss: 225.8188\n",
      "Epoch 44/50\n",
      " - 0s - loss: 235.5526 - val_loss: 221.1271\n",
      "Epoch 45/50\n",
      " - 0s - loss: 230.9056 - val_loss: 220.1332\n",
      "Epoch 46/50\n",
      " - 0s - loss: 227.9522 - val_loss: 213.5417\n",
      "Epoch 47/50\n",
      " - 0s - loss: 223.7334 - val_loss: 211.6435\n",
      "Epoch 48/50\n",
      " - 0s - loss: 222.7725 - val_loss: 206.8605\n",
      "Epoch 49/50\n",
      " - 0s - loss: 218.2162 - val_loss: 202.6073\n",
      "Epoch 50/50\n",
      " - 0s - loss: 214.8629 - val_loss: 201.7484\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 4600.1625 - val_loss: 1332.6615\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1245.2049 - val_loss: 553.7511\n",
      "Epoch 3/50\n",
      " - 0s - loss: 643.4884 - val_loss: 486.1319\n",
      "Epoch 4/50\n",
      " - 0s - loss: 496.2424 - val_loss: 417.1202\n",
      "Epoch 5/50\n",
      " - 0s - loss: 430.3037 - val_loss: 355.0898\n",
      "Epoch 6/50\n",
      " - 0s - loss: 358.8072 - val_loss: 306.8870\n",
      "Epoch 7/50\n",
      " - 0s - loss: 305.4375 - val_loss: 267.7638\n",
      "Epoch 8/50\n",
      " - 0s - loss: 266.8630 - val_loss: 234.9293\n",
      "Epoch 9/50\n",
      " - 0s - loss: 235.5273 - val_loss: 213.1181\n",
      "Epoch 10/50\n",
      " - 0s - loss: 208.4123 - val_loss: 194.2654\n",
      "Epoch 11/50\n",
      " - 0s - loss: 188.6900 - val_loss: 180.9304\n",
      "Epoch 12/50\n",
      " - 0s - loss: 173.6142 - val_loss: 170.2052\n",
      "Epoch 13/50\n",
      " - 0s - loss: 163.0863 - val_loss: 165.2209\n",
      "Epoch 14/50\n",
      " - 0s - loss: 150.7282 - val_loss: 157.6041\n",
      "Epoch 15/50\n",
      " - 0s - loss: 146.8841 - val_loss: 156.0094\n",
      "Epoch 16/50\n",
      " - 0s - loss: 140.5914 - val_loss: 150.1032\n",
      "Epoch 17/50\n",
      " - 0s - loss: 137.1647 - val_loss: 147.6220\n",
      "Epoch 18/50\n",
      " - 0s - loss: 136.1548 - val_loss: 147.1658\n",
      "Epoch 19/50\n",
      " - 0s - loss: 135.1729 - val_loss: 150.2185\n",
      "Epoch 20/50\n",
      " - 0s - loss: 131.3430 - val_loss: 149.1305\n",
      "Epoch 21/50\n",
      " - 0s - loss: 129.6926 - val_loss: 143.6918\n",
      "Epoch 22/50\n",
      " - 0s - loss: 127.1420 - val_loss: 139.7459\n",
      "Epoch 23/50\n",
      " - 0s - loss: 125.3186 - val_loss: 138.5499\n",
      "Epoch 24/50\n",
      " - 0s - loss: 125.7239 - val_loss: 137.4288\n",
      "Epoch 25/50\n",
      " - 0s - loss: 124.3372 - val_loss: 138.5888\n",
      "Epoch 26/50\n",
      " - 0s - loss: 125.0137 - val_loss: 135.3232\n",
      "Epoch 27/50\n",
      " - 0s - loss: 123.4795 - val_loss: 136.9883\n",
      "Epoch 28/50\n",
      " - 0s - loss: 122.6909 - val_loss: 144.4363\n",
      "Epoch 29/50\n",
      " - 0s - loss: 120.0892 - val_loss: 132.0182\n",
      "Epoch 30/50\n",
      " - 0s - loss: 120.1698 - val_loss: 130.8211\n",
      "Epoch 31/50\n",
      " - 0s - loss: 120.0784 - val_loss: 130.8765\n",
      "Epoch 32/50\n",
      " - 0s - loss: 118.7291 - val_loss: 130.3420\n",
      "Epoch 33/50\n",
      " - 0s - loss: 119.1958 - val_loss: 127.7768\n",
      "Epoch 34/50\n",
      " - 0s - loss: 118.1565 - val_loss: 126.9789\n",
      "Epoch 35/50\n",
      " - 0s - loss: 116.3681 - val_loss: 126.1650\n",
      "Epoch 36/50\n",
      " - 0s - loss: 119.7230 - val_loss: 125.7545\n",
      "Epoch 37/50\n",
      " - 0s - loss: 115.3445 - val_loss: 128.8676\n",
      "Epoch 38/50\n",
      " - 0s - loss: 115.4332 - val_loss: 132.2302\n",
      "Epoch 39/50\n",
      " - 0s - loss: 125.3893 - val_loss: 136.0647\n",
      "Epoch 40/50\n",
      " - 0s - loss: 117.2585 - val_loss: 126.6355\n",
      "Epoch 41/50\n",
      " - 0s - loss: 112.1959 - val_loss: 126.2269\n",
      "Epoch 42/50\n",
      " - 0s - loss: 120.7392 - val_loss: 122.2878\n",
      "Epoch 43/50\n",
      " - 0s - loss: 115.0470 - val_loss: 122.1317\n",
      "Epoch 44/50\n",
      " - 0s - loss: 114.8395 - val_loss: 121.9578\n",
      "Epoch 45/50\n",
      " - 0s - loss: 110.7236 - val_loss: 124.4897\n",
      "Epoch 46/50\n",
      " - 0s - loss: 112.7491 - val_loss: 125.0232\n",
      "Epoch 47/50\n",
      " - 0s - loss: 112.3298 - val_loss: 123.9876\n",
      "Epoch 48/50\n",
      " - 0s - loss: 124.1512 - val_loss: 123.7441\n",
      "Epoch 49/50\n",
      " - 0s - loss: 115.2973 - val_loss: 134.0961\n",
      "Epoch 50/50\n",
      " - 0s - loss: 116.6198 - val_loss: 121.9401\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 247477.3127 - val_loss: 191899.1313\n",
      "Epoch 2/50\n",
      " - 0s - loss: 149715.8446 - val_loss: 111806.0355\n",
      "Epoch 3/50\n",
      " - 0s - loss: 84778.6468 - val_loss: 61981.2726\n",
      "Epoch 4/50\n",
      " - 0s - loss: 45766.2267 - val_loss: 32746.5580\n",
      "Epoch 5/50\n",
      " - 0s - loss: 23612.4479 - val_loss: 16748.8790\n",
      "Epoch 6/50\n",
      " - 0s - loss: 11894.5808 - val_loss: 8652.0822\n",
      "Epoch 7/50\n",
      " - 0s - loss: 6181.2035 - val_loss: 4791.7111\n",
      "Epoch 8/50\n",
      " - 0s - loss: 3640.5266 - val_loss: 3071.3055\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2579.1124 - val_loss: 2383.5196\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2145.1442 - val_loss: 2131.7988\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2010.1937 - val_loss: 2005.5607\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1934.6912 - val_loss: 1942.6100\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1889.2543 - val_loss: 1898.3416\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1852.4648 - val_loss: 1858.1211\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1817.3318 - val_loss: 1819.6703\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1779.1513 - val_loss: 1785.9328\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1743.7605 - val_loss: 1749.4260\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1707.1716 - val_loss: 1713.9579\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1674.4241 - val_loss: 1676.8468\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1637.8432 - val_loss: 1643.2826\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1603.3958 - val_loss: 1609.3268\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1568.3033 - val_loss: 1577.1603\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1536.1388 - val_loss: 1542.8187\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1504.7713 - val_loss: 1509.9015\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1470.6572 - val_loss: 1476.8441\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1438.2499 - val_loss: 1447.5947\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1408.3633 - val_loss: 1421.3147\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1379.9072 - val_loss: 1392.7437\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1352.4514 - val_loss: 1359.7729\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1324.5974 - val_loss: 1332.2223\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1297.8620 - val_loss: 1307.3452\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1272.9394 - val_loss: 1288.4356\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1248.2172 - val_loss: 1262.1623\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1224.1883 - val_loss: 1237.8721\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1202.9677 - val_loss: 1212.5228\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1182.7208 - val_loss: 1187.8616\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1164.2375 - val_loss: 1175.1621\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1141.0060 - val_loss: 1152.4043\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1122.3135 - val_loss: 1131.9878\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1107.3743 - val_loss: 1110.5097\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1085.7782 - val_loss: 1100.8009\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1069.5087 - val_loss: 1081.9638\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1053.6730 - val_loss: 1065.3414\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1037.8273 - val_loss: 1049.5105\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1025.1250 - val_loss: 1029.6671\n",
      "Epoch 46/50\n",
      " - 0s - loss: 1009.2963 - val_loss: 1022.5327\n",
      "Epoch 47/50\n",
      " - 0s - loss: 995.6830 - val_loss: 1004.0687\n",
      "Epoch 48/50\n",
      " - 0s - loss: 981.1366 - val_loss: 990.7995\n",
      "Epoch 49/50\n",
      " - 0s - loss: 968.9335 - val_loss: 976.7795\n",
      "Epoch 50/50\n",
      " - 0s - loss: 956.2123 - val_loss: 965.0714\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 71558.7914 - val_loss: 56579.7627\n",
      "Epoch 2/50\n",
      " - 0s - loss: 42530.8525 - val_loss: 32319.9980\n",
      "Epoch 3/50\n",
      " - 0s - loss: 22553.5451 - val_loss: 15544.3365\n",
      "Epoch 4/50\n",
      " - 0s - loss: 10434.0217 - val_loss: 6769.4865\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5509.8926 - val_loss: 4249.0372\n",
      "Epoch 6/50\n",
      " - 0s - loss: 4487.4177 - val_loss: 3666.3097\n",
      "Epoch 7/50\n",
      " - 0s - loss: 3890.7208 - val_loss: 3009.5350\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2940.2445 - val_loss: 2266.7852\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2159.7458 - val_loss: 1693.0377\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1607.0279 - val_loss: 1212.1768\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1157.7707 - val_loss: 863.5861\n",
      "Epoch 12/50\n",
      " - 0s - loss: 853.4890 - val_loss: 682.6149\n",
      "Epoch 13/50\n",
      " - 0s - loss: 680.0426 - val_loss: 568.7931\n",
      "Epoch 14/50\n",
      " - 0s - loss: 570.3241 - val_loss: 503.3935\n",
      "Epoch 15/50\n",
      " - 0s - loss: 510.1603 - val_loss: 460.5652\n",
      "Epoch 16/50\n",
      " - 0s - loss: 476.8172 - val_loss: 440.4561\n",
      "Epoch 17/50\n",
      " - 0s - loss: 456.4522 - val_loss: 413.4516\n",
      "Epoch 18/50\n",
      " - 0s - loss: 429.7342 - val_loss: 399.8767\n",
      "Epoch 19/50\n",
      " - 0s - loss: 417.2167 - val_loss: 384.2681\n",
      "Epoch 20/50\n",
      " - 0s - loss: 403.4283 - val_loss: 372.0275\n",
      "Epoch 21/50\n",
      " - 0s - loss: 394.3873 - val_loss: 369.6613\n",
      "Epoch 22/50\n",
      " - 0s - loss: 396.2738 - val_loss: 349.3805\n",
      "Epoch 23/50\n",
      " - 0s - loss: 367.7023 - val_loss: 349.1052\n",
      "Epoch 24/50\n",
      " - 0s - loss: 357.1026 - val_loss: 325.6441\n",
      "Epoch 25/50\n",
      " - 0s - loss: 349.3997 - val_loss: 316.3104\n",
      "Epoch 26/50\n",
      " - 0s - loss: 338.0322 - val_loss: 305.3007\n",
      "Epoch 27/50\n",
      " - 0s - loss: 332.2210 - val_loss: 298.3527\n",
      "Epoch 28/50\n",
      " - 0s - loss: 322.0777 - val_loss: 287.9507\n",
      "Epoch 29/50\n",
      " - 0s - loss: 311.2656 - val_loss: 277.8584\n",
      "Epoch 30/50\n",
      " - 0s - loss: 302.4261 - val_loss: 270.1846\n",
      "Epoch 31/50\n",
      " - 0s - loss: 295.4324 - val_loss: 261.6323\n",
      "Epoch 32/50\n",
      " - 0s - loss: 285.4402 - val_loss: 254.4116\n",
      "Epoch 33/50\n",
      " - 0s - loss: 279.8572 - val_loss: 250.4717\n",
      "Epoch 34/50\n",
      " - 0s - loss: 274.4884 - val_loss: 243.2506\n",
      "Epoch 35/50\n",
      " - 0s - loss: 264.4563 - val_loss: 232.7221\n",
      "Epoch 36/50\n",
      " - 0s - loss: 258.3974 - val_loss: 226.1357\n",
      "Epoch 37/50\n",
      " - 0s - loss: 251.1847 - val_loss: 219.6319\n",
      "Epoch 38/50\n",
      " - 0s - loss: 245.2824 - val_loss: 219.2440\n",
      "Epoch 39/50\n",
      " - 0s - loss: 238.5433 - val_loss: 214.8634\n",
      "Epoch 40/50\n",
      " - 0s - loss: 233.0606 - val_loss: 206.7206\n",
      "Epoch 41/50\n",
      " - 0s - loss: 228.8549 - val_loss: 201.1745\n",
      "Epoch 42/50\n",
      " - 0s - loss: 222.1608 - val_loss: 195.4718\n",
      "Epoch 43/50\n",
      " - 0s - loss: 216.1179 - val_loss: 193.0423\n",
      "Epoch 44/50\n",
      " - 0s - loss: 215.9843 - val_loss: 187.2829\n",
      "Epoch 45/50\n",
      " - 0s - loss: 206.7580 - val_loss: 182.4791\n",
      "Epoch 46/50\n",
      " - 0s - loss: 202.2193 - val_loss: 177.8744\n",
      "Epoch 47/50\n",
      " - 0s - loss: 201.1317 - val_loss: 183.4380\n",
      "Epoch 48/50\n",
      " - 0s - loss: 195.3413 - val_loss: 172.7678\n",
      "Epoch 49/50\n",
      " - 0s - loss: 190.6324 - val_loss: 175.7870\n",
      "Epoch 50/50\n",
      " - 0s - loss: 191.5488 - val_loss: 164.8032\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 43460.2985 - val_loss: 19032.4520\n",
      "Epoch 2/50\n",
      " - 0s - loss: 10556.7334 - val_loss: 3742.1698\n",
      "Epoch 3/50\n",
      " - 0s - loss: 3676.9101 - val_loss: 3185.3842\n",
      "Epoch 4/50\n",
      " - 0s - loss: 3513.3947 - val_loss: 2921.7887\n",
      "Epoch 5/50\n",
      " - 0s - loss: 3183.5770 - val_loss: 2659.5267\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2979.8853 - val_loss: 2508.7725\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2804.3091 - val_loss: 2370.2272\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2631.2311 - val_loss: 2222.0891\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2470.8971 - val_loss: 2086.3223\n",
      "Epoch 10/50\n",
      " - 0s - loss: 2296.6599 - val_loss: 1941.5128\n",
      "Epoch 11/50\n",
      " - 0s - loss: 2138.9336 - val_loss: 1813.8675\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1980.7660 - val_loss: 1688.9285\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1845.1632 - val_loss: 1566.4827\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1707.2672 - val_loss: 1448.7218\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1573.4774 - val_loss: 1352.3703\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1450.4474 - val_loss: 1249.6358\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1339.9607 - val_loss: 1157.2225\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1235.0193 - val_loss: 1070.5081\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1141.8588 - val_loss: 988.9421\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1045.1231 - val_loss: 919.9424\n",
      "Epoch 21/50\n",
      " - 0s - loss: 963.9324 - val_loss: 849.9488\n",
      "Epoch 22/50\n",
      " - 0s - loss: 888.3332 - val_loss: 788.6814\n",
      "Epoch 23/50\n",
      " - 0s - loss: 825.8341 - val_loss: 730.5819\n",
      "Epoch 24/50\n",
      " - 0s - loss: 761.7433 - val_loss: 675.9044\n",
      "Epoch 25/50\n",
      " - 0s - loss: 703.0090 - val_loss: 627.3095\n",
      "Epoch 26/50\n",
      " - 0s - loss: 648.3136 - val_loss: 583.1794\n",
      "Epoch 27/50\n",
      " - 0s - loss: 604.0388 - val_loss: 542.0626\n",
      "Epoch 28/50\n",
      " - 0s - loss: 558.8489 - val_loss: 505.6329\n",
      "Epoch 29/50\n",
      " - 0s - loss: 520.1174 - val_loss: 471.1094\n",
      "Epoch 30/50\n",
      " - 0s - loss: 485.3882 - val_loss: 440.5524\n",
      "Epoch 31/50\n",
      " - 0s - loss: 457.1566 - val_loss: 412.0168\n",
      "Epoch 32/50\n",
      " - 0s - loss: 427.1185 - val_loss: 386.3058\n",
      "Epoch 33/50\n",
      " - 0s - loss: 404.0949 - val_loss: 366.0806\n",
      "Epoch 34/50\n",
      " - 0s - loss: 380.5380 - val_loss: 342.8257\n",
      "Epoch 35/50\n",
      " - 0s - loss: 355.9462 - val_loss: 324.9438\n",
      "Epoch 36/50\n",
      " - 0s - loss: 336.8123 - val_loss: 310.1514\n",
      "Epoch 37/50\n",
      " - 0s - loss: 322.5920 - val_loss: 293.0299\n",
      "Epoch 38/50\n",
      " - 0s - loss: 305.4305 - val_loss: 278.7979\n",
      "Epoch 39/50\n",
      " - 0s - loss: 291.1227 - val_loss: 267.5598\n",
      "Epoch 40/50\n",
      " - 0s - loss: 279.0544 - val_loss: 257.1178\n",
      "Epoch 41/50\n",
      " - 0s - loss: 269.3587 - val_loss: 246.3131\n",
      "Epoch 42/50\n",
      " - 0s - loss: 258.6822 - val_loss: 238.6206\n",
      "Epoch 43/50\n",
      " - 0s - loss: 250.6389 - val_loss: 229.6044\n",
      "Epoch 44/50\n",
      " - 0s - loss: 242.7511 - val_loss: 223.2034\n",
      "Epoch 45/50\n",
      " - 0s - loss: 237.4535 - val_loss: 217.0536\n",
      "Epoch 46/50\n",
      " - 0s - loss: 230.9559 - val_loss: 210.2907\n",
      "Epoch 47/50\n",
      " - 0s - loss: 224.6461 - val_loss: 205.2062\n",
      "Epoch 48/50\n",
      " - 0s - loss: 219.9026 - val_loss: 200.1755\n",
      "Epoch 49/50\n",
      " - 0s - loss: 216.8637 - val_loss: 196.0815\n",
      "Epoch 50/50\n",
      " - 0s - loss: 210.7231 - val_loss: 192.9089\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 4026.4295 - val_loss: 1128.6211\n",
      "Epoch 2/50\n",
      " - 0s - loss: 1352.2379 - val_loss: 1209.2239\n",
      "Epoch 3/50\n",
      " - 0s - loss: 1106.1755 - val_loss: 944.8861\n",
      "Epoch 4/50\n",
      " - 0s - loss: 985.9336 - val_loss: 847.7694\n",
      "Epoch 5/50\n",
      " - 0s - loss: 859.4613 - val_loss: 773.7467\n",
      "Epoch 6/50\n",
      " - 0s - loss: 778.6992 - val_loss: 702.6952\n",
      "Epoch 7/50\n",
      " - 0s - loss: 701.8512 - val_loss: 639.4890\n",
      "Epoch 8/50\n",
      " - 0s - loss: 637.6756 - val_loss: 583.6561\n",
      "Epoch 9/50\n",
      " - 0s - loss: 577.8590 - val_loss: 534.8058\n",
      "Epoch 10/50\n",
      " - 0s - loss: 525.5518 - val_loss: 484.8686\n",
      "Epoch 11/50\n",
      " - 0s - loss: 480.0189 - val_loss: 444.5525\n",
      "Epoch 12/50\n",
      " - 0s - loss: 439.6554 - val_loss: 405.2902\n",
      "Epoch 13/50\n",
      " - 0s - loss: 405.9605 - val_loss: 369.5239\n",
      "Epoch 14/50\n",
      " - 0s - loss: 373.3706 - val_loss: 343.7931\n",
      "Epoch 15/50\n",
      " - 0s - loss: 346.6946 - val_loss: 316.1733\n",
      "Epoch 16/50\n",
      " - 0s - loss: 320.4015 - val_loss: 296.7592\n",
      "Epoch 17/50\n",
      " - 0s - loss: 296.8206 - val_loss: 270.8311\n",
      "Epoch 18/50\n",
      " - 0s - loss: 279.0083 - val_loss: 253.1614\n",
      "Epoch 19/50\n",
      " - 0s - loss: 262.5642 - val_loss: 240.9602\n",
      "Epoch 20/50\n",
      " - 0s - loss: 249.2724 - val_loss: 220.6964\n",
      "Epoch 21/50\n",
      " - 0s - loss: 234.8600 - val_loss: 209.5972\n",
      "Epoch 22/50\n",
      " - 0s - loss: 217.6831 - val_loss: 198.6526\n",
      "Epoch 23/50\n",
      " - 0s - loss: 207.0508 - val_loss: 189.2628\n",
      "Epoch 24/50\n",
      " - 0s - loss: 197.6153 - val_loss: 177.2253\n",
      "Epoch 25/50\n",
      " - 0s - loss: 190.0221 - val_loss: 169.2654\n",
      "Epoch 26/50\n",
      " - 0s - loss: 181.8453 - val_loss: 164.0188\n",
      "Epoch 27/50\n",
      " - 0s - loss: 174.8062 - val_loss: 157.0218\n",
      "Epoch 28/50\n",
      " - 0s - loss: 169.2788 - val_loss: 152.5607\n",
      "Epoch 29/50\n",
      " - 0s - loss: 164.2656 - val_loss: 147.7538\n",
      "Epoch 30/50\n",
      " - 0s - loss: 160.9952 - val_loss: 149.3085\n",
      "Epoch 31/50\n",
      " - 0s - loss: 159.3705 - val_loss: 139.8018\n",
      "Epoch 32/50\n",
      " - 0s - loss: 155.7609 - val_loss: 137.9464\n",
      "Epoch 33/50\n",
      " - 0s - loss: 150.6752 - val_loss: 135.4404\n",
      "Epoch 34/50\n",
      " - 0s - loss: 148.0916 - val_loss: 130.4396\n",
      "Epoch 35/50\n",
      " - 0s - loss: 147.0864 - val_loss: 136.1186\n",
      "Epoch 36/50\n",
      " - 0s - loss: 143.2293 - val_loss: 126.2464\n",
      "Epoch 37/50\n",
      " - 0s - loss: 145.6928 - val_loss: 127.3387\n",
      "Epoch 38/50\n",
      " - 0s - loss: 141.5577 - val_loss: 129.1683\n",
      "Epoch 39/50\n",
      " - 0s - loss: 140.7377 - val_loss: 121.3178\n",
      "Epoch 40/50\n",
      " - 0s - loss: 139.1486 - val_loss: 119.3507\n",
      "Epoch 41/50\n",
      " - 0s - loss: 138.7910 - val_loss: 139.0329\n",
      "Epoch 42/50\n",
      " - 0s - loss: 136.1695 - val_loss: 116.1387\n",
      "Epoch 43/50\n",
      " - 0s - loss: 133.8426 - val_loss: 127.2988\n",
      "Epoch 44/50\n",
      " - 0s - loss: 135.0944 - val_loss: 113.9017\n",
      "Epoch 45/50\n",
      " - 0s - loss: 132.2254 - val_loss: 120.6167\n",
      "Epoch 46/50\n",
      " - 0s - loss: 134.5148 - val_loss: 111.8123\n",
      "Epoch 47/50\n",
      " - 0s - loss: 130.7082 - val_loss: 113.9511\n",
      "Epoch 48/50\n",
      " - 0s - loss: 129.6634 - val_loss: 110.5992\n",
      "Epoch 49/50\n",
      " - 0s - loss: 130.1624 - val_loss: 110.0415\n",
      "Epoch 50/50\n",
      " - 0s - loss: 128.5252 - val_loss: 109.6246\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 114243.9062 - val_loss: 81990.0518\n",
      "Epoch 2/50\n",
      " - 0s - loss: 56497.8189 - val_loss: 36294.5993\n",
      "Epoch 3/50\n",
      " - 0s - loss: 22805.3698 - val_loss: 13573.9970\n",
      "Epoch 4/50\n",
      " - 0s - loss: 8942.5704 - val_loss: 6606.6320\n",
      "Epoch 5/50\n",
      " - 0s - loss: 5839.3957 - val_loss: 5444.3878\n",
      "Epoch 6/50\n",
      " - 0s - loss: 5430.0265 - val_loss: 5237.7447\n",
      "Epoch 7/50\n",
      " - 0s - loss: 5255.7319 - val_loss: 5052.5886\n",
      "Epoch 8/50\n",
      " - 0s - loss: 5055.4333 - val_loss: 4901.5730\n",
      "Epoch 9/50\n",
      " - 0s - loss: 4867.1994 - val_loss: 4711.3759\n",
      "Epoch 10/50\n",
      " - 0s - loss: 4676.4419 - val_loss: 4540.4436\n",
      "Epoch 11/50\n",
      " - 0s - loss: 4498.5792 - val_loss: 4358.6563\n",
      "Epoch 12/50\n",
      " - 0s - loss: 4321.4261 - val_loss: 4168.9521\n",
      "Epoch 13/50\n",
      " - 0s - loss: 4140.5824 - val_loss: 4009.0568\n",
      "Epoch 14/50\n",
      " - 0s - loss: 3966.5423 - val_loss: 3857.5448\n",
      "Epoch 15/50\n",
      " - 0s - loss: 3802.8163 - val_loss: 3679.5005\n",
      "Epoch 16/50\n",
      " - 0s - loss: 3637.6465 - val_loss: 3526.9103\n",
      "Epoch 17/50\n",
      " - 0s - loss: 3480.9134 - val_loss: 3376.7552\n",
      "Epoch 18/50\n",
      " - 0s - loss: 3324.9957 - val_loss: 3230.1552\n",
      "Epoch 19/50\n",
      " - 0s - loss: 3185.5515 - val_loss: 3081.2841\n",
      "Epoch 20/50\n",
      " - 0s - loss: 3037.0062 - val_loss: 2968.6703\n",
      "Epoch 21/50\n",
      " - 0s - loss: 2903.5933 - val_loss: 2828.9465\n",
      "Epoch 22/50\n",
      " - 0s - loss: 2770.0844 - val_loss: 2701.4400\n",
      "Epoch 23/50\n",
      " - 0s - loss: 2648.7182 - val_loss: 2593.6465\n",
      "Epoch 24/50\n",
      " - 0s - loss: 2527.7819 - val_loss: 2479.0108\n",
      "Epoch 25/50\n",
      " - 0s - loss: 2412.3812 - val_loss: 2367.6078\n",
      "Epoch 26/50\n",
      " - 0s - loss: 2306.1244 - val_loss: 2262.2210\n",
      "Epoch 27/50\n",
      " - 0s - loss: 2201.5645 - val_loss: 2167.8510\n",
      "Epoch 28/50\n",
      " - 0s - loss: 2105.2620 - val_loss: 2076.7579\n",
      "Epoch 29/50\n",
      " - 0s - loss: 2010.9126 - val_loss: 1979.9158\n",
      "Epoch 30/50\n",
      " - 0s - loss: 1920.0764 - val_loss: 1901.5341\n",
      "Epoch 31/50\n",
      " - 0s - loss: 1843.1043 - val_loss: 1818.4589\n",
      "Epoch 32/50\n",
      " - 0s - loss: 1761.4658 - val_loss: 1738.0899\n",
      "Epoch 33/50\n",
      " - 0s - loss: 1682.8170 - val_loss: 1671.0695\n",
      "Epoch 34/50\n",
      " - 0s - loss: 1623.4010 - val_loss: 1628.6719\n",
      "Epoch 35/50\n",
      " - 0s - loss: 1556.1236 - val_loss: 1528.8539\n",
      "Epoch 36/50\n",
      " - 0s - loss: 1479.7680 - val_loss: 1479.9133\n",
      "Epoch 37/50\n",
      " - 0s - loss: 1420.8476 - val_loss: 1428.7623\n",
      "Epoch 38/50\n",
      " - 0s - loss: 1362.4911 - val_loss: 1359.5718\n",
      "Epoch 39/50\n",
      " - 0s - loss: 1308.2736 - val_loss: 1306.8784\n",
      "Epoch 40/50\n",
      " - 0s - loss: 1257.1373 - val_loss: 1269.2460\n",
      "Epoch 41/50\n",
      " - 0s - loss: 1215.1610 - val_loss: 1208.3894\n",
      "Epoch 42/50\n",
      " - 0s - loss: 1163.0194 - val_loss: 1177.7076\n",
      "Epoch 43/50\n",
      " - 0s - loss: 1121.9040 - val_loss: 1131.1839\n",
      "Epoch 44/50\n",
      " - 0s - loss: 1079.4294 - val_loss: 1086.9220\n",
      "Epoch 45/50\n",
      " - 0s - loss: 1041.2821 - val_loss: 1050.3973\n",
      "Epoch 46/50\n",
      " - 0s - loss: 1003.7266 - val_loss: 1010.7454\n",
      "Epoch 47/50\n",
      " - 0s - loss: 967.6249 - val_loss: 978.3237\n",
      "Epoch 48/50\n",
      " - 0s - loss: 936.1086 - val_loss: 941.0125\n",
      "Epoch 49/50\n",
      " - 0s - loss: 902.6658 - val_loss: 921.0505\n",
      "Epoch 50/50\n",
      " - 0s - loss: 875.5322 - val_loss: 880.9834\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 3836.5556 - val_loss: 2573.3195\n",
      "Epoch 2/50\n",
      " - 0s - loss: 2858.2764 - val_loss: 2140.1542\n",
      "Epoch 3/50\n",
      " - 0s - loss: 2330.4282 - val_loss: 1752.3078\n",
      "Epoch 4/50\n",
      " - 0s - loss: 1938.6262 - val_loss: 1430.0729\n",
      "Epoch 5/50\n",
      " - 0s - loss: 1592.8988 - val_loss: 1184.3591\n",
      "Epoch 6/50\n",
      " - 0s - loss: 1332.2770 - val_loss: 1008.7868\n",
      "Epoch 7/50\n",
      " - 0s - loss: 1125.3610 - val_loss: 849.3162\n",
      "Epoch 8/50\n",
      " - 0s - loss: 941.7863 - val_loss: 724.5019\n",
      "Epoch 9/50\n",
      " - 0s - loss: 802.3419 - val_loss: 609.8859\n",
      "Epoch 10/50\n",
      " - 0s - loss: 673.7006 - val_loss: 524.4699\n",
      "Epoch 11/50\n",
      " - 0s - loss: 580.7512 - val_loss: 448.1092\n",
      "Epoch 12/50\n",
      " - 0s - loss: 497.8955 - val_loss: 391.5490\n",
      "Epoch 13/50\n",
      " - 0s - loss: 436.7786 - val_loss: 343.6189\n",
      "Epoch 14/50\n",
      " - 0s - loss: 387.1408 - val_loss: 308.1746\n",
      "Epoch 15/50\n",
      " - 0s - loss: 350.6022 - val_loss: 281.1185\n",
      "Epoch 16/50\n",
      " - 0s - loss: 312.8858 - val_loss: 253.4301\n",
      "Epoch 17/50\n",
      " - 0s - loss: 286.6205 - val_loss: 230.3748\n",
      "Epoch 18/50\n",
      " - 0s - loss: 261.7042 - val_loss: 213.3206\n",
      "Epoch 19/50\n",
      " - 0s - loss: 242.5962 - val_loss: 198.6745\n",
      "Epoch 20/50\n",
      " - 0s - loss: 226.7109 - val_loss: 184.2054\n",
      "Epoch 21/50\n",
      " - 0s - loss: 208.6691 - val_loss: 172.3946\n",
      "Epoch 22/50\n",
      " - 0s - loss: 196.3170 - val_loss: 163.8952\n",
      "Epoch 23/50\n",
      " - 0s - loss: 189.5100 - val_loss: 159.0150\n",
      "Epoch 24/50\n",
      " - 0s - loss: 175.8112 - val_loss: 150.5017\n",
      "Epoch 25/50\n",
      " - 0s - loss: 167.3000 - val_loss: 141.3341\n",
      "Epoch 26/50\n",
      " - 0s - loss: 160.1846 - val_loss: 136.9464\n",
      "Epoch 27/50\n",
      " - 0s - loss: 152.0078 - val_loss: 130.7104\n",
      "Epoch 28/50\n",
      " - 0s - loss: 145.0771 - val_loss: 125.7527\n",
      "Epoch 29/50\n",
      " - 0s - loss: 142.4902 - val_loss: 123.0996\n",
      "Epoch 30/50\n",
      " - 0s - loss: 135.6011 - val_loss: 118.5541\n",
      "Epoch 31/50\n",
      " - 0s - loss: 130.2173 - val_loss: 113.6077\n",
      "Epoch 32/50\n",
      " - 0s - loss: 126.9990 - val_loss: 111.1049\n",
      "Epoch 33/50\n",
      " - 0s - loss: 126.1982 - val_loss: 109.4666\n",
      "Epoch 34/50\n",
      " - 0s - loss: 123.2913 - val_loss: 106.6110\n",
      "Epoch 35/50\n",
      " - 0s - loss: 116.0062 - val_loss: 105.9010\n",
      "Epoch 36/50\n",
      " - 0s - loss: 112.0373 - val_loss: 102.1487\n",
      "Epoch 37/50\n",
      " - 0s - loss: 111.4953 - val_loss: 102.3325\n",
      "Epoch 38/50\n",
      " - 0s - loss: 108.1247 - val_loss: 100.8725\n",
      "Epoch 39/50\n",
      " - 0s - loss: 104.1274 - val_loss: 95.9133\n",
      "Epoch 40/50\n",
      " - 0s - loss: 102.9552 - val_loss: 95.0029\n",
      "Epoch 41/50\n",
      " - 0s - loss: 103.3717 - val_loss: 92.1664\n",
      "Epoch 42/50\n",
      " - 0s - loss: 97.7521 - val_loss: 91.4927\n",
      "Epoch 43/50\n",
      " - 0s - loss: 96.9649 - val_loss: 93.9431\n",
      "Epoch 44/50\n",
      " - 0s - loss: 95.1707 - val_loss: 87.3980\n",
      "Epoch 45/50\n",
      " - 0s - loss: 95.2550 - val_loss: 88.3298\n",
      "Epoch 46/50\n",
      " - 0s - loss: 91.9974 - val_loss: 87.4973\n",
      "Epoch 47/50\n",
      " - 0s - loss: 92.1331 - val_loss: 84.8029\n",
      "Epoch 48/50\n",
      " - 0s - loss: 90.2078 - val_loss: 83.3797\n",
      "Epoch 49/50\n",
      " - 0s - loss: 88.5008 - val_loss: 83.6092\n",
      "Epoch 50/50\n",
      " - 0s - loss: 85.2548 - val_loss: 80.7173\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 173907.7591 - val_loss: 128176.0149\n",
      "Epoch 2/50\n",
      " - 0s - loss: 100385.4148 - val_loss: 70589.5038\n",
      "Epoch 3/50\n",
      " - 0s - loss: 53470.5934 - val_loss: 35408.3596\n",
      "Epoch 4/50\n",
      " - 0s - loss: 25848.7471 - val_loss: 15865.8298\n",
      "Epoch 5/50\n",
      " - 0s - loss: 11210.8180 - val_loss: 6423.1658\n",
      "Epoch 6/50\n",
      " - 0s - loss: 4484.5954 - val_loss: 2680.0047\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2004.6749 - val_loss: 1472.8468\n",
      "Epoch 8/50\n",
      " - 0s - loss: 1220.1883 - val_loss: 1208.1808\n",
      "Epoch 9/50\n",
      " - 0s - loss: 1038.7617 - val_loss: 1162.6384\n",
      "Epoch 10/50\n",
      " - 0s - loss: 998.1893 - val_loss: 1146.6909\n",
      "Epoch 11/50\n",
      " - 0s - loss: 972.1882 - val_loss: 1125.0705\n",
      "Epoch 12/50\n",
      " - 0s - loss: 950.9134 - val_loss: 1097.8460\n",
      "Epoch 13/50\n",
      " - 0s - loss: 926.4461 - val_loss: 1073.8280\n",
      "Epoch 14/50\n",
      " - 0s - loss: 903.8109 - val_loss: 1049.3663\n",
      "Epoch 15/50\n",
      " - 0s - loss: 880.3829 - val_loss: 1023.6241\n",
      "Epoch 16/50\n",
      " - 0s - loss: 857.8447 - val_loss: 1001.5756\n",
      "Epoch 17/50\n",
      " - 0s - loss: 833.4201 - val_loss: 975.5333\n",
      "Epoch 18/50\n",
      " - 0s - loss: 811.5882 - val_loss: 950.5617\n",
      "Epoch 19/50\n",
      " - 0s - loss: 788.1045 - val_loss: 925.3718\n",
      "Epoch 20/50\n",
      " - 0s - loss: 765.6949 - val_loss: 900.8955\n",
      "Epoch 21/50\n",
      " - 0s - loss: 743.4974 - val_loss: 878.7289\n",
      "Epoch 22/50\n",
      " - 0s - loss: 721.0106 - val_loss: 855.2992\n",
      "Epoch 23/50\n",
      " - 0s - loss: 700.1651 - val_loss: 831.9729\n",
      "Epoch 24/50\n",
      " - 0s - loss: 678.9292 - val_loss: 808.6088\n",
      "Epoch 25/50\n",
      " - 0s - loss: 658.0059 - val_loss: 787.6111\n",
      "Epoch 26/50\n",
      " - 0s - loss: 638.7292 - val_loss: 765.6183\n",
      "Epoch 27/50\n",
      " - 0s - loss: 619.1893 - val_loss: 745.5052\n",
      "Epoch 28/50\n",
      " - 0s - loss: 600.5160 - val_loss: 724.0355\n",
      "Epoch 29/50\n",
      " - 0s - loss: 583.0692 - val_loss: 707.0215\n",
      "Epoch 30/50\n",
      " - 0s - loss: 565.2925 - val_loss: 688.3760\n",
      "Epoch 31/50\n",
      " - 0s - loss: 546.0880 - val_loss: 664.2347\n",
      "Epoch 32/50\n",
      " - 0s - loss: 530.1923 - val_loss: 647.6810\n",
      "Epoch 33/50\n",
      " - 0s - loss: 513.7563 - val_loss: 628.6905\n",
      "Epoch 34/50\n",
      " - 0s - loss: 501.0317 - val_loss: 619.6103\n",
      "Epoch 35/50\n",
      " - 0s - loss: 482.1484 - val_loss: 597.6177\n",
      "Epoch 36/50\n",
      " - 0s - loss: 466.7427 - val_loss: 580.6036\n",
      "Epoch 37/50\n",
      " - 0s - loss: 451.9555 - val_loss: 565.2726\n",
      "Epoch 38/50\n",
      " - 0s - loss: 437.9463 - val_loss: 552.9495\n",
      "Epoch 39/50\n",
      " - 0s - loss: 425.1631 - val_loss: 536.1574\n",
      "Epoch 40/50\n",
      " - 0s - loss: 411.3984 - val_loss: 523.1926\n",
      "Epoch 41/50\n",
      " - 0s - loss: 399.4588 - val_loss: 511.1835\n",
      "Epoch 42/50\n",
      " - 0s - loss: 387.3698 - val_loss: 495.5228\n",
      "Epoch 43/50\n",
      " - 0s - loss: 377.0612 - val_loss: 484.0094\n",
      "Epoch 44/50\n",
      " - 0s - loss: 364.7842 - val_loss: 468.6572\n",
      "Epoch 45/50\n",
      " - 0s - loss: 354.1926 - val_loss: 458.4757\n",
      "Epoch 46/50\n",
      " - 0s - loss: 344.4478 - val_loss: 446.0152\n",
      "Epoch 47/50\n",
      " - 0s - loss: 334.6454 - val_loss: 435.2501\n",
      "Epoch 48/50\n",
      " - 0s - loss: 326.3221 - val_loss: 424.1472\n",
      "Epoch 49/50\n",
      " - 0s - loss: 317.0084 - val_loss: 417.1334\n",
      "Epoch 50/50\n",
      " - 0s - loss: 306.9756 - val_loss: 403.1608\n",
      "Train on 504 samples, validate on 217 samples\n",
      "Epoch 1/50\n",
      " - 2s - loss: 48590.2777 - val_loss: 27346.1998\n",
      "Epoch 2/50\n",
      " - 0s - loss: 16847.2568 - val_loss: 7847.4490\n",
      "Epoch 3/50\n",
      " - 0s - loss: 4848.5557 - val_loss: 3055.6270\n",
      "Epoch 4/50\n",
      " - 0s - loss: 2719.2106 - val_loss: 2741.3114\n",
      "Epoch 5/50\n",
      " - 0s - loss: 2542.3426 - val_loss: 2638.4656\n",
      "Epoch 6/50\n",
      " - 0s - loss: 2410.5858 - val_loss: 2477.6110\n",
      "Epoch 7/50\n",
      " - 0s - loss: 2289.6286 - val_loss: 2357.4402\n",
      "Epoch 8/50\n",
      " - 0s - loss: 2185.8358 - val_loss: 2250.8158\n",
      "Epoch 9/50\n",
      " - 0s - loss: 2087.3415 - val_loss: 2150.6885\n",
      "Epoch 10/50\n",
      " - 0s - loss: 1993.2707 - val_loss: 2050.8073\n",
      "Epoch 11/50\n",
      " - 0s - loss: 1904.6782 - val_loss: 1960.1135\n",
      "Epoch 12/50\n",
      " - 0s - loss: 1822.3388 - val_loss: 1873.4252\n",
      "Epoch 13/50\n",
      " - 0s - loss: 1744.0843 - val_loss: 1794.3907\n",
      "Epoch 14/50\n",
      " - 0s - loss: 1675.4268 - val_loss: 1717.8302\n",
      "Epoch 15/50\n",
      " - 0s - loss: 1605.8591 - val_loss: 1647.0521\n",
      "Epoch 16/50\n",
      " - 0s - loss: 1546.1780 - val_loss: 1581.1867\n",
      "Epoch 17/50\n",
      " - 0s - loss: 1489.8895 - val_loss: 1520.0469\n",
      "Epoch 18/50\n",
      " - 0s - loss: 1434.8435 - val_loss: 1466.2811\n",
      "Epoch 19/50\n",
      " - 0s - loss: 1386.6476 - val_loss: 1406.8865\n",
      "Epoch 20/50\n",
      " - 0s - loss: 1335.1299 - val_loss: 1358.8167\n",
      "Epoch 21/50\n",
      " - 0s - loss: 1292.9326 - val_loss: 1311.3595\n",
      "Epoch 22/50\n",
      " - 0s - loss: 1252.0054 - val_loss: 1266.9699\n",
      "Epoch 23/50\n",
      " - 0s - loss: 1215.0736 - val_loss: 1225.0202\n",
      "Epoch 24/50\n",
      " - 0s - loss: 1175.2463 - val_loss: 1185.0873\n",
      "Epoch 25/50\n",
      " - 0s - loss: 1141.7377 - val_loss: 1146.8644\n",
      "Epoch 26/50\n",
      " - 0s - loss: 1113.6041 - val_loss: 1111.0028\n",
      "Epoch 27/50\n",
      " - 0s - loss: 1078.3704 - val_loss: 1083.9729\n",
      "Epoch 28/50\n",
      " - 0s - loss: 1049.4654 - val_loss: 1045.0442\n",
      "Epoch 29/50\n",
      " - 0s - loss: 1018.3387 - val_loss: 1015.7019\n",
      "Epoch 30/50\n",
      " - 0s - loss: 993.0619 - val_loss: 986.1355\n",
      "Epoch 31/50\n",
      " - 0s - loss: 967.1768 - val_loss: 958.6879\n",
      "Epoch 32/50\n",
      " - 0s - loss: 943.2192 - val_loss: 933.3414\n",
      "Epoch 33/50\n",
      " - 0s - loss: 919.9846 - val_loss: 908.2368\n",
      "Epoch 34/50\n",
      " - 0s - loss: 899.0717 - val_loss: 885.4742\n",
      "Epoch 35/50\n",
      " - 0s - loss: 876.0834 - val_loss: 862.7612\n",
      "Epoch 36/50\n",
      " - 0s - loss: 858.5549 - val_loss: 842.4744\n",
      "Epoch 37/50\n",
      " - 0s - loss: 835.5277 - val_loss: 821.3723\n",
      "Epoch 38/50\n",
      " - 0s - loss: 820.3171 - val_loss: 800.7840\n",
      "Epoch 39/50\n",
      " - 0s - loss: 802.0413 - val_loss: 783.1054\n",
      "Epoch 40/50\n",
      " - 0s - loss: 791.6570 - val_loss: 766.3014\n",
      "Epoch 41/50\n",
      " - 0s - loss: 767.9152 - val_loss: 748.1298\n",
      "Epoch 42/50\n",
      " - 0s - loss: 753.4555 - val_loss: 732.0453\n",
      "Epoch 43/50\n",
      " - 0s - loss: 737.1520 - val_loss: 717.6953\n",
      "Epoch 44/50\n",
      " - 0s - loss: 726.5498 - val_loss: 702.2889\n",
      "Epoch 45/50\n",
      " - 0s - loss: 711.6333 - val_loss: 687.7681\n",
      "Epoch 46/50\n",
      " - 0s - loss: 702.3531 - val_loss: 676.5302\n",
      "Epoch 47/50\n",
      " - 0s - loss: 684.6155 - val_loss: 662.4567\n",
      "Epoch 48/50\n",
      " - 0s - loss: 676.7399 - val_loss: 648.7738\n",
      "Epoch 49/50\n",
      " - 0s - loss: 662.8891 - val_loss: 638.8240\n",
      "Epoch 50/50\n",
      " - 0s - loss: 651.3331 - val_loss: 625.7927\n",
      "[815.0545674859374, 384.88484673137793, 103.2628999439093, 749.0642302360009, 130.09875873526258, 278.0447822840748, 345.8185989633449, 159.83479745603395, 553.0003693828685, 123.4892358748003, 154.01360010990922, 978.4792136404005, 555.2736845779092, 562.6538354768928, 104.672539195746, 1954.7765511652408, 1464.8181261956352, 142.27959429467495, 3196.412435113943, 429.2234373728038, 291.284930392845, 201.07307424611258, 976.497352834639, 343.3525957654487, 511.584869181728, 293.43438577029207, 241.96401454109338, 942.2007122237497, 149.79003417201662, 2280.7954603453095, 303.35382651408463, 661.4330538649543, 857.2093516780809, 3039.560111785358, 919.8734739933822, 1226.6931745460117, 805.5420295694903, 271.7704805819535, 169.9419031159919, 117.06118094217953, 223.03800506844772, 113.57711520839744, 932.7273733369358, 192.91752215385782, 218.63826484472884, 128.445135311528, 937.674799831722, 92.2980006244855, 338.41122004677123, 605.5141592395521]\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "mse = []\n",
    "for i in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3)\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, validation_split=0.3, epochs=50, verbose=2)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse.append(mean_squared_error(y_test,y_pred))\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(50), mse)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('MSE vs. Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_mse = np.mean(mse)\n",
    "std_mse = np.std(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MSE:  631.4562743193583\n",
      "Standard Deviation MSE: 689.2178547774291\n"
     ]
    }
   ],
   "source": [
    "print('Mean MSE: ', mean_mse)\n",
    "print('Standard Deviation MSE:', std_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # fit the model\n",
    "# from keras.callbacks import EarlyStopping\n",
    "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1,)\n",
    "# history = model.fit(predictors, target, validation_split=0.3, epochs=50, verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# # Define EarlyStopping callback\n",
    "# # early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# # Define ModelCheckpoint callback to save the best model weights\n",
    "# model_checkpoint = ModelCheckpoint('best_model_weights.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# # Create and compile your model\n",
    "# model = regression_model()  # Replace with your model creation code\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# # Fit the model with the EarlyStopping and ModelCheckpoint callbacks\n",
    "# history = model.fit(predictors_norm, target, validation_split=0.3, epochs=100, verbose=2, callbacks=[early_stopping, model_checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have already trained your model and obtained the training history\n",
    "# # The history variable typically contains information about training and validation loss over epochs.\n",
    "\n",
    "# # Plot training & validation loss values\n",
    "# training_loss = history.history['loss']\n",
    "# validation_loss = history.history['val_loss']\n",
    "\n",
    "# epochs = range(1, len(training_loss) + 1)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, training_loss, 'b', label='Training Loss')\n",
    "# plt.plot(epochs, validation_loss, 'r', label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# validation_loss\n",
    "# training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<strong>You can refer to this [link](https://keras.io/models/sequential/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01) to learn about other functions that you can use for prediction or evaluation.</strong>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to vary the following and note what impact each change has on the model's performance:\n",
    "\n",
    "1. Increase or decreate number of neurons in hidden layers\n",
    "2. Add more hidden layers\n",
    "3. Increase number of epochs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by [Alex Aklson](https://www.linkedin.com/in/aklson/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01). I hope you found this lab interesting and educational. Feel free to contact me if you have any questions!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-21  | 2.0  | Srishti  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\">  IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *Introduction to Deep Learning & Neural Networks with Keras*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0101EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2019 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0101ENSkillsNetwork945-2022-01-01).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
